\documentclass[10pt,a4paper,twoside, english]{article}
%\frenchspacing
\usepackage{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[T1,mtbold,lucidacal,mtplusscr,subscriptcorrection]{mathtime}
\usepackage{times}
\usepackage[pdftex]{graphicx}
\usepackage{color}
%\usepackage{palatino}
\usepackage[pdftex,colorlinks=true,citecolor=black,
            pagecolor=black,linkcolor=black,menucolor=black,
            urlcolor=black]{hyperref}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{eufrak}
\usepackage{amsbsy}
\usepackage{eucal}
\usepackage{subfigure}
\usepackage{longtable}
\usepackage{url}
\RequirePackage{moreverb}
\RequirePackage{listings}
\urlstyle{same}
\usepackage{natbib}
\bibliographystyle{apalike}

\lstloadlanguages{[visual]C++,matlab}
\lstset{basicstyle=\normalsize,columns=fullflexible,frame=lines}

\oddsidemargin 1cm    %   Note \oddsidemargin = \evensidemargin
\evensidemargin 1cm
\marginparwidth 0.2 true cm
%\marginparwidth 0.75 true in
%\topmargin 0 true pt           % Nominal distance from top of page to top of
%\topmargin 0.125in
\topmargin -1.25cm
\addtolength{\headsep}{0.25in}
\textheight 21.5 true cm       % Height of text (including footnotes & figures)
\textwidth 14 true cm        % Width of text line.

% \topmargin 5mm
% \oddsidemargin 16mm
% \evensidemargin 16mm
% \textheight 200mm
% \textwidth 125mm
% \setlength{\parindent}{0pt}

\pdfinfo{            
          /Title      (EFK/UKF toolbox for Matlab)
          /Author     (Jouni Hartikainen, Simo Särkkä)
          /Keywords   (Kalman filter Extended Unscented optimal filtering)
}

%\newtheorem{Lemma}{Lemma}
%\newtheorem{Definition}{Definition}
%\DeclareMathOperator{\Poisson}{Poisson}
%\DeclareMathOperator{\GP}{\mathcal{GP}}
%\DeclareMathOperator{\Kfu}{\mathbf{K}_{f,u}}
%\DeclareMathOperator{\Kuf}{\mathbf{K}_{u,f}}
%\DeclareMathOperator{\Kff}{\mathbf{K}_{f,f}}
%\DeclareMathOperator{\Kfa}{\mathbf{K}_{f,\ast}}
%\DeclareMathOperator{\Kaf}{\mathbf{K}_{\ast,f}}
%\DeclareMathOperator{\Kaa}{\mathbf{K}_{\ast,\ast}}
%\DeclareMathOperator{\Kuu}{\mathbf{K}_{u,u}}
%\DeclareMathOperator{\Kau}{\mathbf{K}_{\ast,u}}
%\DeclareMathOperator{\Qff}{\mathbf{Q}_{f,f}}
%\DeclareMathOperator{\Qaa}{\mathbf{Q}_{\ast,\ast}}
%\DeclareMathOperator{\Qfa}{\mathbf{Q}_{f,\ast}}
%\DeclareMathOperator{\Qaf}{\mathbf{Q}_{\ast,f}}
%\DeclareMathOperator{\x}{\mathbf{x}}
%\DeclareMathOperator{\f}{\mathbf{f}}
%\DeclareMathOperator{\y}{\mathbf{y}}
%\DeclareMathOperator{\uu}{\mathbf{u}}
%\DeclareMathOperator{\LL}{\mathbf{\Lambda}}
%\DeclareMathOperator{\bb}{\mathbf{b}}

%\newcommand{\bm}{\mathbf}

\providecommand\figurename{Figure}
\providecommand\tablename{Table}
\providecommand\partname{Part}
\providecommand\appendixname{Appendix}
\providecommand\equationname{Eq\-ua\-tion}
%\providecommand\Itemname{item}
\providecommand\chaptername{chapter}
%\providecommand\sectionname{section}
\providecommand\subsectionname{section}
\providecommand\subsubsectionname{section}
%\providecommand\paragraphname{paragraph}
%\providecommand\theoremname{Theorem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% macros begin
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\alg}[1]{\mathscr{#1}}
\newcommand{\spc}[1]{\mathbb{#1}}
\newcommand{\ope}[1]{\EuScript{#1}}
\newcommand{\mea}[1]{#1}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
%\renewcommand{\vec}[1]{#1}
%\newcommand{\mat}[1]{#1}
\newcommand{\V}[1]{\mathbf{#1}}


\newcommand{\diff}[0]{\mathrm{d}}
%\newcommand{\diff}[0]{d}
%\newcommand{\B}[0]{\mathrm{\beta}}
%\newcommand{\Dt}[0]{\partial t}
\newcommand{\Dt}[0]{\delta t}
\newcommand{\Dx}[0]{\delta \vec{x}}
\newcommand{\Eg}[0]{\EuScript{E}}

\newcommand{\vectheta}[0]{\boldsymbol{\theta}}
\newcommand{\vecalpha}[0]{\boldsymbol{\alpha}}
\newcommand{\vecbeta}[0]{\boldsymbol{\beta}}
\newcommand{\veceta}[0]{\boldsymbol{\eta}}
\newcommand{\vecmu}[0]{\boldsymbol{\mu}}
\newcommand{\vecsigma}[0]{\boldsymbol{\sigma}}

%\DeclareMathOperator{\tr}{trace}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\chol}{chol}
\DeclareMathOperator{\dchol}{dchol}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\gammad}{Gamma}
\DeclareMathOperator{\expd}{Exp}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\dt}{\Delta t}
\DeclareMathOperator{\dtk}{\Delta t_k}


\newcommand{\balpha}[0]{\boldsymbol{\alpha}}
\newcommand{\bbeta}[0]{\boldsymbol{\beta}}
\newcommand{\btheta}[0]{\boldsymbol{\theta}}
\newcommand{\bphi}[0]{\boldsymbol{\phi}}
\newcommand{\bmu}[0]{\boldsymbol{\mu}}
\newcommand{\bSigma}[0]{\boldsymbol{\Sigma}}
\newcommand{\blambda}[0]{\boldsymbol{\lambda}}
\newcommand{\brho}[0]{\boldsymbol{\rho}}
\newcommand{\bGamma}[0]{\boldsymbol{\Gamma}}

%\newtheorem{example}{Example}[section]
%\newtheorem{definition}{Definition}[section]
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}{Lemma}[section]
%\newtheorem{corollary}{Corollary}[section]
%\newtheorem{property}{Property}[section]

%\newtheorem{algorithm}{Algorithm}[chapter]
%\newtheorem{example}{Example}[chapter]
%\newtheorem{definition}{Definition}[chapter]
%\newtheorem{theorem}{Theorem}[chapter]
%\newtheorem{lemma}{Lemma}[chapter]
%\newtheorem{corollary}{Corollary}[chapter]
%\newtheorem{property}{Property}[chapter]
%\newtheorem{remark}{Remark}[chapter]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% macros end
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Lists
\newenvironment{boxlist1}{
  \begin{list}{}{\leftmargin=2pt}}{ % \leftmargin=0pt,
    % $\bullet$  ,\parsep=0pt
  \end{list}}
\newenvironment{boxlist2}{
  \begin{list}{-}{\itemsep=-1pt,\topsep=0pt,\setlength{\leftmargin}{8pt}}}{
    % 
  \end{list}}

\title{Optimal filtering with Kalman filters and smoothers -- a
  Manual for Matlab toolbox EKF/UKF}
\author{Jouni Hartikainen and Simo Särkkä\\
Laboratory of Computational Engineering,\\
Helsinki University of Technology,\\
P.O.Box 9203, FIN-02015 TKK, Espoo, Finland \\
{\it jmjharti@cc.hut.fi, simo.sarkka@hut.fi}\vspace{-.5\baselineskip}}
%\date{}

\begin{document}
\maketitle
\begin{center}
Version 1.0
\end{center}
\begin{abstract}
%\noindent
%{\bf Abstract} 

In this paper we present a documentation for optimal filtering toolbox for mathematical
software package Matlab. The methods in the toolbox include Kalman
filter, extended Kalman filter and unscented Kalman filter for
discrete time state space models. Algorithms for multiple model
systems are provided in the form of Interacting Multiple Model (IMM)
filter and it's non-linear extensions, which are based on banks of
extended and unscented Kalman filters. Also included in the toolbox are the
Rauch-Tung-Striebel and two-filter smoother counter-parts for
each filter, which can be used to smooth the previous state estimates,
after obtaining new measurements. The usage and function of each method
are illustrated with eight demonstrations problems.    


\end{abstract}


\newenvironment{demo1}{\noindent\textbf{demo\_2input:}\begin{quote}\small\itshape}
{\end{quote}}

\newenvironment{example}{\small\begin{verbatim}}
{\end{verbatim}}

\newenvironment{demo2}{\noindent\textbf{demo\_2ingp:}\begin{quote}\small\itshape}
{\end{quote}}

%---------------------------------------
\newpage

\tableofcontents

\newpage
%---------------------------------------

\section*{Preface}

Most of the software provided with this toolbox are originally created by
Simo Särkkä while he was doing research on his doctoral thesis
(Särkkä, 2006b) in the Laboratory of Computational Engineering (LCE) at
Helsinki University of Technology (HUT). This document has been written by
Jouni Hartikainen at LCE during spring 2007 with a little help
from Simo Särkkä. Jouni also checked and commented the software code
thoroughly. Many (small) bugs were fixed, and also some new functions were
implemented (for example 2nd order EKF and augmented form UKF). Jouni
also provided the software code for first three demonstrations, modified
the two last ones a bit, and ran all the simulations.

First author would like to thank Doc. Aki Vehtari for helpful comments
during the work and for coming up with the idea of this
toolbox in the first place. Prof. Jouko Lampinen also deserves
thanks for ultimately making this work possible.

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introduction}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The term optimal filtering refers to methodology used for estimating
the {\it state}
of a time varying system, from which we observe indirect noisy
measurements. The state refers to the physical state, which can be
described by dynamic variables, such as position, velocity and
acceleration of a moving object. The noise in
the measurements means that there is a certain degree of uncertainty in them.
The dynamic system evolves as a function of time, and there is also noise in
the dynamics of system, {\it process noise}, meaning that the dynamic
system cannot be modelled entirely deterministically. In this context, the term
filtering basically means the process of filtering out the noise in
the measurements and providing an optimal estimate for the state given the observed
measurements and the assumptions made about the dynamic system. 

This toolbox provides basic tools for estimating the state of a linear
dynamic system, the Kalman filter, and also two extensions for it, the
extended Kalman filter (EKF) and unscented Kalman filter (UKF), both of
which can be used for estimating the states of nonlinear dynamic
systems. Also the smoother counterparts of the filters are
provided. Smoothing in this context means giving an estimate of the state of
the system on some time step given all the measurements including ones
encountered after that particular time step, in other words, the smoother
gives a smoothed estimate for the history of the system's evolved state given
all the measurements obtained so far. 

This documentation is organized as follows: 
\begin{itemize}

\item First we briefly introduce the
concept of discrete-time state space models. After that we consider
linear, discrete-time state space models in more detail and review
Kalman filter, which is the basic method for recursively solving the
linear state space estimation problems. Also Kalman smoother is
introduced. After that the function of Kalman filter and smoother
and their usage in this toolbox in demonstrated with one example
(CWPA-model).

\item Next we move from linear to nonlinear state space
  models and review the extended Kalman filter (and smoother), which
  is the classical extension to Kalman filter for nonlinear
  estimation. The usage of EKF in this toolbox is illustrated
  exclusively with one
  example (Tracking a random sine signal), which also compares the
  performances of EKF, UKF and their smoother counter-parts. 

\item After EKF we review unscented Kalman filter (and smoother), which is a newer
  extension to traditional Kalman filter to cover nonlinear filtering
  problems. The usage of UKF is illustrated with one example
  (UNGM-model), which also demonstrates the differences between
  different nonlinear filtering techniques.

\item To give a more thorough demonstration to the provided methods two
  more classical nonlinear filtering examples are provided (Bearings
  Only Tracking and Reentry Vehicle Tracking).

\item In section 3 we shortly review the concept multiple model
  systems in general, and in sections 3.1 and 3.2 we take a look at linear
  and non-linear multiple model systems in more detail. We also
  review the standard method, the Interacting Multiple Model (IMM)
  filter, for estimating such systems. It's usage and function is
  demonstrated with three examples.

\item Lastly we list and describe briefly all the functions included in the toolbox.

\end{itemize}

The mathematical notation used in this document follows the notation used
in (Särkkä, 2006b). 

%Most of the code has been written by Simo Särkkä in the
%Laboratory of Computational Engineering at Helsinki University of Technology.  

%\section{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Discrete-Time State Space Models}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section we shall consider models where the states are defined
in discrete-time models. The models are defined recursively in terms of distributions 
%
\begin{equation}
\begin{split}
    \vec{x}_k &\sim p(\vec{x}_k\,|\,\vec{x}_{k-1}) \\
    \vec{y}_k &\sim p(\vec{y}_k\,|\,\vec{x}_k),
\end{split}
\label{eq:dss_model}
\end{equation}
%
where
%
\begin{itemize}
\item $\vec{x}_k \in \spc{R}^n$ is the {\em state} of the system on
  the time step $k$.
  
\item $\vec{y}_k \in \spc{R}^m$ is the measurement on the time step $k$.

\item $p(\vec{x}_k\,|\,\vec{x}_{k-1})$ is the dynamic model which charaterizes
  the dynamic behaviour of the system. Usually the model is a
  probability density (continous state), but it can also be a counting
  measure (discrete state), or a combination of them, if the state is
  both continuous and discrete.
%
\item $p(\vec{y}_k\,|\,\vec{x}_k)$ is the model for measurements,
  which describes how the measurements are distributed given the
  state. This model characterizes how the dynamic model is perceived
  by the observers.
\end{itemize}

A system defined this way has the so called \emph{Markov}-property, which means that the state
$\vec{x}_k$ given $\vec{x}_{k-1}$ is independent from the history of states and measurements, which
can also be expressed with the following equality:
%
\begin{equation}
p(\vec{x}_k\,|\,\vec{x}_{1:k-1},\vec{y}_{1:k-1})
= p(\vec{x}_k\,|\,\vec{x}_{k-1}).
\end{equation}
%
The past doesn't depend on the future given the present, which is the same as
%
\begin{equation}
p(\vec{x}_{k-1}\,|\,\vec{x}_{k:T},\vec{y}_{k:T})
= p(\vec{x}_{k-1}\,|\,\vec{x}_{k}).
\end{equation}
% 
The same applies also to measurements meaning that the measurement $\vec{y}_k$ is independent from
the histories of measurements and states, which can be expressed with equality
%
\begin{equation}
p(\vec{y}_k\,|\,\vec{x}_{1:k},\vec{y}_{1:k-1})
= p(\vec{y}_k\,|\,\vec{x}_{k}).
\end{equation}

In actual application problems, we are interested in predicting and estimating dynamic system's
state given the measurements obtained so far. In probabilistic terms, we are interested in the
predictive distribution for the state at the next time step  
%
\begin{equation}
  p(\vec{x}_{k}\,|\,\vec{y}_{1:k-1}),
\end{equation}
%
and in the marginal posterior distribution for the state at the current time step
%
\begin{equation}
  p(\vec{x}_{k}\,|\,\vec{y}_{1:k}).
\end{equation}
%
The formal solutions for these distribution are given by the following
recursive Bayesian filtering equations (e.g. Särkkä, 2006b):
%
\begin{equation}
    p(\vec{x}_k\,|\,\vec{y}_{1:k-1})
    = \int p(\vec{x}_k\,|\,\vec{x}_{k-1})
       \, p(\vec{x}_{k-1}\,|\,\vec{y}_{1:k-1}) \, \diff\vec{x}_{k-1}.
\label{eq:bayesf_p}
\end{equation}
and
\begin{equation}
    p(\vec{x}_k\,|\,\vec{y}_{1:k})
    = \frac{1}{Z_k} p(\vec{y}_k\,|\,\vec{x}_k)
       \, p(\vec{x}_k\,|\,\vec{y}_{1:k-1}),
\label{eq:bayesf_u}
\end{equation}
where the normalization constant $Z_k$ is given as
 %
\begin{equation}
  Z_k = \int p(\vec{y}_k\,|\,\vec{x}_k)
          \, p(\vec{x}_k\,|\,\vec{y}_{1:k-1}) \, \diff\vec{x}_k.
\label{eq:zk}
\end{equation}
%

In many cases we are also interested in smoothed state estimates of previous time steps given the
measurements obtained so far. In other words, we are interested in the marginal posterior
distribution 
%
\begin{equation}
  p(\vec{x}_{k}\,|\,\vec{y}_{1:T}),
\end{equation}
%
where $T > k$. As with the filtering equations above also in this case
we can express the formal solution as a set of recursive Bayesian
equations (e.g. Särkkä, 2006b):
%
\begin{equation}
\begin{split}
  p(\vec{x}_{k+1}\,|\,\vec{y}_{1:k})
  &= \int p(\vec{x}_{k+1}\,|\,\vec{x}_k) \,
          p(\vec{x}_k\,|\,\vec{y}_{1:k}) \, \diff\vec{x}_k \\
  p(\vec{x}_k\,|\,\vec{y}_{1:T})                          
  &= p(\vec{x}_k\,|\,\vec{y}_{1:k})
     \int \left[ \frac{p(\vec{x}_{k+1}\,|\,\vec{x}_k)
                 \, p(\vec{x}_{k+1}\,|\,\vec{y}_{1:T})}
                 {p(\vec{x}_{k+1}\,|\,\vec{y}_{1:k})} \right]
         \diff \vec{x}_{k+1}.
\end{split}
\end{equation}
%





%The formal solutions for these marginal distributions are not described here,
%but interested readers should see, for example, (Särkkä, 2006b) for more details.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Linear state space estimation}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The simplest of the state space models considered in this documentation are
linear models, which can be expressed with equations of
the following form:
%
\begin{equation}
\begin{split}
  \vec{x}_{k} &= \mat{A}_{k-1} \, \vec{x}_{k-1} + \vec{q}_{k-1} \\
  \vec{y}_{k} &= \mat{H}_{k}   \, \vec{x}_{k}   + \vec{r}_{k},
\end{split}
\label{eq:kalman_model}
\end{equation}
%
where 
%
\begin{itemize}
\item $\vec{x}_k \in \spc{R}^n$ is the {\em state} of the system on
  the time step $k$.
%
\item $\vec{y}_k \in \spc{R}^m$ is the measurement on the time step $k$.
%
\item $\vec{q}_{k-1} \sim \N(\vec{0},\mat{Q}_{k-1})$ is the process noise on
the time step $k-1$.
%
\item $\vec{r}_{k} \sim \N(\vec{0},\mat{R}_{k})$ is the measurement noise on
the time step $k$.
%
\item $\mat{A}_{k-1}$ is the transition matrix of the dynamic model.
%
\item $\mat{H}_k$ is the measurement model matrix.
%
\item The prior distribution for the state is $\vec{x}_{0} \sim \N(\vec{m}_0,\mat{P}_0)$,
where parameters $\vec{m}_0$ and $\mat{P}_0$ are set using the information
known about the system under the study.
%
\end{itemize}

The model can also be equivalently expressed in probabilistic terms with distributions
%
\begin{equation}
\begin{split}
  p(\vec{x}_{k}\,|\,\vec{x}_{k-1})
  &= \N(\vec{x}_{k}\,|\,\mat{A}_{k-1} \, \vec{x}_{k-1}, \mat{Q}_{k-1}) \\
  p(\vec{y}_{k}\,|\,\vec{x}_{k})
  &= \N(\vec{y}_{k}\,|\,\mat{H}_{k}   \, \vec{x}_{k}  , \mat{R}_{k}).
\end{split}
\label{eq:kalman_model2}
\end{equation}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Discretization of continuous-time linear time-invariant systems}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Often many linear time-invariant models are described with continous-time state
equations of the following form:
%
\begin{equation}
%
\frac{d \vec{x}(t)}{d t} = \mat{F} \vec{x}(t) + \mat{L} \vec{w}(t),
%
\label{eq:lti_cont_model}
%
\end{equation}
%
where 
%
\begin{itemize}
%
\item the initial conditions are $\vec{x}(0) \sim \N(\vec{m}(0),\mat{P}(0))$,
%
\item $\mat{F}$ and $\mat{L}$ are constant matrices, which characterize the
behaviour of the model,
%
%\item and $\vecbeta(t)$ is a Brownian motion with constant diffusion matrix $\mat{Q}_c$.
\item $\vec{w}(t)$ is a white noise process with a power spectral density $\mat{Q}_c$.
%
\end{itemize}

To be able to use the Kalman filter defined in the next section the model
(\ref{eq:lti_cont_model}) must be discretized somehow, so that it can be described
with a model of the form (\ref{eq:kalman_model}). The solution
for the discretized matrices $\mat{A}_k$ and $\mat{Q}_k$ can be given
as (e.g. Särkkä, 2006b; Bar-Shalom et al, 2001)
%
\begin{align}
\mat{A}_k &= \exp(\mat{F} \, \Delta t_k) \label{eq:ode_ak} \\
\mat{Q}_k &=
  \int_{0}^{\Delta t_k} \exp( \mat{F} \, (\Delta t_k - \tau)) \, \mat{L} \,
  \mat{Q}_c \, \mat{L}^T \, \exp( \mat{F} \, (\Delta t_k - \tau))^T \diff \tau,
\label{eq:ode_qk}
\end{align}
%
where $\Delta t_k = t_{k+1} - t_k$ is the stepsize of the discretization. In
some cases the $\mat{Q}_k$ can be calculated analytically, but in cases where
it isn't possible, the matrix can still be calculated efficiently using the
following matrix fraction decomposition:
%
%
  \begin{equation}
  \left( \begin{array}{c}
    \mat{C}_k \\
    \mat{D}_k
  \end{array} \right) =
  \exp \left\{
  \left( \begin{array}{cc}
     \mat{F} & \mat{L} \, \mat{Q}_c \, \mat{L}^T \\
     \mat{0} & -\mat{F}^T
  \end{array} \right) \Delta t_k \right\}
  \left( \begin{array}{c}
    \mat{0} \\
    \mat{I}
  \end{array} \right).
  \label{eq:noisy_ltisolution_cov1}
  \end{equation}
  %
The matrix $\mat{Q}_k$ is then given as $\mat{Q}_k = \mat{C}_k \mat{D}_k^{-1}$.

In this toolbox the discretization can be done with the
function \texttt{lti\_disc} (see page \pageref{page:lti_disc}), which
uses the matrix fractional decomposition.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Kalman filter}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The classical Kalman filter was first introduced by Rudolph E. Kalman in his
seminal paper (Kalman, 1960). The purpose of the discrete-time Kalman filter
is to provide the closed form recursive solution for estimation of linear discrete-time dynamic
systems, which can be described by equations of the form (\ref{eq:kalman_model}).

Kalman filter has two steps: the prediction step, where the next state of the
system is predicted given the previous measurements, and the update step, where
the current state of the system is estimated  given the measurement at that time
step. The steps translate to equations as follows (see, e.g., Särkkä, 2006b,
Bar-Shalom et al.,2001, for derivation):

\begin{itemize}
\item {\em Prediction:}
%
\begin{equation}
\begin{split}
  \vec{m}^-_{k} &= \mat{A}_{k-1} \, \vec{m}_{k-1} \\
  \mat{P}^-_{k} &= \mat{A}_{k-1} \, \mat{P}_{k-1} \, \mat{A}_{k-1}^T
                 + \mat{Q}_{k-1}.
\end{split}
\label{eq:dkf_predict}
\end{equation}
%
\item {\em Update:} 
%
\begin{equation}
\begin{split}
  \vec{v}_{k} &= \vec{y}_k - \mat{H}_{k} \, \vec{m}^-_{k} \\
  \vec{S}_{k} &= \mat{H}_{k} \, \mat{P}^-_{k} \, \mat{H}_{k}^T + \vec{R}_{k} \\
  \vec{K}_{k} &= \mat{P}^-_{k} \, \mat{H}^T_{k} \, \vec{S}^{-1}_{k} \\
  \vec{m}_{k} &= \vec{m}^-_{k} + \vec{K}_{k} \, \vec{v}_k \\
  \mat{P}_{k} &= \mat{P}^-_{k} - \vec{K}_{k} \, \vec{S}_{k} \, \vec{K}^T_{k}, \\
\end{split}
\label{eq:dkf_update}
\end{equation}
\end{itemize}
%
where
%
\begin{itemize}
%
\item $\vec{m}^-_{k}$ and $\mat{P}^-_{k}$ are the predicted mean and covariance of the state,
respectively,  on the time step $k$ before seeing the measurement.
%
\item $\vec{m}_{k}$ and $\mat{P}_{k}$ are the estimated mean and covariance of
the state, respectively, on time step $k$ after seeing the measurement.

\item $\vec{v}_k$ is the innovation or the measurement residual on time step $k$.
%
\item $\mat{S}_k$ is the measurement prediction covariance on the time step $k$.
%
\item $\mat{K}_k$ is the filter gain, which tells how much the predictions should be corrected on time step $k$.
%
\end{itemize}
%
Note that in this case the predicted and estimated state covariances on different time steps do not
depend on any measurements, so that they could be calculated off-line before making any measurements
provided that the matrices $\mat{A}$, $\mat{Q}$, $\mat{R}$ and $\mat{H}$ are known on those
particular time steps. Usage for this property, however, is not currently provided explicitly
with this toolbox.

It is also possible to predict the state of system as many steps ahead as wanted just by looping
the predict step of Kalman filter, but naturally the accuracy of the estimate decreases with every
step.

The prediction and update steps can be calculated with functions \texttt{kf\_predict}
and \texttt{kf\_update}, described on pages \pageref{page:kf_predict} and \pageref{page:kf_update},
respectively.

\subsubsection{Kalman smoother}

The discrete-time Kalman smoother, also known as the Rauch-Tung-Striebel-smoother (RTS), 
(Rauch et al., 1965; Gelb, 1974; Bar-Shalom et al., 2001) can be used for computing
the smoothing solution for the model (\ref{eq:kalman_model}) given as distribution
%
\begin{equation}
  p(\vec{x}_{k}\,|\,\vec{y}_{1:T}) =
    \N(\vec{x}_{k}\,|\,\vec{m}^s_{k},\mat{P}^s_{k}).
\end{equation}
%
The mean and covariance $\vec{m}^s_{k}$ and $\mat{P}^s_{k}$ are calculated with the following equations:
%
\begin{equation}
\begin{split}
    \vec{m}^-_{k+1} &= \mat{A}_k \, \vec{m}_k  \\
    \mat{P}^-_{k+1} &= \mat{A}_k \, \mat{P}_k \, \mat{A}_k^T + \mat{Q}_k \\
    \vec{C}_{k} &= \mat{P}_k \, \mat{A}_k^T \, [\mat{P}^-_{k+1}]^{-1} \\
    \vec{m}^s_k &= \vec{m}_k
    + \mat{C}_k \, [\vec{m}^s_{k+1} - \vec{m}^-_{k+1}] \\
    \mat{P}^s_k &= \mat{P}_k
    + \mat{C}_k \, [\mat{P}^s_{k+1} - \mat{P}^-_{k+1}] \, \mat{C}^T_k,
\end{split}
\label{eq:kfrts}
\end{equation}
%
where
%
\begin{itemize}
%
\item $\vec{m}^s_k$ and $\mat{P}^s_k$ are the smoother estimates for the state mean and state
covariance on time step $k$.
%
\item $\vec{m}_k$ and $\mat{P}_k$ are the filter estimates for the state mean and state 
covariance on time step $k$.
%
\item $\vec{m}^-_{k+1}$ and $\mat{P}^-_{k+1}$ are the predicted state mean and state covariance
on time step $k+1$, which are the same as in the Kalman filter.
%
\item $\mat{C}_k$ is the smoother gain on time step $k$, which tells how much the smooted estimates
should be corrected on that particular time step.
%
\end{itemize}
%
The difference between Kalman filter and Kalman smoother is that the recursion in filter moves
forward and in smoother backward, as can be seen from the equations above. In smoother the recursion
starts from the last time step T with $\vec{m}^s_T = \vec{m}_T$ and $\mat{P}^s_T = \mat{P}_T$. 

The smoothed estimate for states and covariances using the RTS smoother can be
calculated with the function \texttt{rts\_smooth}, which is described on
page \pageref{page:rts_smooth}.

In addition to RTS smoother it is possible to formulate the smoothing operation as a combination
of two optimum filters (Fraser and Potter, 1969), of which the first filter sweeps the data forward going from the
first measurement towards the last one, and the second sweeps backwards towards the opposite direction.
It can be shown, that combining the estimates produced by these two filters in a suitable way produces
an smoothed estimate for the state, which has lower mean square error than any of these two filters alone
(Gelb, 1974). With linear models the forward-backward smoother gives the same error as the RTS-smoother,
but in non-linear cases the error behaves differently in some
situations. In this toolbox forward-backward smoothing solution can be
calculated with function \texttt{tf\_smooth} (see page \pageref{page:tf_smooth}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Demonstration: 2D CWPA-model}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let's now consider a very simple case, where we track an object moving
in two dimensional space with a sensor, which gives measurements of
target's position in cartesian coordinates $x$ and $y$. In addition to
position target also has state variables for its velocities and accelerations toward both coordinate
axes, $\dot{x}$, $\dot{y}$, $\ddot{x}$ and $\ddot{y}$. In other words, the state of a
moving object on time step $k$ can be expressed as a vector
%
\begin{equation}
\vec{x}_k = 
\begin{pmatrix}
x_k & y_k & \dot{x}_k & \dot{y}_k & \ddot{x}_k & \ddot{y}_k
\end{pmatrix}^T.
%
\end{equation}
%
In continous case the dynamics of the target's motion can be modelled as a linear, time-invariant system
%
\begin{equation}
%
\frac{d \vec{x}(t)}{dt} = \underbrace{\begin{pmatrix}
0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 
\end{pmatrix}}_{\mat{F}} \vec{x}(t)
+
\underbrace{\begin{pmatrix}
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
1 & 0 \\
0 & 1 \\
\end{pmatrix}}_{\mat{L}}\vec{w}(t), \label{eq:cwpa_cont_model}
%
\end{equation}
%
where $\vec{x}(t)$ is the target's state on the time $t$ and
$\vec{w}(t)$ is a white noise process with power spectral density
%
\begin{equation}
\mat{Q}_c = 
\begin{pmatrix}
q & 0 \\
0   & q
\end{pmatrix}
= 
\begin{pmatrix}
0.2 & 0 \\
0   & 0.2
\end{pmatrix}.
\end{equation}
%
As can be seen from the equation the acceleration of the object is perturbed with a white noise process
and hence this model has the name continous Wiener process acceleration (CWPA) model. There is also
other models similar to this, as for example the continous white noise acceleration (CWNA) model (Bar-Shalom et al., 2001),
where the velocity is perturbed with a white noise process.

The measurement matrix is set to
%
\begin{equation}
%
\mat{H} = \begin{pmatrix}
1 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 \\
\end{pmatrix},
%
\end{equation}
%
which means that the observe only the position of the moving object.
To be able to estimate this system with a discrete-time Kalman filter the differential equation defined
above must be discretized somehow to get a discrete-time state equation of the form (\ref{eq:kalman_model}). It
turns out, that the matrices $\mat{A}$ and $\mat{Q}$ can be calculated analytically with equations (\ref{eq:ode_ak})
and (\ref{eq:ode_qk}) to give the following:
%
\begin{align}
%
\mat{A}  = \begin{pmatrix}
1 & 0 & \dt & 0    & \frac{1}{2}\dt^2    & 0 \\
0 & 1 & 0    & \dt & 0    & \frac{1}{2}\dt^2\\
0 & 0 & 1    & 0    & \dt & 0 \\
0 & 0 & 0    & 1    & 0    & \dt \\
0 & 0 & 0    & 0    & 1    & 0 \\
0 & 0 & 0    & 0    & 0    & 1 \\ 
\end{pmatrix}  \\
\mat{Q}  =   \begin{pmatrix}
\frac{1}{20}\dt^5 & 0 & \frac{1}{8}\dt^4 & 0 & \frac{1}{6}\dt^3 & 0 \\
0 & \frac{1}{20}\dt^5 & 0 & \frac{1}{8}\dt^4 & 0 & \frac{1}{6}\dt^3 \\
\frac{1}{8}\dtk4 & 0 & \frac{1}{6}\dt^3 & 0 &\frac{1}{2}\dt^2  & 0 \\
0 & \frac{1}{8}\dt^4 & 0 & \frac{1}{6}\dt^3 & 0 & \frac{1}{2}\dt^2 \\
\frac{1}{6}\dt^3 & 0 & \frac{1}{2}\dt^2 & 0 & \dt & 0 \\
0 & \frac{1}{6}\dt^3 & 0 & \frac{1}{2}\dt^2 & 0 & \dt 
\end{pmatrix}q,
%
\end{align}
%
where the stepsize is set to $\dt = 0.5$. These matrices can also
calculated using the function \texttt{lti\_disc}
introduced in section $2.1$ with the following code line:
%
\begin{verbatim}
[A,Q] = lti_disc(F,L,Qc,dt);
\end{verbatim}
% 
where matrices \texttt{F} and \texttt{L} are assumed to contain the matrices from equation
(\ref{eq:cwpa_cont_model}). 

The object starts from origo with zero velocity and acceleration
and the process is simulated 50 steps. The variance for the measurements is set to
%
\begin{equation}
\mat{R} = \begin{pmatrix}
10 & 0  \\
0  & 10 \\
\end{pmatrix},
\end{equation}
%
which is relatively high so that the the difference between the filtering and
smoothing (described in next section) estimates becomes clearer. The real
position of the object and measurements of it are plotted in the figure
\ref{fig:example1_1}. 

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/demo1_f1}
\caption{The real position of the moving object and the simulated
  measurements of it using the CWPA model. The circle marks the
  starting point of the object.}
\label{fig:example1_1}
\end{center}
\end{figure}

The filtering is done with the following code fragment:
%
\begin{verbatim}
MM = zeros(size(m,1), size(Y,2));
PP = zeros(size(m,1), size(m,1), size(Y,2));

for i = 1:size(Y,2)
   [m,P] = kf_predict(m,P,A,Q);
   [m,P] = kf_update(m,P,Y(:,i),H,R);
   MM(:,i) = m;
   PP(:,:,i) = P;
end
\end{verbatim}
%
In the first 2 lines the space for state mean and covariance estimates is
reserved, and the rest of the code contains the actual filtering loop, where
we make the predict and update steps of the Kalman filter. The variables
\texttt{m} and \texttt{P} are assumed to contain the initial guesses for the
state mean and covariance before reaching the for-statement. Variable
\texttt{Y} is assumed to contain the measurements made from the system
(See the full source code of the example (\texttt{kf\_cwpa\_demo.m}) provided with the toolbox
to see how we generated the measurements by simulating the dynamic system). In
the end of each iteration the acquired estimates are stored to matrices
\texttt{MM} and \texttt{PP}, for which we reserved space earlier. The estimates
for object's position and velocity with Kalman filter and are plotted in figure
\ref{fig:example1_2}.\\

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/demo1_f2}
\caption{Estimates for position and velocity of the moving object
  using the Kalman filter and CWPA model.}
\label{fig:example1_2}
\end{center}
\end{figure}

The smoothed estimates for the state mean and covariance can be calculated with the
following code line:
%
\begin{verbatim}
[SM,SP] = rts_smooth(MM,PP,A,Q);
\end{verbatim}
%
The calculated smoothed estimates for object's position and velocity for the
earlier demonstration are plotted in figure \ref{fig:example1_3}. As expected
the smoother produces more accurate estimates than the filter as it uses all
measurements for the estimation each time step. Note that the difference between
the smoothed and filtered estimated would be smaller, if the measurements were
more accurate, as now the filter performs rather badly due to the great
uncertaintity in the measurements. The smoothing results of a
forward-backward smoother are not plotted here, as the result are
exactly the same as with the RTS smoother. 

As one would expect the estimates for object's velocity are clearly less
accurate than the estimates for the object's position as the positions are
observed directly and the velocities only indirectly. If the
velocities were also observed not only the velocity estimates would get
more accurate, but also the position ones as well. 

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/demo1_f3}
\caption{Estimate for position and velocity of the moving object using the RTS smoother and CWPA model.}
\label{fig:example1_3}
\end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Nonlinear state space estimation}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In many cases interesting dynamic systems are not linear by nature, so the
traditional Kalman filter cannot be applied in estimating the state of such a
system. In these kind of systems, both the dynamics and the measurement processes
can be nonlinear, or only one them. In this section, we describe two extensions
to the traditional Kalman filter, which can be applied for estimating nonlinear
dynamical systems by forming Gaussian approximations to the joint distribution of
the state $\vec{x}$ and measurement $\vec{y}$. First we present the Extended Kalman
filter (EKF), which is based on Taylor series approximation of the joint
distribution, and then the Unscented Kalman filter (UKF), which is
respectively based on the unscented transformation of the joint distribution.      


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\subsubsection{Taylor Series Based Approximations}

Next we present linear and quadratic approximations for the
distribution of variable $\vec{y}$, which is generated with a non-linear transformation
of a Gaussian random variable $\vec{x}$ as follows:
%
\begin{equation}
\begin{split}
  \vec{x} &\sim \N(\vec{m},\mat{P}) \\
  \vec{y} &= \vec{g}(\vec{x}),
\end{split}
\label{eq:trans_g}
\end{equation}
%
where $\vec{x} \in \spc{R}^n$, $\vec{y} \in \spc{R}^m$, and $\vec{g} :
\spc{R}^n \mapsto \spc{R}^m$ is a general non-linear function. Solving
the distribution of $\vec{y}$ formally is in general not possible,
because it is non-Gaussian for all by linear $\vec{g}$, so in practice
it must be approximated somehow. The joint distribution of $\vec{x}$
and $\vec{y}$ can be formed with, for example, linear and quadratic
approximations, which we present next. See, for example, (Bar-Shalom
et al., 2001) for the derivation of these approximations.

\subsubsection*{Linear Approximation}

The linear approximation based Gaussian approximation of the joint
distribution of variables $\vec{x}$ and $\vec{y}$ defined by equations (\ref{eq:trans_g})
is given as
  %
  \begin{equation}
     \begin{pmatrix}
       \vec{x} \\ \vec{y}
     \end{pmatrix} \sim
     \N\left(
     \begin{pmatrix}
       \vec{m} \\ \vecmu_L
     \end{pmatrix},
     \begin{pmatrix}
       \vec{P}   & \vec{C}_L \\
       \vec{C}_L^T & \mat{S}_L
     \end{pmatrix} \right),
  \end{equation}
  %
  where
  %
  \begin{equation}
  \begin{split}
  \vecmu_L &= \vec{g}(\vec{m}) \\
    \mat{S}_L &= \mat{G}_{\vec{x}}(\vec{m}) \, \mat{P} \,
    \mat{G}^T_{\vec{x}}(\vec{m}) \\
    \mat{C}_L &= \mat{P} \, \mat{G}^T_{\vec{x}}(\vec{m}),
  \end{split}
  \end{equation}
  %
  and $\mat{G}_{\vec{x}}(\vec{m})$ is the Jacobian matrix of $\vec{g}$
  with elements
  %
  \begin{equation}
    \left[ \mat{G}_{\vec{x}}(\vec{m}) \right]_{j,j'} =
    \frac{\partial g_j(\vec{x})}{\partial x_{j'}}
    \Bigg|_{\vec{x} = \vec{m}}.
  \label{eq:G1}
  \end{equation}

\subsubsection*{Quadratic Approximation}

The quadratic approximations retain also the second order terms of the
Taylor series expansion of the non-linear function:
  %
  \begin{equation}
     \begin{pmatrix}
       \vec{x} \\ \vec{y}
     \end{pmatrix} \sim
     \N\left(
     \begin{pmatrix}
       \vec{m} \\ \vecmu_Q
     \end{pmatrix},
     \begin{pmatrix}
       \vec{P}   & \vec{C}_Q \\
       \vec{C}_Q^T & \mat{S}_Q
     \end{pmatrix} \right),
  \end{equation}
  %
  where the parameters are
  %
  \begin{equation}
  \begin{split}
  \vecmu_Q &= \vec{g}(\vec{m}) 
  + \frac{1}{2} \sum_i \vec{e}_i \, \tr\left\{
     \mat{G}_{\vec{x}\vec{x}}^{(i)}(\vec{m}) \,
    \mat{P} \right\} \\
    \mat{S}_Q &= \mat{G}_{\vec{x}}(\vec{m}) \, \mat{P} \, \mat{G}^T_{\vec{x}}(\vec{m})
  + \frac{1}{2} \sum_{i,i'} \vec{e}_i \, \vec{e}^T_{i'} \,
    \tr\left\{ \mat{G}_{\vec{x}\vec{x}}^{(i)}(\vec{m}) \, \mat{P} \,
     \mat{G}_{\vec{x}\vec{x}}^{(i')}(\vec{m}) \, \mat{P} \right\} \\
    \mat{C}_Q &= \mat{P} \, \mat{G}^T_{\vec{x}}(\vec{m}),
  \end{split}
  \end{equation}
%
  $\mat{G}_{\vec{x}}(\vec{m})$ is the Jacobian matrix \eqref{eq:G1}
  and $\mat{G}_{\vec{x}\vec{x}}^{(i)}(\vec{m})$ is the Hessian matrix
  of $g_i(\cdot)$ evaluated at $\vec{m}$:
%
\begin{equation}
   \left[ \mat{G}_{\vec{x}\vec{x}}^{(i)}(\vec{m}) \right]_{j,j'}
   = \frac{\partial^2 g_i(\vec{x})}{\partial x_j \, \partial x_{j'}},
  \Bigg|_{\vec{x} = \vec{m}}.
\end{equation}
%
$\vec{e}_i = (0~\cdots~0~1~0~\cdots~0)^T$ is the unit
vector in direction of the coordinate axis $i$.



\subsubsection{Extended Kalman filter}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The extended Kalman filter (see, for instance, Jazwinski, 1970; Maybeck, 1982a;
Bar-Shalom et al., 2001; Grewal and Andrews, 2001; Särkkä, 2006) extends the scope
of Kalman filter to nonlinear optimal filtering problems by forming a Gaussian
approximation to the joint distribution of state $\vec{x}$ and measurements
$\vec{y}$ using a Taylor series based transformation. First and second order
extended Kalman filters are presented, which are based on linear and quadratic
approximations to the transformation. Higher order filters are also possible,
but not presented here.


The filtering model used in the EKF is
%
\begin{equation}
\begin{split}
\vec{x}_{k} &= \vec{f}(\vec{x}_{k-1},k-1) + \vec{q}_{k-1} \\
\vec{y}_{k} &= \vec{h}(\vec{x}_{k},k) + \vec{r}_{k},
\end{split} \label {eq:nonlinear_prob}
\end{equation}
%
where $\vec{x}_k \in \spc{R}^n$ is the state, $\vec{y}_k \in \spc{R}^m$
is the measurement,  $\vec{q}_{k-1} \sim \N(\vec{0},\mat{Q}_{k-1})$ is
the process noise, $\vec{r}_{k} \sim \N(\vec{0},\mat{R}_{k})$ is the
measurement noise, $\vec{f}$ is the (possibly nonlinear) dynamic model
function and $\vec{h}$ is the (again possibly nonlinear) measurement model
function. The first and second order extended Kalman filters approximate the
distribution of state $\vec{x}_k$ given the observations $\vec{y}_{1:k}$
with a Gaussian:
%
\begin{equation}
  p(\vec{x}_{k}\,|\,\vec{y}_{1:k}) \approx
    \N(\vec{x}_{k}\,|\,\vec{m}_{k},\mat{P}_{k}).
\end{equation}
%

\subsubsection*{First Order Extended Kalman Filter}

Like Kalman filter, also the extended Kalman filter is separated to two steps.
The steps for the first order EKF are as follows:
  %
  \begin{itemize}
  \item {\em Prediction:}
  %
  \begin{equation}
  \begin{split}
    \vec{m}^-_{k} &= \vec{f}(\vec{m}_{k-1},k-1) \\
    \mat{P}^-_{k} &= \mat{F}_{\vec{x}}(\vec{m}_{k-1},k-1) \, \mat{P}_{k-1} \,
     \mat{F}_{\vec{x}}^T (\vec{m}_{k-1},k-1) + \mat{Q}_{k-1}.
  \end{split}
  \label{eq:dekf_predict1}
  \end{equation}
  
  \item {\em Update:}
  %
  \begin{equation}
  \begin{split}
    \vec{v}_{k} &= \vec{y}_k - \mat{h} (\vec{m}^-_{k},k) \\
    \vec{S}_{k} &= \mat{H}_{\vec{x}}(\vec{m}^-_{k},k) \, \mat{P}^-_{k} \,
    \mat{H}^T_{\vec{x}}(\vec{m}^-_{k},k) + \vec{R}_{k} \\
    \vec{K}_{k} &= \mat{P}^-_{k} \, \mat{H}^T_{\vec{x}}(\vec{m}^-_{k},k) \,
    \vec{S}^{-1}_{k} \\
    \vec{m}_{k} &= \vec{m}^-_{k} + \vec{K}_{k} \, \vec{v}_k \\
    \mat{P}_{k} &= \mat{P}^-_{k} - \vec{K}_{k} \, \vec{S}_{k} \, \vec{K}^T_{k},
  \end{split}
  \label{eq:dekf_update1}
  \end{equation}
  \end{itemize}
  %
  where the matrices $\mat{F}_{\vec{x}}(\vec{m},k-1)$ and
  $\mat{H}_{\vec{x}}(\vec{m},k)$ are the Jacobians of
  $\vec{f}$ and $\vec{h}$, with elements
  %
  \begin{align}
    \left[ \mat{F}_{\vec{x}}(\vec{m},k-1) \right]_{j,j'} =
    \frac{\partial f_j(\vec{x},k-1)}{\partial x_{j'}}
    \Bigg|_{\vec{x} = \vec{m}}
  \label{eq:F1} \\
    \left[ \mat{H}_{\vec{x}}(\vec{m},k) \right]_{j,j'} =
    \frac{\partial h_j(\vec{x},k)}{\partial x_{j'}}
    \Bigg|_{\vec{x} = \vec{m}}.
  \label{eq:H1}
  \end{align}
%
Note that the difference between first order EKF and KF is that the
matrices $\mat{A}_k$ and $\mat{H}_k$ in KF are replaced with Jacobian
matrices $\mat{F}_{\vec{x}}(\vec{m}_{k-1},k-1)$ and
$\mat{H}_{\vec{x}}(\vec{m}_k^-,k)$ in EKF. Predicted mean
$\vec{m}_k^-$ and residual of prediction $\vec{v}_k$ are also calculated differently in the EKF.  
In this toolbox the prediction and update steps of the first order EKF can be computed
with functions \texttt{ekf\_predict1} and \texttt{ekf\_update1} (see
page \pageref{page:ekf_predict1}), respectively. 

\subsubsection*{Second Order Extended Kalman Filter}

The corresponding steps for the second order EKF are as follows:
  %
  \begin{itemize}
  \item {\em Prediction:}
  %
  \begin{equation}
  \begin{split}
    \vec{m}^-_{k} &= \vec{f}(\vec{m}_{k-1},k-1) 
    + \frac{1}{2} \sum_i \vec{e}_i \,
      \tr\left\{ \mat{F}_{\vec{x}\vec{x}}^{(i)}(\vec{m}_{k-1},k-1) \,
      \mat{P}_{k-1} \right\} \\
    \mat{P}^-_{k} &= \mat{F}_{\vec{x}}(\vec{m}_{k-1},k-1) \, \mat{P}_{k-1} \,
     \mat{F}^T_{\vec{x}}(\vec{m}_{k-1},k-1) \\
   &+ \frac{1}{2} \sum_{i,i'} \vec{e}_i \, \vec{e}^T_{i'}
      \tr\left\{ \mat{F}_{\vec{x}\vec{x}}^{(i)}(\vec{m}_{k-1},k-1) \mat{P}_{k-1} 
       \mat{F}_{\vec{x}\vec{x}}^{(i')}(\vec{m}_{k-1},k-1) \mat{P}_{k-1} \right\} \\
    &+ \mat{Q}_{k-1}.
  \end{split}
  \label{eq:dekf_predict2}
  \end{equation}

  \item {\em Update:}
  %
  \begin{equation}
  \begin{split}
    \vec{v}_{k} &= \vec{y}_k - \mat{h} (\vec{m}^-_{k},k) 
    - \frac{1}{2} \sum_i \vec{e}_i \,
      \tr\left\{ \mat{H}_{\vec{x}\vec{x}}^{(i)}(\vec{m}^-_{k},k) \,
      \mat{P}^-_{k} \right\} \\
    \vec{S}_{k} &= \mat{H}_{\vec{x}}(\vec{m}^-_{k},k) \, \mat{P}^-_{k} \,
    \mat{H}^T_{\vec{x}}(\vec{m}^-_{k},k) \\
    &\quad + \frac{1}{2} \sum_{i,i'} \vec{e}_i \, \vec{e}^T_{i'} \,
      \tr\left\{ \mat{H}_{\vec{x}\vec{x}}^{(i)}(\vec{m}^-_{k},k) \, \mat{P}^-_{k} \,
       \mat{H}_{\vec{x}\vec{x}}^{(i')}(\vec{m}^-_{k},k) \, \mat{P}^-_{k} \right\} 
    + \vec{R}_{k} \\
    \vec{K}_{k} &= \mat{P}^-_{k} \,
    \mat{H}^T_{\vec{x}}(\vec{m}^-_{k},k) \, \vec{S}^{-1}_{k} \\
    \vec{m}_{k} &= \vec{m}^-_{k} + \vec{K}_{k} \, \vec{v}_k \\
    \mat{P}_{k} &= \mat{P}^-_{k} - \vec{K}_{k} \, \vec{S}_{k} \, \vec{K}^T_{k},
  \end{split}
  \label{eq:dekf_update2}
  \end{equation}
  \end{itemize}
  %
  where matrices $\mat{F}_{\vec{x}}(\vec{m},k-1)$ and
  $\mat{H}_{\vec{x}}(\vec{m},k)$ are Jacobians as in the first order EKF, given by
  Equations \eqref{eq:F1} and \eqref{eq:H1}. The matrices
  $\mat{F}^{(i)}_{\vec{x}\vec{x}}(\vec{m},k-1)$ and
  $\mat{H}^{(i)}_{\vec{x}\vec{x}}(\vec{m},k)$ are the Hessian matrices
  of $f_i$ and $h_i$:
  \begin{align}
   \left[ \mat{F}_{\vec{x}\vec{x}}^{(i)}(\vec{m},k-1) \right]_{j,j'}
   = \frac{\partial^2 f_i(\vec{x},k-1)}{\partial x_j \, \partial x_{j'}}
  \Bigg|_{\vec{x} = \vec{m}}
  \label{eq:F2} \\
   \left[ \mat{H}_{\vec{x}\vec{x}}^{(i)}(\vec{m},k) \right]_{j,j'}
   = \frac{\partial^2 h_i(\vec{x},k)}{\partial x_j \, \partial x_{j'}}
  \Bigg|_{\vec{x} = \vec{m}},
  \label{eq:H2}
  \end{align}
%
$\vec{e}_i = (0~\cdots~0~1~0~\cdots~0)^T$ is a unit vector in direction of the
coordinate axis $i$, that is, it has a $1$ at position $i$ and $0$ at other positions.

The prediction and update steps of the second order EKF can be computed
in this toolbox with functions \texttt{ekf\_predict2} and
\texttt{ekf\_update2}, respectively. By taking the second order terms
into account, however, doesn't quarantee, that the results get any
better. Depending on problem they might even get worse, as we shall
see in the later examples.
\subsubsection*{The Limitations of EKF}

As discussed in, for example, (Julier and Uhlmann, 2004) the EKF has a
few serious drawbacks, which should be kept in mind when it's used:
%
\begin{enumerate}
\item As we shall see in some of the later demonstrations, the linear
  and quadratic transformations produces realiable results only
  when the error propagation can be well approximated by a linear or a
  quadratic function. If this condition is not met, the performance of the 
  filter can be extremely poor. At worst, its estimates can diverge altogether.
\item The Jacobian matrices (and Hessian matrices with second order
  filters) need to exist so that the transformation can be
  applied. However, there are cases, where this isn't true. For
  example, the system might be jump-linear, in which the parameters can
  change abruptly (Julier and Uhlmann, 2004). 
\item In many cases the calculation of Jacobian and Hessian matrices can be a very
  difficult process, and its also prone to human errors (both
  derivation and programming). These errors are usually very hard to
  debug, as its hard to see which parts of the system produces the
  errors by looking at the estimates, especially as
  usually we don't know which kind of performance we should
  expect. For example, in the last demonstration (Reentry Vehicle
  Tracking) the first order derivatives were quite troublesome to
  calcute, even though the equations themselves were relatively
  simple. The second order derivatives would have even taken many more
  times of work.  
\end{enumerate}
%
\label{page:ekf_problems}


  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Extended Kalman smoother}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The difference between the first order extended Kalman smoother (Cox, 1964;
Sage and Melsa, 1971) and the traditional Kalman smoother is the same as the
difference between first order EKF and KF, that is, matrix $\mat{A}_k$ in
Kalman smoother is replaced with Jacobian
$\mat{F}_{\vec{x}}(\vec{m}_{k-1},k-1)$, and $\vec{m}_{k+1}^-$ is
calculated using the model function $\vec{f}$. 
Thus, the equations for the extended Kalman smoother can be written as
%
\begin{equation}
\begin{split}
    \vec{m}^-_{k+1} &= \vec{f}(\vec{m}_{k},k) \\
    \mat{P}^-_{k+1} &= \mat{F}_{\vec{x}}(\vec{m}_{k},k) \, \mat{P}_{k} \,
     \mat{F}_{\vec{x}}^T(\vec{m}_{k},k) + \mat{Q}_{k} \\
    \vec{C}_{k} &= \mat{P}_k \, \mat{F}_{\vec{x}}^T(\vec{m}_{k},k) \,
      [\mat{P}^-_{k+1}]^{-1} \\
    \vec{m}^s_k &= \vec{m}_k
    + \mat{C}_k \, [\vec{m}^s_{k+1} - \vec{m}^-_{k+1}] \\
    \mat{P}^s_k &= \mat{P}_k
    + \mat{C}_k \, [\mat{P}^s_{k+1} - \mat{P}^-_{k+1}] \, \mat{C}^T_k.
    \label{eq:derts}
\end{split}
\end{equation}
%
First order smoothing solutiong with a RTS type smoother can be computed with function
\texttt{erts\_smooth1}, and with forward-backward type smoother the
computation can be done with function \texttt{etf\_smooth1}.

Higher order smoothers are also possible, but not described here, as they are not
currently implemented in this toolbox.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Demonstration: Tracking a random sine signal}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Next we consider a simple, yet practical, example of a nonlinear dynamic
system, in which we estimate a random sine signal using the extended
Kalman filter. By random we mean that the angular velocity and the amplitude
of the signal can vary through time. In this example the nonlinearity in the
system is expressed through the measurement model, but it would also be possible
to express it with the dynamic model and let the measurement model be linear.

The state vector in this case can be expressed as
%
\begin{equation}
%
\vec{x}_k = 
\begin{pmatrix}
\theta_k & \omega_k & a_k
\end{pmatrix}^T,
%
\end{equation}
%
where $\theta_k$ is the parameter for the sine function on time step $k$,
$\omega_k$ is the angular velocity on time step $k$ and $a_k$ is the amplitude
on time step $k$. The evolution of parameter $\theta$ is modelled with a
discretized Wiener velocity model, where the velocity is now the angular velocity:
%
\begin{equation}
%
\frac{d \theta}{d t} = \omega.
%
\end{equation}
%
The values of $\omega$ and $a$ are perturbed with one dimensional white noise
processes $w_a(t)$ and $w_w(t)$, so the signal
isn't totally deterministic:
%
\begin{eqnarray}
%
\frac{da}{dt} &=& w_a(t) \\
\frac{dw}{dt} &=& w_w(t).
%
\end{eqnarray}
%
Thus, the continous-time dynamic equation can be
written as
%
\begin{equation}
%
\frac{d \vec{x}(t)}{dt} = \begin{pmatrix}
0 & 1 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\end{pmatrix}\vec{x}(t)
+
\begin{pmatrix}
0 & 0 \\
1 & 0 \\
0 & 1
\end{pmatrix}\vec{w}(t), \label{eq:sine_cont_model}
%
\end{equation}
%
where the white noise process $\vec{w}(t)$ has power spectral density
%
\begin{equation}
%
\mat{Q}_c =  \begin{pmatrix}
q_1 & 0 \\
0 & q_2 
\end{pmatrix}.
%
\end{equation}
%
Variables $q_1$ and $q_2$ describe the strengths of random perturbations of the angular
velocity and the amplitude, respectively, which are in this demonstration
are set to $q_1 = 0.2$ and $q_2 = 0.1$. By using the equation (\ref{eq:ode_ak})
the discretized form of the dynamic equation can written as
%
\begin{equation}
%
\vec{x}_k = \begin{pmatrix}
1 & \dt & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}\vec{x}_{k-1}
+
\vec{q}_{k-1},
%
\end{equation}
%
where $\dt$ is the step size (with value $\dt = 0.01$ in this case),
and using the equation (\ref{eq:ode_qk}) the
covariance matrix $\mat{Q}_{k-1}$ of the discrete Gaussian white noise process
$\vec{q}_{k-1} \sim N(\vec{0},\mat{Q}_{k-1})$ can be easily computed
to give 
%
\begin{equation}
%
\mat{Q}_{k-1} = \begin{pmatrix}
\frac{1}{3}\dt^3q_1 & \frac{1}{2}\dt^2q_1 & 0\\
\frac{1}{2}\dt^2q_1 & \dt q_1             & 0\\
0                   & 0                   & \dt q_2
\end{pmatrix}.
\end{equation}
%

As stated above, the non-linearity in this case is expressed by the measurement
model, that is, we propagate the current state through a non-linear measurement function
$\vec{h}(\vec{x}_{k},k)$ to get an actual measurement. Naturally the function in this
case is the actual sine function
%
\begin{equation}
%
\vec{h}(\vec{x}_{k},k) = a_k\sin(\theta_k). \label{eq:ekf_demo1_h}
%
\end{equation}
%
With this the measurement model can be written as
%
\begin{equation}
%
y_k = \vec{h}(\vec{x}_k,k) + r_k = a_k\sin(\theta_k) + r_k,
%
\end{equation}
%
where $r_k$ is white, univariate Gaussian noise with zero mean and variance
$\sigma_r = 1$.
 
The derivatives of the measurement function with respect to state variables are
%
\begin{equation}
\begin{split}
%
\frac{\partial \vec{h}(\vec{x}_{k},k)}{\partial \theta_k} &= a_k\cos(\theta_k)\\
\frac{\partial \vec{h}(\vec{x}_{k},k)}{\partial \omega_k} &= 0\\
\frac{\partial \vec{h}(\vec{x}_{k},k)}{\partial a_k} &= \sin(\theta_k),
%
\end{split}
\end{equation}
%
so the Jacobian matrix (actually in this case, a vector, as the measurements are only
one dimensional) needed by the EKF can be written as
%
\begin{equation}
%
\mat{H}_{\vec{x}}(\vec{m},k) = \begin{pmatrix}
a_k\cos(\theta_k) & 0 & \sin(\theta_k) \label{eq:ekf_demo1_jacobian}
\end{pmatrix}.
%
\end{equation}
%

We also filter the signal with second order EKF, so we need to
evaluate the Hessian matrix of the measurement model function.
In this case the second order derivatives of $\vec{h}$ with respect to
all state variables can written as

\begin{equation}
\begin{split}
%
\frac{\partial^2 \vec{h}(\vec{x}_{k},k)}{\partial \theta_k \partial \theta_k} &= -a_k\sin(\theta_k)\\
\frac{\partial^2 \vec{h}(\vec{x}_{k},k)}{\partial \theta_k \partial \omega_k} &= 0\\
\frac{\partial^2 \vec{h}(\vec{x}_{k},k)}{\partial \theta_k \partial a_k} &= \cos(\theta_k)\\
\frac{\partial^2 \vec{h}(\vec{x}_{k},k)}{\partial \omega_k \partial \omega_k} &= 0\\
\frac{\partial^2 \vec{h}(\vec{x}_{k},k)}{\partial \omega_k \partial a_k} &= 0\\
\frac{\partial^2 \vec{h}(\vec{x}_{k},k)}{\partial a_k \partial a_k} &= 0.
%
\end{split}
\end{equation}
With these the Hessian matrix can expressed as
%
\begin{equation}
%
\mat{H}_{\vec{xx}}(\vec{m},k) = \begin{pmatrix}
-a_k\sin(\theta_k) & 0 & \cos(\theta_k) \\
0 & 0 & 0 \\
\cos(\theta_k) & 0 & 0  \label{eq:ekf_demo1_hessian}
\end{pmatrix}.
%
\end{equation}
%
Note that as the measurements are only one dimensional we need to
evaluate only one Hessian, and as the expressions are rather simple the computation
of this Hessian is trivial. In case of higher dimensions we would need to evaluate
the Hessian for each dimension separately, which could easily result in high
amount of dense algebra.


In this demonstration program the measurement function
(\ref{eq:ekf_demo1_h}) is computed with the following code:
%
\begin{verbatim}
function Y = ekf_demo1_h(x,param)
f = x(1,:);
a = x(3,:);
Y = a.*sin(f);
if size(x,1) == 7
    Y = Y + x(7,:);
end
\end{verbatim}
%
where the parameter \texttt{x} is a vector containing a single state
value, or a matrix containing multiple state values. It is also
necessary to include the parameter \texttt{param}, which contains the
other possible parameters for the functions (not present in this
case). The last three lines are included for the augmented version of unscented Kalman
filter (UKF), which is described later in this document.
The Jacobian matrix of the measurement function (eq.
(\ref{eq:ekf_demo1_jacobian})) is
computed with the following function:
%
\begin{verbatim}
function dY = ekf_demo1_dh_dx(x, param)
f = x(1,:);
w = x(2,:);
a = x(3,:);
dY = [(a.*cos(f))' zeros(size(f,2),1) (sin(f))'];
\end{verbatim}
%
The Hessian matrix of the measurement function (eq.
\ref{eq:ekf_demo1_hessian}) is computed with the following function:
%
\begin{verbatim}
function df = ekf_sine_d2h_dx2(x,param)
f = x(1);
a = x(3);

df = zeros(1,3,3);
df(1,:,:) = [-a*sin(f) 0 cos(f);
             0         0      0;
             cos(f)    0      0];
\end{verbatim}
% 
These functions are defined in files \texttt{efk\_sine\_h.m},
\texttt{ekf\_sine\_dh\_dx.m} and \texttt{ekf\_sine\_d2h\_dx2.m},
respectively. The handles of these functions are saved in the actual
demonstration script file (\texttt{ekf\_sine\_demo.m}) with the following code lines:
%
\begin{verbatim}
h_func = @ekf_sine_h;
dh_dx_func = @ekf_sine_dh_dx;
d2h_dx2_func = @ekf_sine_d2h_dx2;
\end{verbatim}
%

It is also important to check out that the implementation on calculating
the derivatives is done right, as it is, especially with more complex
models, easy to make errors in the equations. This can be done with function
\texttt{der\_check} (see page \pageref{page:der_check} for more details):
%
\begin{verbatim}
der_check(h_func, dh_dx_func, 1, [f w a]');
\end{verbatim}
%
The third parameter with value \texttt{1} signals that we want to test
the derivative of function's first (and in this case the only) dimension.
Above we have assumed, that the variable \texttt{f} contains the
parameter value for the sine function, \texttt{w} the angular velocity
of the signal and \texttt{a} the amplitude of the signal. 

After we have discretized the dynamic model and generated the real states and
measurements same as in the previous example (the actual code lines
are not stated here, see the full source code at end of this
document), we can use the EKF to get the filtered estimates for the
state means and covariances. The filtering (with first order EKF) is done almost the same as
in the previous example:
%
\begin{verbatim}
  MM = zeros(size(M,1),size(Y,2));
  PP = zeros(size(M,1),size(M,1),size(Y,2));

  for k=1:size(Y,2)
     [M,P] = ekf_predict1(M,P,A,Q);
     [M,P] = ekf_update1(M,P,Y(:,k),dh_dx_func,R*eye(1),h_func);
     MM(:,k)   = M;
     PP(:,:,k) = P;
  end
\end{verbatim}
%
As the model dynamics are in this case linear the prediction step
functions exactly the same as in the case of traditional Kalman
filter. In update step we pass the handles to
the measurement model function and it's derivative function and the
variance of measurement noise (parameters 6, 4 and 5,
respectively), in addition to other parameters. These functions also
have additional parameters, which might be needed in some cases. For example,
the dynamic and measurement model functions might have parameters,
which are needed when those functions are called. See the full
function specifications in section 3 for more details about the parameters. 

With second order EKF the filtering loop remains almost the same with
the exception of update step:
%
\begin{verbatim}
  MM2 = zeros(size(M,1),size(Y,2));
  PP2 = zeros(size(M,1),size(M,1),size(Y,2));

  for k=1:size(Y,2)
     [M,P] = ekf_predict1(M,P,A,Q);
     [M,P] = ekf_update2(M,P,Y(:,k),dh_dx_func,...
                         d2h_dx2_func,R*eye(1),h_func);
     MM(:,k)   = M;
     PP(:,:,k) = P;
  end
\end{verbatim}
%


The smoothing of state estimates using the extended RTS smoother is done
sameways as in the previous example:
%
\begin{verbatim}  
[SM1,SP1] = erts_smooth1(MM,PP,A,Q);
\end{verbatim}
%
With the extended forward-backward smoother the smoothing is done with
the following function call:
%
\begin{verbatim}
[SM2,SP2] = etf_smooth1(MM,PP,Y,A,Q,[],[],[],...
                         dh_dx_func,R*eye(1),h_func);
\end{verbatim}
%
Here we have assigned empty vectors for parameters 6,7 and 8 (inverse
prediction, its derivative w.r.t. to noise and its parameters, respectively),
because they are not needed in this case.

To visualize the filtered and smoothed signal estimates we must evaluate
the measurement model function with every state estimate to project the
estimates to the measurement space. This can be done with the built-in
Matlab function \texttt{feval}:
%
\begin{verbatim}
Y_m = feval(h_func, MM);
\end{verbatim}
%
The filtered and smoothed estimates of the signals using the first
order EKF, ERTS and ETF are plotted in
figures \ref{fig:example2_1}, \ref{fig:example2_2} and
\ref{fig:example2_3}, respectively. The estimates produced by second order EKF are
not plotted as they do not differ much from first order ones.
As can be seen from the figures both smoothers give clearly better
estimates than the filter. Especially in the beginning of the signal
it takes a while for the filter to catch on to right track. 


\label{page:sine_rmse}

The difference between the smoothers doesn't become clear just by
looking these figures. In some cases the forward-backward smoother
gives a little better estimates, but it tends to be more sensitive
about numerical accuracy and the process and measurement noises. 
To make a comparison between the performances of different methods we
have listed the average of root mean square errors (RMSE) on 100 Monte Carlo
simulations with different methods in table \ref{table:sine_errors}.
In addition to RMSE of each state variable we also provide the
estimation error in measurement space, because we might be
more interested in estimating the actual value of signal than its components.
Usually, however, the primary goal of these methods is to estimate the
hidden state variables. The following methods were used:
%
\begin{itemize}
\item EKF1: First order extended Kalman filter.
\item ERTS1: First order extended Rauch-Tung-Striebel smoother.
\item ETF1: First order extended Forward-Backward smoother.
\item EKF2: Second order extended Kalman filter.
\item ERTS2: First order extended Rauch-Tung-Striebel smoother applied
  to second order EKF estimates.
\item ETF2: First order extended Forward-Backward smoother applied to
  second order EKF estimates.
\item UKF: unscented Kalman filter.
\item URTS: unscented Rauch-Tung-Striebel smoother.
\end{itemize}
%
From the errors we can see that with filters EKF2 gives clearly the lowest
errors with variables $\theta$ and $a$. Due to this also with
smoothers ERTS2 and ETF2 give clearly lower errors than others. 
On the other hand EKF1 gives the lowest estimation error
with variable $\omega$. Furthermore, with filters EKF1 also gives lowest error in
measurement space. Each smoother, however, gives approximately the
same error in measurement space. It can also be seen, that the UKF
functions the worst in this case. This is due to linear and quadratic
approximations used in EKF working well with this model. However, with
more nonlinear models UKF is often superior over EKF, as we shall see
in later sections. 

In all, none of the used methods proved to be
clearly superior over the others with this model. It is clear,
however, that EKF should be preferred over UKF as it gives lower error
and is slightly less demanding in terms of computation power.
Whether first or second order EKF should be used is ultimately up to the goal of
application. If the actual signal value is of interest, which is usually
the case, then one should use first order EKF, but second order one
might better at predicting new signal values as the variables $\theta$
and $a$ are closer to real ones on average. 

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/demo2_f1}
\caption{Filtered estimate of the sine signal using the first order extended Kalman filter.}
\label{fig:example2_1}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/demo2_f2}
\caption{Smoothed estimate of the sine signal using the extended Kalman (RTS) smoother.}
\label{fig:example2_2}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/demo2_f3}
\caption{Smoothed estimate of the sine signal using a combination of
  two extended Kalman filters.}
\label{fig:example2_3}
\end{center}
\end{figure}


\begin{table}
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
{\it Method}&{\it RMSE[$\theta$]}&{\it RMSE[$\omega$]}&{\it
  RMSE[$a$]}& {\it RMSE[$y$]}\\
\hline
EKF1 & 0.64 & 0.53 & 0.40 & 0.24 \\
ERTS1& 0.52 & 0.31 & 0.33 & 0.15 \\
ETF1& 0.53 & 0.31 & 0.34 & 0.15 \\
EKF2 & 0.34 & 0.54 & 0.31 & 0.29 \\
ERTS2& 0.24 & 0.30 & 0.18 & 0.15 \\
ETF2& 0.24 & 0.30 & 0.18 & 0.15 \\
UKF  & 0.59 & 0.56 & 0.39 & 0.27 \\
URTS & 0.45 & 0.30 & 0.30 & 0.15 \\ 
\hline   
\end{tabular}
\caption{RMSEs of estimating the random sinusoid over 100 Monte Carlo simulations.}
\label{table:sine_errors}
\end{center}
\end{table} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Unscented Transform}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Like Taylor series based approximation presented above also the {\it unscented
  transform} (UT) (Julier and Uhlmann, 1995, 2004; Wan and van der
Merwe, 2001) can be used for forming a Gaussian approximation to the
joint distribution of random variables $\vec{x}$ and $\vec{y}$, which
are defined with equations (\ref{eq:trans_g}). In UT we
deterministically choose a fixed number of sigma points, which capture
the desired moments (atleast mean and covariance) of the original
distribution of $\vec{x}$ exactly. After that we propagate the sigma
points through the non-linear function $\vec{g}$ and estimate the
moments of the transformed variable from them. 

The advantage of UT over the Taylor series based
approximation is that UT is better at capturing the higher order
moments caused by the non-linear transform, as discussed in (Julier
and Uhlmann, 2004). Also the Jacobian and Hessian matrices are not
needed, so the estimation procedure is in general easier and less error-prone. 
 
The unscented transform can be used to provide a Gaussian approximation for the
joint distribution of variables $\vec{x}$ and $\vec{y}$ of the form
%
  \begin{equation}
     \begin{pmatrix}
       \vec{x} \\ \vec{y}
     \end{pmatrix} \sim
     \N\left(
     \begin{pmatrix}
       \vec{m} \\ \vecmu_U
     \end{pmatrix},
     \begin{pmatrix}
       \vec{P}   & \vec{C}_U \\
       \vec{C}_U^T & \mat{S}_U
     \end{pmatrix} \right).
  \end{equation}
%
The (nonaugmented) transformation is done as follows:
%
\begin{enumerate}
\item Compute the set of $2n+1$ sigma points from the columns of the
  matrix $\sqrt{(n + \lambda) \, \mat{P}}$:
%
  \begin{equation}
  \begin{split}
    \vec{x}^{(0)} &= \vec{m} \\
    \vec{x}^{(i)} &= \vec{m} +
       \left[\sqrt{(n + \lambda) \, \mat{P}}\right]_i,
      \quad i=1,\ldots,n \\
    \vec{x}^{(i)} &= \vec{m} - 
       \left[\sqrt{(n + \lambda) \, \mat{P}}\right]_i,
      \quad i=n+1,\ldots,2n
  \end{split}
  \label{eq:ut_sigmas}
  \end{equation}
%
  and the associated weights:
%
  \begin{equation}
  \begin{split}
    W^{(0)}_m &= \lambda / (n + \lambda) \\
    W^{(0)}_c &= \lambda / (n + \lambda) + (1 - \alpha^2 + \beta) \\
    W^{(i)}_m &= 1 / \{ 2(n + \lambda) \}, \quad i=1,\ldots,2n \\
    W^{(i)}_c &= 1 / \{ 2(n + \lambda) \}, \quad i=1,\ldots,2n. 
  \end{split} \label{eq:ut_weights}
  \end{equation}
%
  Parameter $\lambda$ is a scaling parameter, which is defined as
%
  \begin{equation}
    \lambda = \alpha^2 \, (n + \kappa) - n.
  \end{equation}
 % 
  The positive constants $\alpha$, $\beta$ and $\kappa$ are used
  as parameters of the method.

\item Propagate each of the sigma points through non-linearity as
%
  \begin{equation}
    \vec{y}^{(i)} = \vec{g}(\vec{x}^{(i)}), \quad i=0,\ldots,2n.
  \label{eq:sigma_g}
  \end{equation}

\item Calculate the mean and covariance estimates for $\vec{y}$ as
%
  \begin{align}
    \vecmu_U    &\approx \sum_{i=0}^{2n} W^{(i)}_m \, \vec{y}^{(i)} \\
    \mat{S}_U &\approx \sum_{i=0}^{2n} W^{(i)}_c \, (\vec{y}^{(i)} -
                        \vecmu_U) \, (\vec{y}^{(i)} - \vecmu_U)^T.
  \end{align}
  
\item Estimate the cross-covariance between $\vec{x}$ and
  $\vec{y}$ as
%
  \begin{equation}
    \mat{C}_U \approx \sum_{i=0}^{2n} W^{(i)}_c \, (\vec{x}^{(i)} -
                        \vec{m}) \, (\vec{y}^{(i)} - \vecmu_U)^T.
  \end{equation}
\end{enumerate}
%

The square root of positive definite matrix $\mat{P}$ is defined as
$\mat{A} = \sqrt{\mat{P}}$, where
%
\begin{equation}
%
\mat{P} = \mat{A} \mat{A}^T.
%
\end{equation}
%
To calculate the matrix $\mat{A}$ we can use, for example, lower
triangular matrix of the Cholesky factorialization, which can be
computed with built-in Matlab function \texttt{chol}. For convience, we
have provided a function (\texttt{schol}, see page \ref{page:schol}),
which computes the factorialization also for positive semidefinite matrices.


\subsubsection*{The Matrix Form of UT}

The unscented transform described above can be written conviently in matrix form
as follows:
%
\begin{align}
   \mat{X} &= 
      \begin{bmatrix} \vec{m} & \cdots & \vec{m} \end{bmatrix} 
      + \sqrt{c}
      \begin{bmatrix}
         \vec{0} & \sqrt{\mat{P}} & -\sqrt{\mat{P}}
      \end{bmatrix} \label{eq:ut_x1} \\
   \mat{Y} &= \vec{g}(\mat{X}) \label{eq:ukf_x2} \\
   \vecmu_U    &= \mat{Y} \, \mat{w}_m \label{eq:ut_mu} \\
   \mat{S}_U      &= \mat{Y} \, \mat{W} \, \mat{Y}^T \label{eq:ut_s} \\
   \mat{C}_U      &= \mat{X} \, \mat{W} \, \mat{Y}^T, \label{eq:ut_c}
\end{align}
%
where $\mat{X}$ is the matrix of sigma points, function
$\vec{g}(\cdot)$ is applied to each column of the argument matrix
separately, $c = \alpha^2 \, (n + \kappa)$, and vector $\vec{w}_m$ and
matrix $\mat{W}$ are defined as follows:
%
\begin{align}  
  \vec{w}_m &= \begin{bmatrix}
      W^{(0)}_m & \cdots & W^{(2n)}_m
   \end{bmatrix}^T \label{eq:vec_wm} \\
    \mat{W} &=
     \left(\mat{I} -
     \begin{bmatrix}
       \vec{w}_m & \cdots & \vec{w}_m
     \end{bmatrix} \right) \,
     \nonumber \\ &\times
     \diag(W^{(0)}_c \cdots W^{(2n)}_c) \,
     \nonumber \\ &\times
     \left(\mat{I} -
     \begin{bmatrix}
       \vec{w}_m & \cdots & \vec{w}_m
     \end{bmatrix} 
     \right)^T.
   \label{eq:mat_w}
\end{align}
%
See (Särkkä, 2006) for proof for this.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Unscented Kalman filter}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The {\it unscented Kalman filter} (UKF) (Julier et al., 1995; Julier
and Uhlmann, 2004; Wan and van der Merwe, 2001) makes use of the {\it unscented
transform} described above to give a Gaussian approximation to the
filtering solutions of non-linear optimal filtering problems of form
(same as eq. (\ref{eq:nonlinear_prob}), but restated here for
convience)
%
\begin{equation}
\begin{split}
\vec{x}_{k} &= \vec{f}(\vec{x}_{k-1},k-1) + \vec{q}_{k-1} \\
\vec{y}_{k} &= \vec{h}(\vec{x}_{k},k) + \vec{r}_{k},
\end{split}
\end{equation}
%
where $\vec{x}_k \in \spc{R}^n$ is the state, $\vec{y}_k \in
\spc{R}^m$ is the measurement, $\vec{q}_{k-1} \sim
\N(\vec{0},\mat{Q}_{k-1})$ is the Gaussian process noise, and
$\vec{r}_{k} \sim \N(\vec{0},\mat{R}_{k})$ is the Gaussian measurement
noise.

Using the matrix form of UT described above the {\it prediction} and
{\it update} steps of the UKF can computed as follows:
%
\begin{itemize}
\item {\em Prediction:} Compute the predicted state mean $\vec{m}^-_k$
  and the predicted covariance $\mat{P}^-_k$ as

%
\begin{equation}
\begin{split}
   \mat{X}_{k-1} &= 
      \begin{bmatrix} \vec{m}_{k-1} & \cdots & \vec{m}_{k-1} \end{bmatrix}
      + \sqrt{c}
      \begin{bmatrix}
         \vec{0} & \sqrt{\mat{P}_{k-1}} & -\sqrt{\mat{P}_{k-1}}
      \end{bmatrix} \\
   \hat{\mat{X}}_{k}  &= \vec{f}(\mat{X}_{k-1},k-1) \\
   \vec{m}^-_{k}  &= \hat{\mat{X}}_{k} \, \mat{w}_m \\
   \mat{P}^-_{k}  &= \hat{\mat{X}}_{k} \, \mat{W} \, [ \hat{\mat{X}}_{k} ]^T
   + \mat{Q}_{k-1}.
\end{split} \label{eq:ukf1_predict}
\end{equation}
%
\item {\em Update:} Compute the predicted mean $\vecmu_k$ and
  covariance of the measurement $\mat{S}_k$, and the cross-covariance
  of the state and measurement $\mat{C}_k$:
%
\begin{equation}
\begin{split}
   \mat{X}^-_k &= 
      \begin{bmatrix} \vec{m}^-_k & \cdots & \vec{m}^-_k \end{bmatrix}
      + \sqrt{c}
      \begin{bmatrix}
         \vec{0} & \sqrt{\mat{P}^-_k} & -\sqrt{\mat{P}^-_k}
      \end{bmatrix} \\
   \mat{Y}^-_k    &= \vec{h}(\mat{X}^-_k,k) \\
   \vec{\mu}_k    &= \mat{Y}^-_k \, \mat{w}_m \\
   \mat{S}_k      &= \mat{Y}^-_k \, \mat{W} \, [\mat{Y}^-_k]^T + \mat{R}_{k} \\
   \mat{C}_k      &= \mat{X}^-_k \, \mat{W} \, [\mat{Y}^-_k]^T.
\end{split}
\label{eq:ukf1_update1}
\end{equation}
%

Then compute the filter gain $\mat{K}_k$ and the updated state mean
$\vec{m}_k$ and covariance $\mat{P}_k$:
%
\begin{equation}
\begin{split}
  \mat{K}_k &= \mat{C}_k \, \mat{S}_k^{-1} \\
  \vec{m}_k &= \vec{m}^-_k + \mat{K}_k \,
               \left[ \vec{y}_k - \vecmu_k \right] \\
  \mat{P}_k &= \mat{P}^-_k - \mat{K}_k \, \mat{S}_k \, \mat{K}_k^T.
\end{split}
\label{eq:ukf1_update2}
\end{equation}
\end{itemize}

The prediction and update steps of the nonaugmented UKF can be computed
with functions \texttt{ukf\_predict1} and \texttt{ukf\_update1}, respectively. 

\subsubsection*{Augmented UKF}

It is possible to modify the UKF procedure described above by forming an {\it augmented}
state variable, which concatenates the state and noise components
together, so that the effect of process and measurement noises can be
used to better capture the odd-order moment information. This requires
that the sigma points generated during the predict step are also used
in the update step, so that the effect of noise terms are truly
propagated through the nonlinearity (Wu et al., 2005). If, however, we
generate new sigma points in the update step the augmented approach
give the same results as the nonaugmented, if we had assumed that the
noises were additive. If the noises are not additive the augmented
version should produce more accurate estimates than the nonaugmented
version, even if new sigma points are created during the update step.

The {\it prediction} and {\it update} steps of the augmented UKF in matrix
form are as follows:
%
\begin{itemize}
\item {\em Prediction:} Form a matrix of sigma points of the augmented state variable \\$\tilde{\vec{x}}_{k-1} =
\begin{bmatrix} \vec{x}_{k-1}^T & \vec{q}_{k-1}^T & \vec{r}_{k-1}^T
\end{bmatrix}^T$ as 
%
\begin{equation}
%
\mat{\tilde{X}}_{k-1} = 
   \begin{bmatrix} \tilde{\vec{m}}_{k-1} & \cdots & \tilde{\vec{m}}_{k-1} \end{bmatrix}
   + \sqrt{c}
   \begin{bmatrix}
     \vec{0} & \sqrt{\tilde{\mat{P}}_{k-1}} & -\sqrt{\tilde{\mat{P}}_{k-1}}
   \end{bmatrix},
%
\label{eq:ukf2_predict1}
\end{equation}
%
where 
%
\begin{equation}
%
\tilde{\vec{m}}_{k-1} = \begin{bmatrix} \vec{m}_{k-1}^T & \vec{0}
  & \vec{0} \end{bmatrix}^T
%
\text{ and }
%
\tilde{\mat{P}}_{k-1}  = 
\begin{bmatrix}
\mat{P_{k-1}} & \mat{0} & \mat{0} \\
\mat{0} & \mat{Q}_{k-1} & \mat{0} \\
\mat{0} & \mat{0} & \mat{R}_{k-1} \\
\end{bmatrix}.
%
\label{eq:ukf2_predict2}
\end{equation}

Then compute the predicted state mean $\vec{m}^-_k$
  and the predicted covariance $\mat{P}^-_k$ as
%
\begin{equation}
\begin{split}
   \hat{\mat{X}}_{k}  &= \vec{f}(\mat{X^x}_{k-1},\mat{X^q}_{k-1},k-1) \\
   \vec{m}^-_{k}  &= \hat{\mat{X}}_{k} \, \mat{w}_m \\
   \mat{P}^-_{k}  &= \hat{\mat{X}}_{k} \, \mat{W} \, [ \hat{\mat{X}}_{k} ]^T,
\end{split}
\label{eq:ukf2_predict3}
\end{equation}
%
where we have denoted the components of sigma points which correspond to
actual state variables and process noise with matrices
$\mat{X}^x_{k-1}$ and $\mat{X}^q_{k-1}$, respectively. The state transition
function $\vec{f}$ is also augmented to incorporate the effect of
process noise, which is now passed to the function as a second
parameter. In additive noise case the process noise is directly added
to the state variables, but other forms of noise effect are now also allowed.

%
\item {\em Update:} Compute the predicted mean $\vecmu_k$ and
covariance of the measurement $\mat{S}_k$, and the cross-covariance
of the state and measurement $\mat{C}_k$:
%
\begin{equation}
\begin{split}
   \mat{Y}^-_k    &= \vec{h}(\hat{\mat{X}}_k,\mat{X}^r_{k-1},k) \\
   \vec{\mu}_k    &= \mat{Y}^-_k \, \mat{w}_m \\
   \mat{S}_k      &= \mat{Y}^-_k \, \mat{W} \, [\mat{Y}^-_k]^T \\
   \mat{C}_k      &= \hat{\mat{X}}_k \, \mat{W} \, [\mat{Y}^-_k]^T,
\end{split}
\label{eq:ukf2_update1}
\end{equation}
%
where we have denoted the component of sigma points corresponding to
measurement noise with matrix $\mat{X}^r_{k-1}$. Like the state transition
function $\vec{f}$ also the measurement function $\vec{h}$ is now
augmented to incorporate the effect of measurement noise, which is
passed as a second parameter to the function.

Then compute the filter gain $\mat{K}_k$ and the updated state mean
$\vec{m}_k$ and covariance $\mat{P}_k$:
%
\begin{equation}
\begin{split}
  \mat{K}_k &= \mat{C}_k \, \mat{S}_k^{-1} \\
  \vec{m}_k &= \vec{m}^-_k + \mat{K}_k \,
               \left[ \vec{y}_k - \vecmu_k \right] \\
  \mat{P}_k &= \mat{P}^-_k - \mat{K}_k \, \mat{S}_k \, \mat{K}_k^T.
\end{split}
\label{eq:ukf2_update2}
\end{equation}
\end{itemize}
%

Note that nonaugmented form UKF is computationally less demanding than
augmented form UKF, because it creates a smaller number of sigma
points during the filtering procedure. Thus, the usage of the nonaugmented
version should be preferred over the nonaugmented version, if the
propagation of noise terms doesn't improve the accuracy of the estimates.

The prediction and update steps of the augmented UKF can be computed
with functions \texttt{ukf\_predict3} and \texttt{ukf\_update3},
respectively. These functions concatenates the state variables,
process and measurements noises to the augmented variables, as was
done above. 

It is also possible to separately concatenate only the state variables and process
noises during prediction step and state variables and measurement noises
during update step. Filtering solution based on this formulation can
be computed with functions \texttt{ukf\_predict2} and \texttt{ukf\_update2}. However,
these functions create new sigma points during the update step in
addition to ones created during prediction step, and hence the higher
moments might not get captured so effectively in cases, where the
noise terms are additive. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Unscented Kalman smoother}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Rauch-Rung-Striebel type smoother using the unscented
transformation (Särkkä, 2006c) can be used for computing a Gaussian
approximation to the smoothing distribution of the step $k$:
%
\begin{equation}
%
p(\vec{x}_k|\vec{y}_{1:T}) \sim N(\vec{x}_k|\vec{m}_k^s,\mat{P}_k^s),
%
\end{equation}
% 
as follows (using again the matrix form):
%
\begin{itemize}
%
\item Form a matrix of sigma points of the augmented state variable $\tilde{\vec{x}}_{k-1} =
\begin{bmatrix} \vec{x}_{k-1}^T & \vec{q}_{k-1}^T \end{bmatrix}^T$ as
%
\begin{equation}
%
\mat{\tilde{X}}_{k-1} = 
   \begin{bmatrix} \tilde{\vec{m}}_{k-1} & \cdots & \tilde{\vec{m}}_{k-1} \end{bmatrix}
   + \sqrt{c}
   \begin{bmatrix}
     \vec{0} & \sqrt{\tilde{\mat{P}}_{k-1}} & -\sqrt{\tilde{\mat{P}}_{k-1}}
   \end{bmatrix},
%
\label{eq:urts1}
\end{equation}
%
where
%
\begin{equation}
%
\tilde{\vec{m}}_{k-1} = \begin{bmatrix} \vec{m}_{k-1}^T & \vec{0} \end{bmatrix}^T
%
\text{  and  }
%
\tilde{\mat{P}}_{k-1} = 
\begin{bmatrix}
\mat{P_{k-1}} & \mat{0} \\
\mat{0} & \mat{Q}_{k-1} \\
\end{bmatrix}.
\label{eq:urts2}
\end{equation}
%
\item Propagate the sigma points through the dynamic model:
%
\begin{equation}
%
\tilde{\mat{X}}_{k+1}^- = \vec{f}(\tilde{\mat{X}}^x_{k},\tilde{\mat{X}}^q_{k},k),
%
\label{eq:urts3}
\end{equation}
% 
where $\tilde{\mat{X}}^x_{k}$ and $\tilde{\mat{X}}^q_{k}$ denotes the
parts of sigma points, which correspond to $\vec{x}_k$ and
$\vec{q}_k$, respectively.

\item Compute the predicted mean $\vec{m}^-_{k+1}$, covariance
  $\mat{P}^-_{k+1}$ and cross-covariance $\mat{C}_{k+1}$:
\begin{equation}
\begin{split}
   \vec{m}^-_{k+1} &= \tilde{\mat{X}}_{k+1}^{-x} \, \mat{w}_m \\
   \mat{P}^-_{k+1} &= \tilde{\mat{X}}_{k+1}^{-x} \, \mat{W} \, [\tilde{\mat{X}}_{k+1}^{-x}]^T \\
   \mat{C}_{k+1} &= \tilde{\mat{X}}_{k+1}^{-x} \, \mat{W} \, [\tilde{\mat{X}}^x_{k}]^T,
\end{split}
\label{eq:urts4}
\end{equation}
%
where $\tilde{\mat{X}}_{k+1}^{-x}$ denotes the part of propagated
sigma points $\tilde{\mat{X}}_{k+1}^-$, which corresponds to $\vec{x}_k$.
%
\item Compute the smoother gain $\mat{D}_k$, the smoothed mean $\vec{m}_k^s$ and
  the covariance $\mat{P}_k^s$:
\begin{equation}
\begin{split}
\mat{D}_k & = \mat{C}_{k+1} [\mat{P}_{k+1}^-]^{-1} \\
\vec{m}_k^s & = \vec{m}_k + \mat{D}_k [\vec{m}^s_{k+1} -
\vec{m}_{k+1}^-] \\
\mat{P}_k^s & = \mat{P}_k + \mat{D}_k [\mat{P}^s_{k+1} -
\mat{P}_{k+1}^-] \mat{D}_k^T.
\end{split}
\label{eq:urts5}
\end{equation}

\end{itemize}
%

The smoothing solution of this augmented type RTS smoother can be
computed with function \texttt{urts\_smooth2}. Also a nonaugmented
version of this type smoother has been implemented, and a smoothing
solution with that can be computed with function
\texttt{urts\_smooth1}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Demonstration: UNGM-model}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To illustrate some of the advantages of UKF over EKF and augmented
form UKF over non-augmented lets now consider an example, in which we estimate a model called
Univariate Nonstationary Growth Model (UNGM), which is previously used
as benchmark, for example, in (Kotecha and Djuric, 2003) and (Wu et
al., 2005). What makes this model particularly interesting in this case is that its highly nonlinear
and bimodal, so it is really challenging for traditional filtering
techniques. We also show how in this case the augmented version of UKF
gives better performance than the nonaugmented version.

The dynamic state space model for UNGM can be written as
%
\begin{eqnarray}
x_n & = & \alpha x_{n-1} + \beta \frac{x_{n-1}}{1+x_{n-1}^2} + \gamma
\cos(1.2(n-1)) + u_n \\
y_n & = & \frac{x_n^2}{20} + v_n, n = 1,\ldots,N
\end{eqnarray}
%
where $u_n \sim N(0,\sigma_u^2)$ and $v_n \sim N(0,\sigma_u^2)$. In
this example we have set the parameters to $\sigma_u^2 = 1$,
$\sigma_v^2=1$, $x_0 = 0.1$, $\alpha = 0.5$, $\beta = 25$, $\gamma =
8$, and $N=500$. The cosine term in the state transition equation
simulates the effect of time-varying noise.

In this demonstration the state transition is computed with the
following function:
%
\begin{verbatim}
function x_n = ungm_f(x,param)
n = param(1);
x_n = 0.5*x(1,:) + 25*x(1,:)./(1+x(1,:).*x(1,:)) + 8*cos(1.2*(n-1));  
if size(x,1) > 1
  x_n = x_n + x(2,:);
end    
\end{verbatim}
% 
where the input parameter \texttt{x} contains the state on the
previous time step. The current time step index $n$ needed by the
cosine term is passed in the input parameter
\texttt{param}. The last three lines in the function adds the process
noise to the state component, if the augmented version of the UKF is
used. Note that in augmented UKF the state, process noise
and measurement noise terms are all concatenated together to the
augmented variable, but in URTS the measurement noise term is left
out. That is why we must make sure that the functions we declare
are compatible with all cases (nonaugmented, augmented with and
without measurement noise). In this case we check whether the state
variable has second component (process noise) or not. 

Similarly, the measurement model function is declared as 
%
\begin{verbatim}
function y_n = ungm_h(x_n,param)
y_n = x_n(1,:).*x_n(1,:) ./ 20;
if size(x_n,1) == 3
   y_n = y_n + x_n(3,:);
end
\end{verbatim}
%
The filtering loop for augmented UKF is as follows:
%
\begin{verbatim}
for k = 1:size(Y,2)
   [M,P,X_s,w] = ukf_predict3(M,P,f_func,u_n,v_n,k);
   [M,P] = ukf_update3(M,P,Y(:,k),h_func,v_n,X_s,w,[]);
   MM_UKF2(:,k)   = M;
   PP_UKF2(:,:,k) = P;    
end
\end{verbatim}
%
The biggest difference in this in relation to other filters is that
now the predict step returns the sigma points (variable \texttt{X\_s})
and their weigths (variable \texttt{w}), which must be passed as
parameters to update function.


To compare the EKF and UKF to other possible filtering techniques we
have also used a bootstrap filtering approach (Gordon et al., 1993), which belongs to class
of {\it Sequential Monte Carlo} (SMC) methods (also known as {\it particle filters}).
Basically the idea in SMC methods that is during each time step they draw a
set of weighted particles from some appropriate importance
distribution and after that the moments (that is, mean and covariance)
of the function of interest (e.g. dynamic function in state space
models) are estimated approximately from drawn samples. The weights of
the particles are adjusted so that they are approximations to the
relative posterior probabilities of the
particles. Usually also a some kind of {\it resampling} scheme is used
to avoid the problem of degenerate particles, that is, particles with
near zero weights are removed and those with large weights are
duplicated. In this example we used a {\it stratified resampling}
algorithm (Kitagawa, 1996), which is optimal in terms of variance. In
bootstrap filtering the dynamic model $p(\vec{x}_k|\vec{x}_{k-1})$ is
used as importance distribution, so its implementation is really easy.
However, due to this a large number of particles might be needed for
the filter to be effective. In this case 1000 particles were drawn on
each step. The implementation of the bootstrap filter
is commented out in the actual demonstration script (\texttt{ungm\_demo.m}),
because the used resampling function (\texttt{resampstr.m})
was originally provided in MCMCstuff toolbox (Vanhatalo et al., 2006),
which can be found at \url{http://www.lce.hut.fi/research/mm/mcmcstuff/}. 

In figure \ref{fig:ungm_states} we have plotted the 100 first samples
of the signal as well as the estimates produced by EKF, augmented form
UKF and bootstrap filter. The bimodality is easy to see from the
figure. For example, during samples $10-25$ UKF is able to estimate
the correct mode while the EKF estimates it wrong. Likewise, during
steps $45-55$ and $85-95$ UKF has troubles in following the correct mode
while EKF is more right. Bootstrap filter on the other hand tracks the
correct mode on almost ever time step, although also it produces
notable errors.

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/ungm_states}
\caption{First 100 samples of filtering results of EKF, augmented form UKF and bootstrap
  filter for UNGM-model.}
\label{fig:ungm_states}
\end{center}
\end{figure}

In figure \ref{fig:ungm_c_errors} we have plotted the absolute errors
and $3\sigma$ confidence intervals of the previous figures filtering
results. It can be seen that the EKF is overoptimistic in many cases
while UKF and boostrap filter are better at telling when their
results are unreliable. Also the lower error of bootstrap filter can
be seen from the figure. The bimodality is also easy to notice on
those samples, which were mentioned above.

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/ungm_c_errors}
\caption{Absolute errors of and $3\sigma$ confidence intervals of EKF,
  augmented form UKF and bootstrap in 100 first samples.}
\label{fig:ungm_c_errors}
\end{center}
\end{figure}

The make a comparison between nonaugmented and augmented UKF we have
plotted 100 first samples of their filtering results in figure
\ref{fig:ungm_ukf_comp}. Results are very surprising (although same as
in (Wu et al, 2005)). The reason why
nonaugmented UKF gave so bad results is not clear. However, the
better performance of augmented form UKF can be explained by the fact,
that the process noise is taken into account more effectively when the
sigma points are propagated through nonlinearity. In this case it
seems to be very crucial, as the model is highly nonlinear and multi-modal.

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/ungm_ukf_comp}
\caption{Filtering results of nonaugmented UKF (UKF1) and augmented
  UKF (UKF2) of 100 first samples.}
\label{fig:ungm_ukf_comp}
\end{center}
\end{figure}

Lastly in figure \ref{fig:ungm_mse} we have plotted the mean square errors of each tested methods
of 100 Monte Carlo runs. Average of those errors are listed in table
\ref{table:ungm_errors}. Here is a discussion for the results:
%
\begin{itemize}
\item It is surprising that the nonaugmented UKF seems to be better
  than EKF, while in above figures we have shown, that the nonaugmented UKF gives very bad
results. Reason for this is simple: the variance of the actual signal
is approximately 100, which means that by simply guessing zero
we get better performance than with EKF, on average. The
estimates of nonaugmented UKF didn't variate much on average, so they were
better than those of EKF, which on the other hand variated greatly and
gave huge errors in some cases. Because of this neither of the methods
should be used for this problem, but if one has to choose between the
two, that would be EKF, because in some cases it still gave (more or
less) right answers, whereas UKF were practically always wrong.
\item The second order EKF were also tested, but that diverged almost
  instantly, so it were left out from comparison.
\item Augmented form UKF gave clearly the best
  performance from the tested Kalman filters. As discussed above, this
  is most likely due to the fact that the process noise terms are propagated
  through the nonlinearity, and hence odd-order moment information is
  used to obtain more accurate estimates. The usage of RTS smoother seemed
  to improve the estimates in general, but oddly in some cases it made
  the estimates worse. This is most likely due to the multi-modality of
  the filtering problem.
\item Bootstrap filtering solution was clearly superior over all other
  tested methods. The results had been even better, if greater
  amount of particles had been used.
\end{itemize}

The reason why Kalman filters didn't work that well
in this case is because Gaussian approximations do not in general
apply well for multi-modal cases. Thus, a particle filtering solution
should be preferred over Kalman filters in such cases. However,
usually the particle filters need a fairly large amount of particles to be
effective, so they are generally more demanding in terms of computational
power than Kalman filters, which can be a limiting factor in real
world applications. The errors, even with bootstrap filter, were also
relatively large, so one must be careful when using the estimates
in, for example, making financial decisions. In practice this means
that one has to monitor the filter's covariance estimate, and trust the
state estimates and predictions only when the covariance estimates are
low enough, but even then there is a change, that the filter's
estimate is completely wrong.  


\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/ungm_mse}
\caption{MSEs of different methods in 100 Monte Carlo runs.}
\label{fig:ungm_mse}
\end{center}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\it Method}&{\it MSE[$x$]}\\
\hline
UKF1 & 87.9 \\
URTS1& 69.09 \\
UKF2  & 63.7 \\
URTS2 & 57.7 \\ 
EKF  & 125.9 \\ 
ERTS & 92.2 \\
BS   & 10.2 \\
\hline   
\end{tabular}
\caption{MSEs of estimating the UNGM model over 100 Monte Carlo simulations.}
\label{table:ungm_errors}
\end{center}
\end{table} 
 
\subsubsection{Demonstration: Bearings Only Tracking}

Next we review a classical filtering application (see, e.g.,
Bar-Shalom et al., 2001), in which we track a moving object 
with sensors, which measure only the bearings (or angles) of the
object with respect positions of the sensors. There is a one moving
target in the scene and two angular sensors for tracking it. Solving
this problem is important, because often more general multiple target
tracking problems can be partitioned into sub-problems, in which
single targets are tracked separately at a time (Särkkä, 2006b). 

The state of the target at time step $k$ consists of the position in
two dimensional cartesian coordinates $x_k$ and $y_k$ and the velocity
toward those coordinate axes, $\dot{x}_k$ and $\dot{y}_k$. Thus,
the state vector can be expressed as 
%
\begin{equation}
%
\vec{x}_k = 
\begin{pmatrix}
x_k & y_k & \dot{x}_k & \dot{y}_k 
\end{pmatrix}^T.
%
\end{equation}
%
The dynamics of the target is modelled as a linear, discretized Wiener
velocity model (Bar-Shalom et al., 2001) 
%
\begin{equation}
%
\vec{x}_k = \begin{pmatrix}
1 & 0 & \dt & 0   \\
0 & 1 & 0   & \dt \\
0 & 0 & 1   & 0   \\
0 & 0 & 0   & 1  
\end{pmatrix}
% * 
\begin{pmatrix}
x_{k-1} \\
y_{k-1} \\
\dot{x}_{k-1} \\
\dot{y}_{k-1}    
\end{pmatrix}
+
\vec{q}_{k-1},
%
\end{equation}
% 
where $\vec{q}_{k-1}$ is Gaussian process noise with zero mean and
covariance
%
\begin{equation}
\begin{split}
E[\vec{q}_{k-1}\vec{q}^T_{k-1}] = \begin{pmatrix}
\frac{1}{3}\dt^3 & 0                 & \frac{1}{2}\dt^2 & 0 \\
0                &  \frac{1}{3}\dt^3 & 0                & \frac{1}{2}\dt^2 \\
\frac{1}{2}\dt^2 & 0                 & \dt              & 0 \\
0                & \frac{1}{2}\dt^2  & 0                & \dt  
\end{pmatrix} \\
\end{split}q,
\end{equation}
%
where $q$ is the spectral density of the noise, which is set to
$q=0.1$ in the simulations. The measurement model for sensor $i$ is defined as
%
\begin{equation}
%
\theta^i_k = \arctan \left( \frac{y_k - s^i_y}{x_k - s^i_x}\right) + r^i_k,
%
\end{equation}
%
where $(s^i_x,s^i_y)$ is the position of sensor $i$ and $r^i_k \sim
N(0,\sigma^2)$, with $\sigma = 0.05$ radians. In figure
\ref{fig:bot_measurements} we have plotted a one realization of
measurements in radians obtained from both sensors. The sensors are
placed to $(s^1_x,s^1_y) = (-1,-2)$ and $(s^2_x,s^2_y) = (1,1)$.
%
\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/bot_demo_measurements}
\caption{Measurements from sensors (in radians) in bearings only
  tracking problem .}
\label{fig:bot_measurements}
\end{center}
\end{figure}

The derivatives of the
measurement model, which are needed by EKF, can be computed as
%
\begin{equation}
\begin{split}
\frac{\partial \vec{h}^i(\vec{x}_k)}{\partial x_k} & = \frac{-(y_k-s^i_y)}{(x_k-s^i_x)^2+(y_k-s^i_y)^2} \\
\frac{\partial \vec{h}^i(\vec{x}_k)}{\partial y_k} & = \frac{(x_k-s^i_x)}{(x_k-s^i_x)^2+(y_k-s^i_y)^2} \\
\frac{\partial \vec{h}^i(\vec{x}_k)}{\partial \dot{x}_k} & = 0 \\
\frac{\partial \vec{h}^i(\vec{x}_k)}{\partial \dot{y}_k} & = 0 , i = 1,2.
\end{split}
\label{eq:bot_dx}
\end{equation}
%
With these the Jacobian can written as
%
\begin{equation}
\mat{H}_\vec{x}(\vec{x}_k,k) =  \begin{pmatrix}
\frac{(x_k-s^1_x)}{(x_k-s^1_x)^2+(y_k-s^1_y)^2} & \frac{-(y_k-s^1_y)}{(x_k-s^1_x)^2+(y_k-s^1_y)^2} & 0 & 0 \\
\frac{(x_k-s^2_x)}{(x_k-s^2_x)^2+(y_k-s^2_y)^2} & \frac{-(y_k-s^2_y)}{(x_k-s^2_x)^2+(y_k-s^2_y)^2} & 0 & 0   
\end{pmatrix}.
\end{equation}
%
The non-zero second order derivatives of the measurement function are also
relatively easy to compute in this model:
%
\begin{equation}
\begin{split}
\frac{\partial^2 \vec{h}^i(\vec{x}_k)}{\partial x_k \partial x_k} & = \frac{-2(x_k-s^i_x)}{((x_k-s^i_x)^2+(y_k-s^i_y)^2)^2} \\
\frac{\partial^2 \vec{h}^i(\vec{x}_k)}{\partial x_k \partial y_k} & = \frac{(y_k-s^i_y)^2-(x_k-s^i_x)^2}{((x_k-s^i_x)^2+(y_k-s^i_y)^2)^2} \\
\frac{\partial^2 \vec{h}^i(\vec{x}_k)}{\partial y_k \partial y_k} & = \frac{-2(y_k-s^i_y)}{((x_k-s^i_x)^2+(y_k-s^i_y)^2)^2}.
\end{split}
\end{equation}
%
Thus, the Hessian matrices can be written as
%
\begin{equation}
\mat{H}^i_\vec{xx}(\vec{x}_k,k) = \begin{pmatrix}
 \frac{-2(x_k-s^i_x)}{((x_k-s^i_x)^2+(y_k-s^i_y)^2)^2} &
   \frac{(y_k-s^i_y)^2-(x_k-s^i_x)^2}{((x_k-s^i_x)^2+(y_k-s^i_y)^2)^2}
   & 0 & 0 \\
 \frac{(y_k-s^i_y)^2-(x_k-s^i_x)^2}{((x_k-s^i_x)^2+(y_k-s^i_y)^2)^2} & \frac{-2(y_k-s^i_y)}{((x_k-s^i_x)^2+(y_k-s^i_y)^2)^2} & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0  
\end{pmatrix}, i=1,2.
\end{equation}
%
We do not list the program code for the measurement function and it's
derivatives here as they are straightforward to implement, if the
previous examples have been read.

The target starts with state $\vec{x}_0 = \begin{pmatrix} 0 & 0 & 1 & 0
\end{pmatrix}$, and in the estimation we set the prior distribution
for the state to $\vec{x}_0 \sim N(\vec{0},\mat{P}_0)$, where
%
\begin{equation}
%
\mat{P}_0 = \begin{pmatrix}
0.1 & 0 & 0 & 0\\
0 & 0.1 & 0 & 0\\
0 & 0 & 10 & 0\\
0 & 0 & 0 & 10
\end{pmatrix},
%
\end{equation}
%
which basically means that we are fairly certain about the target's
origin, but very uncertain about the velocity. In the simulations we also give the
target an slightly randomized acceleration, so that it achieves a
curved trajectory, which is approximately the same in different
simulations. The trajectory and estimates of it can be seen in figures
\ref{fig:bot_ekf1}, \ref{fig:bot_ekf2} and \ref{fig:bot_ukf}. As can
be seen from the figures EKF1 and UKF give almost identical
results while the estimates of EKF2 are clearly worse. Especially in
the beginning of the trajectory EKF2 has great difficulties in getting on
the right track, which is due to the relatively big uncertainty in the
starting velocity. After that the estimates are fairly similar. 

In table \ref{table:bot_rmse} we have listed the root mean square errors (mean
of position errors) of all
tested methods (same as in random sine signal example
on page \pageref{page:sine_rmse} with the addition of UTF) over 1000
Monte Carlo runs. The numbers prove the previous observations, that
the EKF1 and UKF give almost identical performances. Same observations
apply also to smoothers. Had the prior distribution for the starting
velocity been more accurate the performance difference
between EKF2 and other methods would have been smaller, but still noticable.

\begin{table}
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\it Method}&{\it RMSE } \\
\hline
EKF1 & 0.114 \\
ERTS1& 0.054 \\
ETF1& 0.054 \\
EKF2 & 0.202 \\
ERTS2& 0.074 \\
ETF2& 0.063 \\
UKF  & 0.113 \\
URTS & 0.055 \\
UTF & 0.055 \\ 
\hline   
\end{tabular}
\caption{RMSEs of estimating the position in Bearings Only Tracking
  problem over 1000 Monte Carlo runs.}
\label{table:bot_rmse}
\end{center}
\end{table} 


%
\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/bot_demo_ekf1}
\caption{Filtering and smoothing results of first order EKF.}
\label{fig:bot_ekf1}
\end{center}
\end{figure}

%
\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/bot_demo_ekf2}
\caption{Filtering and smoothing results of second order EKF.}
\label{fig:bot_ekf2}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/bot_demo_ukf}
\caption{Filtering and smoothing results of UKF}
\label{fig:bot_ukf}
\end{center}
\end{figure}



\subsubsection{Demonstration: Reentry Vehicle Tracking}

Next we review a challenging filtering problem, which was
used in (Julier and Uhlmann, 2004b) to demonstrate the
performance of UKF. Later they released few corrections to the
model specifications and simulation parameters in (Julier and Uhlmann,
2004a). 

This example conserns a reentry tracking problem, where radar
is used for tracking a space vehicle, which enters the atmosphere at
high altitude and very high speed. Figure \ref{fig:reentry_traj} shows
a sample trajectory of the vehicle with respect to earth and radar. 
The dynamics of the vehicle are affected with three kinds of forces:
aerodynamic drag, which is a
function of vehicle speed and has highly nonlinear variations in
altitude. The second type of force is gravity, which causes the
vehicle to accelerate toward the center of the earth. The third type
of forces are random buffeting terms. The state space in this model
consists of vehicles position ($x_1$ and $x_2$), its velocity ($x_3$
and $x_4$) and a parameter of its aerodynamic properties ($x_5$).
The dynamics in continuous case are defined as (Julier and Uhlmann, 2004b)
%
\begin{equation}
\begin{split}
%
\dot{x}_1(t) & = x_3(t) \\
\dot{x}_2(t) & = x_4(t) \\
\dot{x}_3(t) & = D(t)x_3(t) + G(t)x_1(t) + v_1(t) \\
\dot{x}_4(t) & = D(t)x_4(t) + G(t)x_2(t) + v_2(t) \\
\dot{x}_5(t) & = v_3(t),
%
\end{split}
\end{equation}
%
where $\vec{w}(t)$ is the process noise vector, $D(t)$ the
drag-related force and $G(t)$ the gravity-related force.
The force terms are given by
%
\begin{equation}
\begin{split}
%
D(k) & = \beta(t) \exp \left\{ \frac{[R_0 - R(t)]}{H_0}\right\} V(t) \\
G(t) & = -\frac{Gm_0}{R^3(t)} \\
\beta (t) & = \beta_0 \exp x_5(t),
%
\end{split}
\end{equation}
%
where $R(t) = \sqrt{x_1^2(t) + x_2^2(t)}$ is the vehicle's distance from the center of the earth
and $V(t) = \sqrt{x_3^2(t) + x_4^2(t)}$ is the speed of the vehicle.
The constants in previous definition were set to
%
\begin{equation}
\begin{split}
%
\beta_0 & = -0.59783 \\
H_0 & = 13.406 \\
Gm_0 & = 3.9860 \times 10^5 \\
R_0 & = 6374.
%
\end{split}
\end{equation}
%
To keep the implementation simple the continuous-time dynamic
equations were discretized using a simple Euler integration scheme, to give
%
\begin{equation}
\begin{split}
%
x_1(k+1) & = x_1(k) + \dt x_3(k) \\
x_2(k+1) & = x_2(k) + \dt x_4(k) \\
x_3(k+1) & = x_3(k) + \dt (D(k)x_3(k) + G(k)x_1(k)) + w_1(k) \\
x_4(k+1) & = x_4(k) + \dt (D(k)x_4(k) + G(k)x_2(k)) + w_2(k) \\
x_5(k+1) & = x_5(k) + w_3(k),
%
\end{split}
\end{equation}
%
where the step size between time steps was set to $\dt = 0.1s$. Note that this might
be too simple approach in real world applications due to high
nonlinearities in the dynamics, so more advanced integration scheme
(such as Runge-Kutta) might be more preferable. The discrete process noise covariance in the
simulations was set to
%
\begin{equation}
%
Q(k) = \begin{pmatrix}
2.4064 \times 10^{-5} & 0 & 0 \\
0 & 2.4064 \times 10^{-5} & 0 \\
0 & 0 & 10^{-6}
\end{pmatrix}.
%
\end{equation}
%
The lower right element in the covariance was initially in (Julier and
Uhlmann, 2004b) set to zero, but later in (Julier and Uhlmann, 2004a)
changed to $10^{-6}$ to increase filter stability.


The non-zero derivatives of the discretized dynamic equations with respect to
state variables are straightforward (although rather technical) to compute:
%
\begin{equation} 
\begin{split}
%
\frac{\partial x_1(k+1)}{\partial x_1(k)} & = 1 \\
\frac{\partial x_1(k+1)}{\partial x_3(k)} & = \dt \\
\frac{\partial x_2(k+1)}{\partial x_2(k)} & = 1 \\
\frac{\partial x_2(k+1)}{\partial x_4(k)} & = \dt \\
%
\frac{\partial x_3(k+1)}{\partial x_1(k)} & = \dt * (\frac{\partial
  D(k)}{\partial x_1(k)} x_3(k) + \frac{\partial G(k)}{\partial
  x_1(k)} x_1(k) + G(k)) \\
\frac{\partial x_3(k+1)}{\partial x_2(k)} & = \dt * (\frac{\partial
  D(k)}{\partial x_2(k)} x_3(k) + \frac{\partial G(k)}{\partial
  x_2(k)} x_1(k)) \\
\frac{\partial x_3(k+1)}{\partial x_3(k)} & = \dt * (\frac{\partial
  D(k)}{\partial x_3(k)} x_3(k) + D(k)) + 1 \\
\frac{\partial x_3(k+1)}{\partial x_4(k)} & = \dt * (\frac{\partial
  D(k)}{\partial x_4(k)} x_3(k))  \\
\frac{\partial x_3(k+1)}{\partial x_4(k)} & = \dt * (\frac{\partial
  D(k)}{\partial x_5(k)} x_3(k))  \\
%
\frac{\partial x_4(k+1)}{\partial x_1(k)} & = \dt * (\frac{\partial
  D(k)}{\partial x_1(k)} x_4(k) + \frac{\partial G(k)}{\partial
  x_1(k)} x_2(k)) \\
\frac{\partial x_4(k+1)}{\partial x_2(k)} & = \dt * (\frac{\partial
  D(k)}{\partial x_2(k)} x_4(k) + \frac{\partial G(k)}{\partial
  x_2(k)} x_2(k) + G(k)) \\
\frac{\partial x_4(k+1)}{\partial x_3(k)} & = \dt * (\frac{\partial
  D(k)}{\partial x_3(k)} x_4(k)) \\
\frac{\partial x_4(k+1)}{\partial x_4(k)} & = \dt * (\frac{\partial
  D(k)}{\partial x_4(k)} x_4(k) + D(k)) + 1 \\
\frac{\partial x_4(k+1)}{\partial x_5(k)} & = \dt * (\frac{\partial
  D(k)}{\partial x_5(k)} x_4(k))  \\
\frac{\partial x_5(k+1)}{\partial x_5(k)} & = 1,
%
\end{split}
\end{equation}
%
where the (non-zero) derivatives of the force, position and velocity
related terms with respect to state variables can be computed as
%
\begin{equation}
\begin{split}
%
\frac{\partial R(k)}{\partial x_1(k)} & = x_1(k) \frac{1}{R(k)} \\
\frac{\partial R(k)}{\partial x_2(k)} & = x_2(k) \frac{1}{R(k)} \\
\frac{\partial V(k)}{\partial x_3(k)} & = x_3(k) \frac{1}{V(k)} \\
\frac{\partial V(k)}{\partial x_4(k)} & = x_4(k) \frac{1}{V(k)} \\
\frac{\partial \beta(k)}{\partial x_5(k)} & = \beta(k)\frac{1}{R(k)} \\
\frac{\partial D(k)}{\partial x_1(k)} & = -\frac{\partial
  R(k)}{\partial x_1(k)} \frac{1}{H_0} * D(k) \\
\frac{\partial D(k)}{\partial x_2(k)} & = -\frac{\partial
  R(k)}{\partial x_2(k)} \frac{1}{H_0} * D(k) \\
\frac{\partial D(k)}{\partial x_3(k)} & = \beta(k) \exp \left\{
  \frac{[R_0 - R(k)]}{H_0}\right\} \frac{\partial V(k)}{\partial x_3} \\
\frac{\partial D(k)}{\partial x_4(k)} & = \beta(k) \exp \left\{
  \frac{[R_0 - R(k)]}{H_0}\right\} \frac{\partial V(k)}{\partial x_4} \\
\frac{\partial D(k)}{\partial x_5(k)} & = \frac{\partial \beta(k)}{x_5(k)} \exp \left\{
  \frac{[R_0 - R(k)]}{H_0}\right\} V(k) \\
\frac{\partial G(k)}{\partial x_1(k)} & = \frac{3 Gm_0}{(R(k))^4}
\frac{\partial R(k)}{\partial x_1(k)} \\ 
\frac{\partial G(k)}{\partial x_2(k)} & = \frac{3 Gm_0}{(R(k))^4} \frac{\partial R(k)}{\partial x_2(k)}.
%
\end{split}
\end{equation}
%
The prior distribution for the state was set to multivariate
Gaussian, with mean and covariance (same as in (Julier and Uhlmann, 2004b))
%
\begin{equation}
\begin{split}
%
\vec{m}_0 & = \begin{pmatrix}
6500.4 \\
349.14 \\
-1.8093 \\
-6.7967 \\
0 \\
\end{pmatrix} \\
\mat{P}_0 & = \begin{pmatrix}
10^{-6} & 0 & 0 & 0 & 0 \\
0 & 10^{-6}& 0 & 0 & 0 \\
0 & 0 & 10^{-6} & 0 & 0 \\
0 & 0 & 0 & 10^{-6}& 0 \\
0 & 0 & 0 & 0 & 1
\end{pmatrix}.
%
\end{split}
\end{equation}
%
In the simulations the initial state were drawn from multivariate
Gaussian with mean and covariance
%
\begin{equation}
\begin{split}
%
\vec{m}_0 & = \begin{pmatrix}
6500.4 \\
349.14 \\
-1.8093 \\
-6.7967 \\
0.6932 \\
\end{pmatrix} \\
\mat{P}_0 & = \begin{pmatrix}
10^{-6} & 0 & 0 & 0 & 0 \\
0 & 10^{-6}& 0 & 0 & 0 \\
0 & 0 & 10^{-6} & 0 & 0 \\
0 & 0 & 0 & 10^{-6}& 0 \\
0 & 0 & 0 & 0 & 0
\end{pmatrix},
%
\end{split}
\end{equation}
%
that is, vehicle's aerodynamic properties were not known precisely beforehand. 

The radar, which is located at $(s_x, s_y) = (R_0,0)$, is used to
measure the range $r_k$ and bearing $\theta_k$ in relation to the
vehicle on time step $k$. Thus, the measurement model can be written as
%
\begin{equation}
\begin{split}
%
r_k & = \sqrt{(x_1(k) - s_x)^2 + (x_2(k) - s_y)^2} + q_1(k) \\
\theta_k & = \tan^{-1} \left( \frac{x_2(k) - s_y}{x_1(k) - s_x}\right)
+q_2(k),
%
\end{split}
\end{equation}
%
where the measurement noise processes $q_1(k)$ and $q_2(k)$ are
Gaussians with zero means and standard deviations $\sigma_{r} =
10^{-3} \text{km}$ and $\sigma_{\theta} = 0.17 \text{mrad}$,
respectively. The derivatives of $\theta_k$ with respect to state
variables can computed with equations (\ref{eq:bot_dx}). For $r_k$ the
derivatives can be written as
%
\begin{equation}
\begin{split}
\frac{\partial r_k}{\partial x_1(k)} & = x_1(k) \frac{1}{r_k} \\
\frac{\partial r_k}{\partial x_2(k)} & = x_2(k) \frac{1}{r_k}.
\end{split}
\end{equation}
%

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/reentry_trajectory}
\caption{Sample vehicle trajectory, earth and position of radar in
  Reentry Vehicle Tracking problem.}
\label{fig:reentry_traj}
\end{center}
\end{figure}

In the table \ref{table:reentry_rmse} we have listed the RMS errors of
position estimates with tested methods, which were
%
\begin{itemize}
%
\item EKF1: first order extended Kalman filter. 
\item ERTS: first order Rauch-Tung-Striebel smoother.
\item UKF: augmented form unscented Kalman filter.
\item URTS1: unscented Rauch-Tung-Striebel smoother with non-augmented
sigma points.
\item URTS2: unscented Rauch-Tung-Striebel smoother with augmented
sigma points.
\item UTF: unscented Forward-Backward smoother.
%
\end{itemize}
%
Extended Forward-Backward smoother was also tested, but it produced
in many cases divergent estimates, so it was left out from
comparison. Second order EKF was also left out, because
evaluating the Hessians would have taken too much work while considering the
fact, that the estimates might have gotten even worse.  

From the error estimates we can see, that EKF and UKF
give almost identical performances, altough in the article (Julier and
Uhlmann, 2004b) UKF was clearly superior over EKF. The reason for this
might be the fact that they used numerical approximations (central
differences) for calculating the Jacobian in EKF rather than
calculating the derivatives in closed form, as was done in this demonstration. 

In figure \ref{fig:reentry_errors} we have plotted the mean square errors and
variances in estimating $x_1$, $x_3$ and $x_5$ with EKF and ERTS over 100
Monte Carlo runs. It shows that using smoother always gives better
estimates for positions and velocities, but for $x_5$ the errors are
practically the same after $\sim 45$ seconds. This also shows that both methods are pessimistic
in estimating $x_5$, because variances were bigger than the true
errors. Figures for $x_2$ and $x_4$ are not shown, because
they are very similar to the ones of $x_1$ and $x_3$. Also by using UKF and URTS the
resulting figures were in practically identical, and therefore left out.
% 
%  
\begin{table}
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\it Method}&{\it RMSE } \\
\hline
EKF1  & 0.0083 \\
ERTS & 0.0043 \\
UKF   & 0.0083 \\
URTS1 & 0.0043 \\
URTS2 & 0.0043 \\ 
UTF  & 0.0044 \\
\hline 
\end{tabular}
\caption{Average RMSEs of estimating the position in Reentry Vehicle
  Tracking problem over 100 Monte Carlo runs.}
\label{table:reentry_rmse}
\end{center}
\end{table} 
%
\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/reentry_errors}
\caption{MSEs and variances in estimating of $x_1$, $x_3$ and $x_5$ using EKF and ERTS over
  100 Monte Carlo runs.}
\label{fig:reentry_errors}
\end{center}
\end{figure}

\newpage
\clearpage

\section{Multiple Model Systems}

In many practical scenarios it is reasonable to assume that the
the system's model can change through time somehow. For example, a fighter
airplane, which in normal situation flies with stabile flight dynamics,
might commence rapid maneuvers when approached by a hostile
missile, or a radar can have a different SNR in some regions of space
than in others, and so on. Such varying system characteristics are
hard to describe with only a one certain model, so in estimation
one should somehow take into account the possibility that the system's model might
change. 

We now consider systems, whose current model is one from a discrete
set of $n$ models, which are denoted by $M = \{M^1,\ldots,M^n\}$. We assume that for
each model $M^j$ we have some prior probability $\mu_0^j = P\{M_0^j\}$. Also
the probabilities of switching from model $i$ to model $j$ in next
time step are assumed to be known and denoted by $p_{ij} =
P\{M_k^j|M_{k-1}^i\}$. This can be seen as a transition
probability matrix of a first order Markov chain characterizing the
mode transitions, and hence systems of this type are commonly referred as
{\it Markovian switching systems}. 
The optimal approach to filtering the states of multiple model
system of this type requires running optimal filters for every possible model
sequences, that is, for $n$ models $n^k$ optimal filters must be ran
to process the $k$th measurement. Hence, some kind of approximations
are needed in practical applications of multiple model systems.

% One approach to handling the large number of hypotheses in the
% filtering problems is the Generalized Pseudo-Bayesian (GPB)
% algorithms ( 
In this section we describe the Interacting Multiple Model (IMM) filter
(Bar-Shalom et al., 2001), which is a popular method for estimating
systems, whose model changes according to a finite-state,
discrete-time Markov chain. IMM filter can also be used in situations,
in which the unknown system model structure or it's parameters are
estimated from a set of candidate models, and hence it can be also
used as a method for model comparison. 

As previously we start with linear models, and after that we review
the EKF and UKF based nonlinear extensions to the standard IMM-filter
through demonstrating filtering problems. 


\subsection{Linear Systems}

We can now modify the equations of linear systems described in
(\ref{eq:kalman_model}) to have form 
%
\begin{equation}
\begin{split}
  \vec{x}_{k} &= \mat{A}_{k-1}^j \, \vec{x}_{k-1} + \vec{q}_{k-1}^j \\
  \vec{y}_{k} &= \mat{H}_{k}^j   \, \vec{x}_{k}   + \vec{r}_{k}^j,
\end{split}
\label{eq:imm_lin_model}
\end{equation}
%
where now we have denoted by $j$ the model (or mode) which is in effect during
the time step $k-1$. Conditioned on the currently active mode we can
use the classical Kalman filter (section 2.1.2) for estimating the state of the
system on each time step. However, the active mode of the system is not
usually known, so we must estimate it also.

\subsubsection{Interacting Multiple Model Filter}

IMM-filter (Bar-Shalom et al., 2001) is a computationally efficient
and in many cases well performing suboptimal estimation algorithm for
Markovian switching systems of type described above. Basically it
consists of three major steps: interaction (mixing), filtering and
combination. In each time step we
obtain the initial conditions for certain model-matched filter by
mixing the state estimates produced by all filters from the previous
time step under the assumption that this particular model is the right model at
current time step. Then we perform standard Kalman filtering for
each model, and after that we compute a weighted combination of updated state
estimates produced by all the filters yielding a final estimate for
the state and covariance of the Gaussian density in that particular time step. The weights are
chosen according to the probabilities of the models, which are computed
in filtering step of the algorithm.

The equations for each step are as follows:

\begin{itemize}

\item {\em Interaction:}
  
  The mixing probabilities $\mu_k^{i|j}$ for each model $M^i$ and $M^j$ are calculated as
  %
  \begin{eqnarray}
  % 
  \bar{c}_j  & = & \sum_{i=1}^n p_{ij} \mu_{k-1}^i,\\ \label{eq:imm_cj}
  \mu_k^{i|j} & = & \frac{1}{\bar{c}_j} p_{ij} \mu_{k-1}^i,
  % 
  \end{eqnarray}
  %
  where $\mu_{k-1}^i$ is the probability of model $M^i$ in the time
  step $k-1$ and $\bar{c}_j$ a normalization factor.

  Now we can compute the mixed inputs (that is, means and covariances) for each filter as
  %
  \begin{eqnarray}
  %
  \vec{m}_{k-1}^{0j} & = & \sum_{i=1}^n \mu_k^{i|j} \vec{m}_{k-1}^i, \\
  %
  \mat{P}_{k-1}^{0j} & = & \sum_{i=1}^n \mu_k^{i|j} \times \left\{
    \mat{P}_{k-1}^i + \left[\vec{m}_{k-1}^i-\vec{m}_{k-1}^{0j}\right]\left[\vec{m}_{k-1}^i-\vec{m}_{k-1}^{0j}\right]^T\right\},
  %
  \end{eqnarray}
  %
  where $\vec{m}_{k-1}^i$ and $\mat{P}_{k-1}^i$ are the updated mean
  and covariance for model $i$ at time step $k-1$. 

\item {\em Filtering:}

  Now, for each model $M^i$ the filtering is done as
  %
  \begin{eqnarray}
  %
  \left[\vec{m}_{k}^{-,i},\mat{P}_{k}^{-,i}\right] & = &
  \text{KF}_p(\vec{m}_{k-1}^{0j},\mat{P}_{k-1}^{0j},\mat{A}_{k-1}^i,\mat{Q}_{k-1}^i) , \\ \label{eq:imm_predict}
  % 
  \left[\vec{m}_{k}^{i},\mat{P}_{k}^{i}\right] & = & \text{KF}_u(\vec{m}_{k}^{-,i},\mat{P}_{k}^{-,i},\vec{y}_{k},\mat{H}_{k}^i,\mat{R}_{k}^i), \label{eq:imm_update}
  % 
  \end{eqnarray}
  %
  where we have denoted the prediction and update steps (equations
  (\ref{eq:dkf_predict}) and (\ref{eq:dkf_update})) of the
  standard Kalman filter with $\text{KF}_p(\cdot)$ and
  $\text{KF}_u(\cdot)$, correspondingly. In addition to mean and
  covariance we also compute the likelihood of the measurement for
  each filter as
  %
  \begin{equation}
  %
  \Lambda_k^i = \text{N}(\vec{v}_k^i;0,\mat{S}_k^i),
  %
  \end{equation}
  % 
  where $\vec{v}_k^i$ is the measurement residual and $\mat{S}_k^i$ it's
  covariance for model $M^i$ in the KF update step.

  The probabilities of each model $M^i$ at time step $k$ are
  calculated as 
  %
  \begin{eqnarray}
  %
  c & = & \sum_{i=1}^n \Lambda_k^i \bar{c}_i, \\  \label{eq:imm_c}
  \mu_k^i & = & \frac{1}{c} \Lambda_k^i \bar{c}_i, \label{eq:imm_muk}
  %
  \end{eqnarray} 
  % 
  where $c$ is a normalizing factor.

\item {\em Combination:}
  
  In the final stage of the algorithm the combined estimate for the
  state mean and covariance are computed as
  %
  \begin{eqnarray}
  %
  \vec{m}_k & = & \sum_{i=1}^n \mu_k^i \vec{m}_{k}^{i}, \\
  %
  \mat{P}_k & = & \sum_{i=1}^n \mu_k^i \times \left\{\mat{P}_{k}^{i} \left[\vec{m}_{k}^{i} -\vec{m}_k\right] \left[\vec{m}_{k}^{i} -\vec{m}_k\right]^T\right\}.
  %
  \end{eqnarray}
  %
  
\end{itemize}

In this toolbox prediction and updates steps of the IMM-filter can be computed with functions
\texttt{imm\_kf\_predict} and \texttt{imm\_kf\_update}. For convience
we have also provided function \texttt{imm\_filter}, which combines the two previously mentioned
steps.


\subsubsection{Interacting Multiple Model Smoother}

Likewise in the single model case also it is useful to smooth the state estimates
of the IMM filter by using all the obtained measurements. Since
the optimal fixed-interval smoothing with $n$ models and $N$ measurements
requires running $n^N$ smoothers we must resort to suboptimal
approaches. One possibility (Helmick, 1995) is to combine the estimates of two IMM
filters, one running forwards and the other backwards in time. This
approach is restricted to systems having invertible state
dynamics (i.e. system for which the inverse of matrix $\mat{A}^j$ in
(\ref{eq:imm_lin_model}) exists), which is always the case for discretized
continuous-time systems.

First we shall review the equations for the IMM-filter running
backwards in time, and then the actual smoothing equations combining
the estimates of the two filters.

\subsubsection*{Backward-time IMM-filter}

Our aim is now to compute the backward filtering density $p(\vec{x}_k|\vec{y}_{k:N})$ for
each time step, which is expressed as a sum of model conditioned
densities:
% 
\begin{equation}
  % 
  p(\vec{x}_k|\vec{y}_{k:N}) = \sum_{j=1}^{n} \mu_k^{b,j}
  p(\vec{x}_k^j|\vec{y}_{k:N}), \label{eq:imm_backward_total_density}
  % 
\end{equation}
% 
where $\mu_k^{b,j}$ is the backward-time filtered model probability of
$M_k^j$. In the last time step $N$ this is the same as the forward
filter's model probability, that is, $\mu_N^{b,j} = \mu_N^{j}$. 
Assuming the model conditioned densities
$p(\vec{x}_k^j|\vec{y}_{k:N})$ are Gaussians the backward density
in (\ref{eq:imm_backward_total_density}) is a mixture of Gaussians,
which is now going to be approximated with a single Gaussian via moment matching.

The model conditioned backward-filtering densities can be expressed as
%
\begin{equation}
%
p(\vec{x}_k^j|\vec{y}_{k:N}) = \frac{1}{c} p(\vec{y}_{k:N}|\vec{x}_k^j)
p(\vec{x}_k^j|\vec{y}_{k+1:N}),
%
\end{equation}
%
where $c$ is some normalizing constant, $p(\vec{y}_{k:N}|\vec{x}_k^j)$ the
model-conditioned measurement likelihood and
$p(\vec{x}_k^j|\vec{y}_{k+1:N})$
is the model-conditioned density of the state given the future
measurements.
The latter density is expressed as 
%
\begin{equation}
%
p(\vec{x}_k^j|\vec{y}_{k+1:N}) = \sum_{i=1}^n \mu_{k+1}^{b,i|j}
p(\vec{x}_k^j|M_{k+1}^i,\vec{y}_{k+1:N}), \label{eq:imm_back_modelc_predict}
%
\end{equation}
%
where $\mu_{k+1}^{b,i|j}$ is the conditional model probability
computed as
%
\begin{eqnarray}
%
\mu_{k+1}^{b,i|j} & = & P\{M_{k+1}^i|M_k^j,\vec{y}_{k+1:N} \} \\ \label{eq:imms_cond_mod_prob}
   & = & \frac{1}{a_j} p_{ij}^{b,k} \mu_{k+1}^{b,i},
%
\end{eqnarray}
%
where $a_j$ is a normalization constant given by
%
\begin{equation}
%
a_j = \sum_{i=1}^{n} p_{ij}^{b,k} \mu_{k+1}^{b,i}. \label{eq:imms_constant_a}
%
\end{equation}
%
The backward-time transition probabilities of
switching from model $M_{k+1}^i$ to model $M_k^j$ in
(\ref{eq:imms_cond_mod_prob}) and (\ref{eq:imms_constant_a}) are defined as
$p_{ij}^{b,k} = P\{M_k^j|M_{k+1}^i\}$. The prior model probabilities
can be computed off-line recursively for each time step $k$ as 
%
\begin{eqnarray}
%
P\{M_k^j\} & = & \sum_{i=1}^n P\{M_k^j|M_{k-1}^i\} P\{M_{k-1}^i\} \\
           & = & \sum_{i=1}^n p_{ij} P\{M_{k-1}^i\}
%
\end{eqnarray}
%
and using these we can compute $p_{ij}^{b,k}$ as 
%
\begin{equation}
%
p_{ij}^{b,k} = \frac{1}{b_i} p_{ji} P\{M_{k}^j\},
%
\end{equation}
%
where $b_i$ is the normalizing constant
%
\begin{equation}
%
b_i = \sum_{j=1}^n p_{ji} P \{ M_k^j\}.
%
\end{equation}
%

The density $p(\vec{x}_k^j|M_{k+1}^i,\vec{y}_{k+1:N}^N)$ is now approximated
with a Gaussian $N(\vec{x}_k|\vec{m}_{k|k+1}^{b,i},\mat{P}_{k|k+1}^{b,-(i)})$,
where the mean and covariance are given by the Kalman filter
prediction step using the inverse of the state transition matrix:
%
\begin{equation}
%
  \left[\vec{\hat{m}}_{k}^{b,i},\mat{\hat{P}}_{k}^{b,i}\right] =
  \text{KF}_p(\vec{m}_{k+1}^{b,i},\mat{P}_{k+1}^{b,i},(\mat{A}_{k+1}^i)^{-1},\mat{Q}_{k+1}^i). \label{eq:imm_b_predict}
%
\end{equation}
%
The density $p(\vec{x}_k^j|\vec{y}_{k+1:N})$ in
(\ref{eq:imm_back_modelc_predict}) is a mixture of Gaussians, and it's
now approximated with a single Gaussian as
%
\begin{equation}
%
p(\vec{x}_k^j|\vec{y}_{k+1:N}) =
N(\vec{x}_k^j|\vec{\hat{m}}_{k}^{b,0j},\mat{\hat{P}}_{k}^{b,0j}),
%
\end{equation}
%
where the mixed predicted mean and covariance are given as
%
\begin{eqnarray}
%
\vec{\hat{m}}_{k}^{b,0j} & = & \sum_{i=1}^n \mu_{k+1}^{b,i|j}
\vec{\hat{m}}_{k}^{b,i} \\
%
\mat{\hat{P}}_{k}^{b,0j} & = & \sum_{i=1}^n \mu_{k+1}^{b,i|j} \cdot
\left[ \mat{\hat{P}}_{k}^{b,i} +\left( \vec{\hat{m}}_{k}^{b,i} -
    \vec{\hat{m}}_{k}^{b,0j} \right) \left( \vec{\hat{m}}_{k}^{b,i} -
    \vec{\hat{m}}_{k}^{b,0j} \right)^T\right]
%
\end{eqnarray}
%

Now, the filtered density $p(\vec{x}_k^j|\vec{y}_{k:N})$
is a Gaussian $N(\vec{x}_k|\vec{m}_{k}^{b,j},\mat{P}_{k}^{b,j})$, and
solving it's mean and covariance corresponds to running the Kalman filter
update step as follows: 
%
\begin{equation}
%
  \left[\vec{m}_{k}^{b,j},\mat{P}_{k}^{b,j}\right] =
  \text{KF}_u(\vec{\hat{m}}_{k}^{b,0j},\mat{\hat{P}}_{k}^{b,0j},\vec{y}_k,\mat{H}_k^j,\mat{R}_k^j). \label{eq:imm_b_update}  
%
\end{equation}
%
The measurement likelihoods for each model are computed as
% 
\begin{equation}
  % 
  \Lambda_k^{b,i} = \text{N}(\vec{v}_k^{b,i};0,\mat{S}_k^{b,i}),
  % 
\end{equation}
  % 
where $\vec{v}_k^{b,i}$ is the measurement residual and $\mat{S}_k^{b,i}$ it's
covariance for model $M^i$ in the KF update step.
%
With these we can update the model probabilities for time
step $k$ as 
%
\begin{equation}
%
\mu_k^{b,j} = \frac{1}{a} a_j \Lambda_k^{b,i},
%
\end{equation}
%
where $a$ is a normalizing constant
%
\begin{equation}
%
a = \sum_{j=1}^{m} a_j \Lambda_k^{b,i}.
%
\end{equation}
%
Finally, we can form the Gaussian approximation to overall backward
filtered distribution as
%
\begin{equation}
%
p(\vec{x}_k|\vec{y}_{k:N}) = N(\vec{x}_k|\vec{m}_k^b,\mat{P}_{k}^b),
%
\end{equation}
%
where the mean and covariance are mixed as
%
\begin{eqnarray}
%
\vec{m}_k^b & = & \sum_{j=1}^n \mu_k^{b,j} \vec{m}_k^{b,j} \\
%
\mat{P}_{k}^b & = & \sum_{j=1}^{n} \mu_k^{b,j} \left[ \mat{P}_k^{b,j} +
  \left( \vec{m}_k^{b,j} - \vec{m}_k^{b} \right)
  \left( \vec{m}_k^{b,j} - \vec{m}_k^{b}\right)^T \right].
%
\end{eqnarray}
%

\subsubsection*{Two-filter based fixed-interval IMM-Smoother}

We can now proceed to evaluate the fixed-interval smoothing
distribution
%
\begin{equation}
%
p(\vec{x}_k|\vec{y}_{1:N}) = \sum_{j=1}^n \mu_k^{s,j} p(\vec{x}_k^j|\vec{y}_{1:N}), \label{eq:imm_smooth_density}
%
\end{equation}
%
where the smoothed model probabilities are computed as 
%
\begin{eqnarray}
%
\mu_k^{s,j} & = & P\{ M_k^j|\vec{y}_{1:N}\} \\
            & = & \frac{1}{d} d_j \mu_k^j,
%
\end{eqnarray}
%
where $\mu_k^j$ is the forward-time filtered model probability,
$d_j$ the density $d_j = p(\vec{y}_{k+1:N}|M_k^j,\vec{y}_{1:k})$
(which is to be evaluated later) and $d$ the normalization constant given by
%  
\begin{equation}
%
d = \sum_{j=1}^{n} d_j \mu_k^j.
%
\end{equation}
%

The model-conditioned smoothing distributions $p(\vec{x}_k^j|\vec{y}_{1:N})$ in
\eqref{eq:imm_smooth_density} are expressed as a mixtures of Gaussians
%
\begin{equation}
%
p(\vec{x}_k^j|\vec{y}_{1:N}) = \sum_{i=1}^{n} \mu_{k+1}^{s,i|j}
p(\vec{x}^i|M_{k+1}^j,\vec{y}_{1:n}), \label{eq:imm_smooth_modelcs}
%
\end{equation}
%
where the conditional probability $\mu_{k+1}^{s,i|j}$ is given by
%
\begin{eqnarray}
%
\mu_{k+1}^{s,i|j} & = & P\{ M_{k+1}^i| M_k^j, \vec{y}_{1:n}\} \\
                  & = & \frac{1}{d_j} p_{ji} \Lambda_k^{ji}
%
\end{eqnarray}
%
and the likelihood $\Lambda_k^{ji}$ by
%
\begin{equation}
%
\Lambda_k^{ji} = p(\vec{y}_{k+1:N}|M_k^j,M_{k+1}^i,\vec{y}_{1:k}).
%
\end{equation}
%
We approximate this now as
%
\begin{equation}
%
\Lambda_k^{ji} \approx p(\vec{\hat{x}}_k^{b,i}|M_k^j,M_{k+1}^i,\vec{x}_{k}^j),
%
\end{equation}
%
which means that the future measurements $\vec{y}_{k+1:N}$ are
replaced with the $n$ model-conditioned backward-time one-step predicted means and
covariances $\{\vec{\hat{m}}_k^{b,i},\vec{\hat{P}}_k^{b,i}\}_{r=1}^{n}$, and $\vec{y}_{1:k}$ will be replaced by the $n$
model-conditioned forward-time filtered means and covariances
$\{\vec{m}_k^{i}|\vec{P}_k^{i}\}_{r=1}^{n}$. It follows then that
the likelihoods can be evaluated as
%
\begin{equation}
%
\Lambda_k^{ji} = N(\Delta_k^{ji}|0,D_k^{ji}),
%
\end{equation}
%
where
%
\begin{eqnarray}
%
\Delta_k^{ji} & = & \vec{\hat{m}}_k^{b,i} - \vec{m}_k^j \\
%
D_k^{ji} & = & \vec{\hat{P}}_k^{b,i} + \vec{P}_k^j.
%
\end{eqnarray}
%
The terms $d_j$ can now be computed as
%
\begin{equation}
%
d_j = \sum_{i=1}^n p_{ji} \Lambda_k^{ji}.
%
\end{equation}
% 
The smoothing distribution $p(\vec{x}_k^j|M_{k+1}^i,\vec{y}_{1:N})$ of
the state matched to the models $M_k^j$ and $M_{k+1}^i$ over two
successive sampling periods can be expressed as
%
\begin{equation}
%
p(\vec{x}_k^j|M_{k+1}^i,\vec{y}_{1:N}) = \frac{1}{c} p(\vec{y}_{k+1:N}|M_{k+1}^i,\vec{x}_k) p(\vec{x}_k^j|\vec{y}_{1:k}),
%
\end{equation}
% 
where $p(\vec{y}_{k+1:N}|M_{k+1}^i,\vec{x}_k)$ is the forward-time
model-conditioned filtering distribution,
$p(\vec{x}_k^j|\vec{y}_{1:k})$ the backward-time one-step predictive
distribution and $c$ a normalizing constant. Thus, the smoothing distribution can
be expressed as
%
\begin{eqnarray}
%
p(\vec{x}_k^j|M_{k+1}^i,\vec{y}_{1:N}) & \propto& 
N(\vec{x}_k|\vec{\hat{m}}_k^{b,i},\vec{\hat{P}}_k^{b,i}) \cdot
N(\vec{x}_k|\vec{m}_k^{i},\vec{P}_k^{i}) \\
& = & N(\vec{x}_k|\vec{m}_k^{s,ji},\vec{P}_k^{s,ji}),
%
\end{eqnarray}
%
where 
% 
\begin{eqnarray}
%  
\vec{m}_k^{s,ji} & = & \mat{P}_k^{s,ji} \left[
  \left(\vec{P}_k^{i}\right)^{-1}\vec{m}_k^{i} +
  \left(\vec{\hat{P}}_k^{b,i}\right)^{-1}\vec{\hat{m}}_k^{b,i} \right]
\\
%
\mat{P}_k^{s,ji} & = & \left[ \left(\vec{P}_k^{i}\right)^{-1} + \left(\vec{\hat{P}}_k^{b,i}\right)^{-1} \right]^{-1}.
%
\end{eqnarray}
%

The model-conditioned smoothing distributions
$p(\vec{x}_k^j|\vec{y}_{1:N})$, which were expressed as mixtures of
Gaussians in \eqref{eq:imm_smooth_modelcs}, are now approximated by a
single Gaussians via moment matching to yield
%
\begin{equation}
%
p(\vec{x}_k^j|\vec{y}_{1:N}) \approx N(\vec{x}_k^j|\vec{m}_k^{s,j},\mat{P}_k^{s,j}),
%
\end{equation}
%
where
%
\begin{eqnarray}
%
\vec{m}_k^{s,j} & = & \sum_{i=1}^{n} \mu_{k+1}^{s,i|j}
\vec{m}_k^{s,ji} \\
%
\mat{P}_k^{s,j} & = & \sum_{i=1}^n \mu_{k+1}^{s,i|j} \cdot
\left[ \mat{P}_{k}^{s,ij} + \left( \vec{m}_{k}^{s,ij} -
    \vec{m}_{k}^{s,j} \right) \left( \vec{m}_{k}^{s,ij} -
    \vec{m}_{k}^{s,j} \right)^T\right].
%
\end{eqnarray}
%
With these we can match the moments of the overall smoothing distribution
to give a single Gaussian approximation
%
\begin{equation}
%
p(\vec{x}_k|\vec{y}_{1:N}) \approx N(\vec{x}_k|\vec{m}_k^s,\mat{P}_k^s), 
%
\end{equation}
%
where
%
\begin{eqnarray}
%
\vec{m}_k^s & = & \sum_{j=1}^{n} \mu_{k}^{s,j}
\vec{m}_k^{s,ji} \\
%
\mat{P}_k^{s} & = & \sum_{j=1}^n \mu_{k}^{s,j} \cdot
\left[ \mat{P}_{k}^{s,j} + \left( \vec{m}_{k}^{s,j} -
    \vec{m}_k^s \right) \left( \vec{m}_{k}^{s,j} -
    \vec{m}_k^s \right)^T\right].
%
\end{eqnarray}
%

These smoothing equations can be computed with function \texttt{imm\_smooth}.

\subsubsection{Demonstration: Tracking Object with Simple Manouvers}

A moving object with simple manouvers can be modeled by a Markovian
switching system, which describes the normal movement dynamics with a
simple Wiener process velocity model (see
section 2.2.9) with low process noise value, and the manouvers with a
Wiener process acceleration model (see section 2.1.4) with high
process noise value. In the following we shall refer the former as
model 1 and the latter as the model 2, which could actually also be
a velocity model, but we use acceleration model instead to demonstrate
how models with different structure are used in this toolbox. 

The variance of process noise for model 1 is set to 
%
\begin{equation}
\mat{Q}_c^1 = 
\begin{pmatrix}
q_1 & 0 \\
0   & q_1
\end{pmatrix}
= 
\begin{pmatrix}
0.01 & 0 \\
0   & 0.01
\end{pmatrix}
\end{equation}
%
and for model 2 to 
%
\begin{equation}
\mat{Q}_c^2 = 
\begin{pmatrix}
q_2 & 0 \\
0   & q_2
\end{pmatrix}
= 
\begin{pmatrix}
1 & 0 \\
0   & 1
\end{pmatrix}
\end{equation}
%
In both cases the measurement model is the same as in the section
2.1.4 (that is, we observe the position of the object directly) with the exception
that the variance of the measurements is now set to 
%
\begin{equation}
\mat{R} = \begin{pmatrix}
0.1 & 0  \\
0  & 0.1 \\
\end{pmatrix}.
\end{equation}
%
The time step size is set to $\dt = 0.1$. The true starting state of the system
is set to 
%
\begin{equation}
%
\vec{x}_0 = \begin{bmatrix} 0 & 0 & 0 & -1& 0 & 0\end{bmatrix},
%
\end{equation}
%
which means that the object starts to move from origo with velocity
$-1$ along the y-axis.
%
The model transition probability matrix is set to 
%
\begin{equation}
%
\mat{\Phi} = 
\begin{pmatrix}
0.98 & 0.02 \\
0.02 & 0.98
\end{pmatrix}, \label{eq:imm_mtpm}
%
\end{equation}
%
which means that both models have equal probability of shifting to
another model during each sampling period. The prior model probabilities are set to
%
\begin{equation}
%
\vec{\mu}_0 = \begin{bmatrix} 0.9 & 0.1 \end{bmatrix}. \label{eq:imm_modelprior}
%
\end{equation}
%

In software code the filtering loop of IMM filter can be done as follows:
%
\begin{verbatim}
for i = 1:size(Y,2)
    [x_p,P_p,c_j] = imm_predict(x_ip,P_ip,mu_ip,p_ij,ind,dims,A,Q);
    [x_ip,P_ip,mu_ip,m,P] = imm_update(x_p,P_p,c_j,...
                                       ind,dims,Y(:,i),H,R);
    MM(:,i)   = m;
    PP(:,:,i) = P;
    MU(:,i)   = mu_ip';
    MM_i(:,i) = x_ip';
    PP_i(:,i) = P_ip';
end
\end{verbatim}
%
The variables \texttt{x\_ip} and \texttt{P\_ip} contain the updated
state mean and covariance for both models as cell arrays. The cell
arrays are used because the size of the state variable can vary between
different models. For example, in this example the models have 4 and 6 state components.
Similarly, the variables \texttt{x\_p} and \texttt{P\_p} contain the predicted
state means and covariances. Likewise, the model parameters (variables
\texttt{A},\texttt{Q},\texttt{H} and \texttt{R}) are all cell arrays
containing the corresponding model parameters for each model. The
variable \texttt{mu\_ip} contains the
probability estimates of each model as a regular array, which is
initialized before the filtering loop with the prior probabilities of
models (eq. \eqref{eq:imm_modelprior}). The variable \texttt{p\_ij} is
the model transition probability matrix \eqref{eq:imm_mtpm} as a
regular Matlab matrix. The vector \texttt{c\_j} contains the
normalizing constants computed during the prediction step
(eq. \eqref{eq:imm_cj}), which are needed during the update step in
eqs. \eqref{eq:imm_c} and \eqref{eq:imm_muk}. The variable
\texttt{ind} is a cell array containing vectors, which map the state variables of each
model to state variables of the original system under the study. For
example, for model 1 the vector is \texttt{[1 2 3 4]'} and for model 2
\texttt{[1 2 3 4 5 6]'}. The purpose of this might seem a bit unclear
for systems like these, but for more complex models we must know how
the state variables are connected as they are not always in the same
order and some components might be missing. The variable \texttt{dims}
contains the number of dimensions in
the original system under the study, which in this case is 6. The last five lines of code store
the estimation results for each time step.

The smoothing can be done with the function call
\begin{verbatim}
[SM,SP,SM_i,SP_i,MU_S] = imm_smooth(MM,PP,MM_i,PP_i,MU,p_ij,...
                                    mu_0j,ind,dims,A,Q,R,H,Y);
\end{verbatim}
where the variable \texttt{mu\_0j} contains the prior model
probabilities. The overall smoothed state estimates are 
returned in variables \texttt{SM} and \texttt{SP}, and the
model-conditioned smoothed estimates in variables \texttt{SM\_i} and
\texttt{SP\_i}. The variable \texttt{MU\_S} contains the calculated smoothed model
probabilities.

The system is simulated $200$ time steps, and the active
model during the steps $1-50$, $71-120$ and $151-200$ is set to model 1 and
during the steps $51-70$ and $121-150$ to model 2. The purpose of forcing
the model transitions instead of simulating them randomly is to produce two
manouvers, which can be used to demonstrate the properties of
IMM-filter. It also reflects the fact that in real problems we do not
know the model transition probability matrix accurately. 
The figures \ref{fig:imm_kf1}, \ref{fig:imm_kf2} and
\ref{fig:imm_imm} show the true trajectory of the object and the 
measurements made of it. The two manouvers can be clearly seen. 
Figures also show the filtering and smoothing results produced by
Kalman filter and (RTS) smoother using the models 1 and 2 separately (figures
\ref{fig:imm_kf1} and \ref{fig:imm_kf2}, correspodingly) as well as the estimates
produced by the IMM filter and smoother (figure \ref{fig:imm_imm}).
In figure \ref{fig:imm_model_probability} we have plotted the filtered
and smoothed estimates of the probabilities of model 1 in each time
step. It can be seen that it takes some time for the filter to respond
to model transitions. As one can expect, smoothing reduces this
lag as well as giving substantially better overall performance. 

The illustrate the performance differences between different
models we have listed the MSE errors of position estimates for each
model in the table \ref{table:imm_rmse}. From these it can be seen that
the estimates of Kalman filter with model 1 are clearly much worse
than with model 2 or with the IMM filter. On the otherhand, the difference
between the KF with model 2 and the IMM filter is smaller, but still
significant in the favor of IMM. This is most likely due to fact that
model 2 is more flexible than model 1, so model 2 is better in the
regions of model 1 than model 1 in the regions of model 2. The bad
performance of the model 1 can be seen especially during the
manouvers. The IMM filter combines the properties of both models in a
suitable way, so it gives the best overall performance. The effect of
smoothing to tracking accuracy is similar with all models.

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/imm1}
\caption{Tracking results of Kalman filter and smoother with model 1
in Tracking Object with Simple Manouvers example.}
\label{fig:imm_kf1}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/imm2}
\caption{Tracking results of Kalman filter and smoother with model 2
in Tracking Object with Simple Manouvers example.}
\label{fig:imm_kf2}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/imm3}
\caption{Tracking results of IMM filter and smoother
in Tracking Object with Simple Manouvers example.}
\label{fig:imm_imm}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/imm4}
\caption{Filtered and smoothed probability of model 1 in each time step
in Tracking Object with Simple Manouvers example.}
\label{fig:imm_model_probability}
\end{center}
\end{figure}

% 
%  
\begin{table}
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\it Method}&{\it MSE } \\
\hline
KF1  & 0.1554 \\
KS1  & 0.0314 \\
KF2  & 0.0317 \\
KS2  & 0.0071 \\
IMM  & 0.0229 \\ 
IMMS & 0.0057 \\
\hline 
\end{tabular}
\caption{Average MSEs of estimating the position
in Tracking Object with Simple Manouvers example over 1000 Monte Carlo
runs.}
\label{table:imm_rmse}
\end{center}
\end{table} 
%

\subsection{Nonlinear Systems}

The non-linear versions of IMM filter and smoother reviewed in
previous section can be obtained simply by replacing the Kalman filter
prediction and update steps (in eqs. \eqref{eq:imm_predict},
\eqref{eq:imm_update}, \eqref{eq:imm_b_predict} and
\eqref{eq:imm_b_update}) by their extended Kalmam filter or unscented
Kalman filter counterparts, which were reviewed in
sections 2.2.2 and 2.2.6. These algorithms are commonly referred as
IMM-EKF and IMM-UKF. Naturally this approach introduces some
error to estimates of already suboptimal IMM, but can still provide
sufficient accuracy with suitable models. 
% It is also possible to
% construct a particle filter based IMM filter for estimating more
% non-linear non-Gaussian models, but that approach is not currently
% provided in this toolbox.


The prediction and update steps of IMM-EKF is provided by
functions \texttt{eimm\_predict} and \texttt{eimm\_update}, and
smoothing can be done with function \texttt{eimm\_smooth}. The
corresponding functions for IMM-UKF are
\texttt{uimm\_predict}, \texttt{uimm\_update} and \texttt{uimm\_smooth}.


\subsubsection{Demonstration: Coordinated Turn Model}

A common way of modeling a turning object is to use the {\it coordinated
turn model} (see, e.g., Bar-Shalom et al., 2001). The idea is to augment
the state vector with a turning rate $\omega$, which is to be estimated
along with the other system
parameters, which in this example are the position and the velocity of
the target. Thus, the joint system vector can be expressed as
%
\begin{equation}
\vec{x}_k = 
\begin{pmatrix}
x_k & y_k & \dot{x}_k & \dot{y}_k & \omega_k
\end{pmatrix}^T.
%
\end{equation}
%


% In the coordinate turn model we leave out the accelerations, so the system
% vector can be written as
% %
% \begin{equation}
% \vec{x}_k = 
% \begin{pmatrix}
% x_k & y_k & \dot{x}_k & \dot{y}_k & \omega_k
% \end{pmatrix}^T.
% %
% \end{equation}
% %
The dynamic model for the coordinated turns is
% %
% \begin{equation}
% %\begin{split}
% %
% \mat{F}_k = \begin{pmatrix}
% 1 & 0 & \frac{\sin(\omega_{k} \dt)}{\omega_{k}} & \frac{\cos(\omega_{k} \dt) -1}{\omega_{k}} & 0 \\
% 0 & 1 & \frac{1-\cos(\omega_{k} \dt)}{\omega_{k}} & \frac{\sin(\omega_{k}
%   \dt)}{\omega_{k}} & 0 \\
% 0 & 0 & \cos(\omega_{k} \dt) & -\sin(\omega_{k} \dt) & 0 \\
% 0 & 0 & \sin(\omega_{k} \dt) & \cos(\omega_{k} \dt) & 0 \\
% 0 & 0 & 0   & 0 & 1\\
% \end{pmatrix},
% %
% %\end{split}
% \end{equation}
% % 
%
\begin{equation}
%\begin{split}
%
\vec{x}_{k+1} = \underbrace{\begin{pmatrix}
1 & 0 & \frac{\sin(\omega_{k} \dt)}{\omega_{k}} & \frac{\cos(\omega_{k} \dt) -1}{\omega_{k}} & 0 \\
0 & 1 & \frac{1-\cos(\omega_{k} \dt)}{\omega_{k}} & \frac{\sin(\omega_{k}
  \dt)}{\omega_{k}} & 0 \\
0 & 0 & \cos(\omega_{k} \dt) & -\sin(\omega_{k} \dt) & 0 \\
0 & 0 & \sin(\omega_{k} \dt) & \cos(\omega_{k} \dt) & 0 \\
0 & 0 & 0   & 0 & 1\\
\end{pmatrix}}_{\mat{F}_k} \vec{x}_k + \begin{pmatrix}
0 \\
0 \\
0 \\
0 \\
1 \\ 
\end{pmatrix} v_k, \label{eq:ctm_matrix}
% 
%\end{split}
\end{equation}
% 
where $v_k \sim N(0,\sigma_{\omega}^2)$ is univariate white Gaussian process noise for the turn rate
parameter. This model is, despite the
matrix form, non-linear. Equivalently, the dynamic model can be
expressed as a set of equations
%
\begin{equation}
\begin{split}
x_{k+1} & =  x_k + \frac{\sin(\omega_{k} \dt)}{\omega_{k}} \dot{x}_k + \frac{\cos(\omega_{k} \dt) -
  \dt}{\omega_{k}} \dot{x}_k \\
y_{k+1} & =  y_k + \frac{1-\cos(\omega_{k} \dt)}{\omega_{k}} \dot{x}_k + \frac{\sin(\omega_{k}
  \dt)}{\omega_{k}} \dot{y}_k \\
\dot{x}_{k+1} & =  \cos(\omega_{k} \dt) \dot{x}_k - \sin(\omega_{k} \dt)
\dot{y}_k \\
\dot{y}_{k+1} & =  \sin(\omega_{k} \dt) \dot{x}_k + \cos(\omega_{k} \dt)
\dot{y}_k \\
\omega_{k+1} & =  \omega_k + v_k
\end{split}.
\end{equation}
%
The source code of the turning model can found in the m-file
\texttt{f\_turn.m}. The inverse dynamic model can be found in the file
\texttt{f\_turn\_inv.m}, which basically calculates the dynamics using
the inverse $\mat{F}_k$ in eq. \eqref{eq:ctm_matrix} as it's transition matrix.

To use EKF in estimating the turning rate $\omega_k$ the Jacobian
of the dynamic model must be computed, which is given as 
%
\begin{equation}
\mat{F}_{\vec{x}}(\vec{m},k) = \begin{pmatrix}
1 & 0 & \frac{\sin(\omega_{k} \dt)}{\omega_{k}} & \frac{\cos(\omega_{k} \dt) -1}{\omega_{k}} & \frac{\partial x_{k+1}}{\partial \omega_k} \\
0 & 1 & \frac{1-\cos(\omega_{k} \dt)}{\omega_{k}} & \frac{\sin(\omega_{k}
  \dt)}{\omega_{k}} & \frac{\partial y_{k+1}}{\partial \omega_k} \\
0 & 0 & \cos(\omega_{k} \dt) & -\sin(\omega_{k} \dt) & \frac{\partial \dot{x}_{k+1}}{\partial \omega_k} \\
0 & 0 & \sin(\omega_{k} \dt) & \cos(\omega_{k} \dt) & \frac{\partial \dot{y}_{k+1}}{\partial \omega_k} \\
0 & 0 & 0   & 0 & 1\\
\end{pmatrix},
\end{equation}
%
where the partial derivatives w.r.t to turning rate are
%
\begin{equation}
\begin{split}
\frac{\partial x_{k+1}}{\partial \omega_k} & = \frac{\omega \dt
  \cos(\omega_k \dt) - \sin(\omega_k \dt)}{\omega_k^2} \dot{x}_k - \frac{\omega_k \dt
  \sin(\omega_k \dt) + \cos(\omega_k \dt) - 1}{\omega_k^2} \dot{y}_k \\
\frac{\partial y_{k+1}}{\partial \omega_k} & = \frac{\omega_k \dt
  \sin(\omega_k \dt) + \cos(\omega_k \dt) -1}{\omega_k^2} \dot{x}_k - \frac{\omega_k \dt
  \cos(\omega_k \dt) - \sin(\omega_k \dt)}{\omega_k^2} \dot{y}_k \\
\frac{\partial \dot{x}_{k+1}}{\partial \omega_k} & = -\dt
\sin(\omega_k \dt) \dot{x}_k - \dt \cos(\omega_k \dt) \dot{y}_k \\
\frac{\partial \dot{y}_{k+1}}{\partial \omega_k} & = -\dt
\cos(\omega_k \dt) \dot{x}_k - \dt \sin(\omega_k \dt) \dot{y}_k.
\end{split}
\end{equation}
%
The source code for calculating the Jacobian is located in the m-file \texttt{f\_turn\_dx.m}.

Like in previous demonstration we simulate the system 200 time steps
with step size $\dt = 0.1$. The movement of the object is produced
as follows:
%
\begin{itemize}
\item Object starts from origo with velocity $(\dot{x},\dot{y}) =
  (1,0)$.
\item At 4s object starts to turn left with rate $\omega = 1$.
\item At 9s object stops turning and moves straight for 2 seconds with
  a constant total velocity of one.
\item At 11s objects starts to turn right with rate $\omega = -1$.
\item At 16s object stops turning and moves straight for 4 seconds
  with the same velocity.
\end{itemize}
The resulting trajectory is plotted in figure
\ref{fig:eimm1_trajectory}. The figure also shows the measurements,
which were made with the same model as in the previous example, that is,
we observe the position of the object directly with the additive
noise, whose variance in this case is set to $\sigma^2_r = 0.05$.

In the estimation we use the following models:
%
\begin{enumerate}
\item Standard Wiener process velocity model (see, for example,
  section 2.2.9) with process noise variance $q_1 = 0.05$, whose
  purpose is to model the relatively slow turns. This model is linear,
  so we can use the standard Kalman filter for estimation.
\item A combination of Wiener process velocity model and a coordinated
  turn model described above. The variance of the process noise for the velocity model
  is set to $q_2 = 0.01$ and for the turning rate parameter in the
  turning model to $q_{\omega} = 0.15$. The estimation is now done
  with both the EKF and UKF based IMM filters as the turning model is
  non-linear. However, as the measurement model is linear, we can
  still use the standard IMM filter update step instead of a non-linear one.
  In both cases the model transition probability matrix is set to 
  % 
  \begin{equation}
    % 
    \mat{\Phi} = 
    \begin{pmatrix}
      0.9 & 0.1 \\
      0.1 & 0.9
    \end{pmatrix}, \label{eq:imm_mtpm}
    % 
  \end{equation}
  % 
  and the prior model probabilities are
  % 
  \begin{equation}
    % 
    \vec{\mu}_0 = \begin{bmatrix} 0.9 & 0.1 \end{bmatrix}. \label{eq:imm_modelprior}
    % 
  \end{equation}
  % 
%   Actually the turning model
%   itself also contains the velocity model (in case of $\omega = 0$)   
\end{enumerate}

In software code the prediction step of IMM-EKF can be done with the
function call
%
\begin{verbatim}
[x_p,P_p,c_j] = eimm_predict(x_ip,P_ip,mu_ip,p_ij,ind,...
                             dims,A,a_func,a_param,Q);
\end{verbatim}
which is almost the same as the linear IMM-filter prediction step with
the exception that we must now also pass the function handles of the
dynamic model and it's Jacobian as well as their possible parameters to the prediction step function.
In above these are the variables \texttt{A}, \texttt{a\_func} and \texttt{a\_param}
which are cell arrays containing the needed values for each model. If
some of the used models are linear (as is the case now) 
their elements of the cell arrays \texttt{a\_func} and
\texttt{a\_param} are empty, and \texttt{A} contains the dynamic model
matrices instead of Jacobians. The prediction
step function of IMM-UKF has exactly the same interface.

The EKF based IMM smoothing can be done with a function call
%
\begin{verbatim}
[SMI,SPI,SMI_i,SPI_i,MU_S] = eimm_smooth(MM,PP,MM_i,PP_i,MU,p_ij,...
                                         mu_0j,ind,dims,A,ia_func,...
                                         a_param,Q,R,H,h_func,h_param,Y);
\end{verbatim}
%
where we pass the function handles of inverse dynamic models
(variable \texttt{ia\_func}) for the backward time IMM filters as well as
Jacobians of the dynamic models (\texttt{A}) and their parameters
(\texttt{a\_param}), which all are cell arrays as in the filter
described above. Sameway we also pass the handles to the measurement model
functions (\texttt{h\_func}) and their Jacobians (\texttt{H}) as well
as their parameters (\texttt{h\_param}). UKF based IMM smoothing is
done exactly the same. 

In figure \ref{fig:eimm1_1} we have plotted the
filtered and smoothed estimates produced all the tested estimation
methods. It can be seen that the estimates produced by IMM-EKF and
IMM-UKF are very close on each other while IMM-UKF still seems to be a
little closer to the right trajectory. The estimates of standard KF
differ more from the estimates of IMM-EKF and IMM-UKF and seem to have
more problems during the later parts of turns. Smoothing improves all
the estimates, but in the case of KF the estimates are clearly
oversmoothed. 

The estimates for probability of model 2 produced by IMM-EKF and
IMM-UKF are plotted in figure \ref{fig:eimm1_2}. The filtered estimates seem to
be very close on each other whereas the smoothed estimates have little
more variation, but neither one of them don't seem to be clearly better. The
reason why both methods give about $50\%$
probability of model 2 when the actual model is 1 can be explained by
the fact that the coordinated turn model actually contains the velocity model as it's
special case when $\omega = 0$. The figure \ref{fig:eimm1_2}
shows the estimates of the turn rate parameter with both IMM filters
and smoothers, and it can be seen that smoothing improves the
estimates tremendously. It is also easy to see that in some parts IMM-UKF gives a
little better performance than IMM-EKF.

In table \ref{table:eimm1_mse} we have listed the average MSEs of position estimates
produced by the tested methods. It can be seen, that the estimates
produced by IMM-EKF and IMM-UKF using the combination of a velocity
and a coordinated turn model are clearly better than the ones
produced by a standard Kalman filter using the velocity model
alone. The performance difference between IMM-EKF and IMM-UKF is also
clearly noticable in the favor of IMM-UKF, which is also possible to
see in figure \ref{fig:eimm1_1}. The effect of smoothing to
estimation accuracy is similar in all cases. 


\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/eimm1_trajectory}
\caption{
Object's trajectory and a sample of measurements in the Coordinated turn model demonstration.
}
\label{fig:eimm1_trajectory}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/eimm1_1}
\caption{
Position estimates in the Coordinated turn model demonstration.
}
\label{fig:eimm1_1}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/eimm1_2}
\caption{
Estimates for model 2's probability in 
Coordinated turn model demonstration.   
}
\label{fig:eimm1_2}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/eimm1_3}
\caption{
Estimates of the turn rate parameter $\omega_k$ in the Coordinated
turn model demonstration.
}
\label{fig:eimm1_3}
\end{center}
\end{figure}

%  
\begin{table}
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\it Method}&{\it MSE } \\
\hline
KF  & 0.0253 \\
KS  & 0.0052 \\
EIMM1  & 0.0179 \\ 
EIMMS1 & 0.0039 \\
UIMM1  & 0.0155 \\ 
UIMMS1 & 0.0036 \\
\hline 
\end{tabular}
\caption{Average MSEs of estimating the position
in Tracking Object with Simple Manouvers example over 100 Monte Carlo
runs.}
\label{table:eimm1_mse}
\end{center}
\end{table} 
%

\subsubsection{Demonstration: Bearings Only Tracking of a Manouvering Target}

We now extend the previous demonstration by replacing the linear
measurement model with a non-linear bearings only measurement model,
which were reviewed in section 2.2.9. The estimation problem is now harder
as we must use a non-linear version of IMM filter update step in addition
to the prediction step, which was used in the previous demonstration. 

The trajectory of the object is exactly the same as in the previous
example. The sensors producing the angle measurements are located in
$(s_x^1,s_y^1) = (-0.5,3.5)$, $(s_x^2,s_y^2) = (-0.5,3.5)$,
$(s_x^3,s_y^3) = (7,-3.5)$ and $(s_x^4,s_y^4) = (7,3.5)$. The trajectory
and the sensor positions are shown in figure \ref{fig:eimm2_trajectory}.
The standard deviation of the measurements is set to $\sigma = 0.1$
radians, which is relatively high.

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/eimm2_trajectory}
\caption{
Object's trajectory and positions of the sensors in Bearings Only
Tracking of a Manouvering Target demonstration.
}
\label{fig:eimm2_trajectory}
\end{center}
\end{figure}

The function call of IMM-EKF update step is of form
%
\begin{verbatim}
[x_ip,P_ip,mu_ip,m,P] = eimm_update(x_p,P_p,c_j,ind,dims,...
                                    Y(:,i),H,h_func,R,h_param);
\end{verbatim}
%
which differs from the standard IMM filter update with the additional
parameters \texttt{h} and \texttt{h\_param}, which contain the handles
to measurement model functions and their parameters,
respectively. Also, the parameter \texttt{H} now contains the handles
to functions calculating the Jacobian's of the measurement
functions. In IMM-UKF the update function is specified similarly.

The position of the object is estimated with the following methods:
\begin{itemize}
\item EKF and EKS: Extended Kalman filter and (RTS) smoother
  using the same Wiener process velocity model as in the previous
  demonstration in the case standard Kalman filter.
\item UKF and UKS: Unscented Kalman filter and (RTS) smoother using
  the same model as the EKF.
\item IMM-EKF and IMM-EKS: EKF based IMM filter and smoother using the
  same combination of Wiener process velocity model and a coordinated
  turn model as was used in the previous demonstration in the case of
  IMM-EKF and IMM-UKF.
\item IMM-UKF and IMM-UKS: UKF based IMM filter and smoother using the
  same models as IMM-EKF.
\end{itemize}
%
A sample of trajectory estimates are plotted in figure
\ref{fig:eimm2_1}. The estimates are clearly more inaccurate than the
ones in the previous section. In figure \ref{fig:eimm2_2} we have plotted the
estimates of model 2's probability for IMM-EKF and IMM-UKF. The figure
look very similar to the one in the previous demonstration, despite
the non-linear and more noisy measurements. Also the turn rate
estimates, which are plotted in figure \ref{fig:eimm2_3}, are very
similar to the ones in the previous section with exception that now
the difference between the smoothed estimates of IMM-EKF and IMM-UKF
is bigger.

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/eimm2_1}
\caption{
Filtered and smoothed estimates of object's position using all the
tested methods in Bearings Only Tracking of a Manouvering Target demonstration.
}
\label{fig:eimm2_1}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/eimm2_2}
\caption{
Estimates for model 2's probability in 
Bearings Only Tracking of a Manouvering Target demonstration.
}
\label{fig:eimm2_2}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=11cm]{pics/eimm2_3}
\caption{
Estimates for the turn rate parameter $\omega_k$ in 
Bearings Only Tracking of a Manouvering Target demonstration
}
\label{fig:eimm2_3}
\end{center}
\end{figure}

In table \ref{table:eimm_rmse2} we have listed the average mean square errors
of position estimates over 100 Monte Carlo runs.
It can be observed that the estimates of EKF and UKF are identical in practice,
which is to be expected from Bearings Only Tracking
demonstration. The difference between IMM-UKF and IMM-EKF has grown in
the favor of IMM-UKF, whereas the accuracy of IMM-EKF is now more
close to the ones of EKF and UKF. On the other hand the smoothed
estimates of IMM-UKF and IMM-EKF are still very close to one another,
and are considerably better than the smoothed estimates of EKF and
UKF. 

%  
\begin{table}
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\it Method}&{\it MSE } \\
\hline
EKF  & 0.0606 \\
ERTS  & 0.0145 \\
UKF  & 0.0609 \\
URTS  & 0.0144 \\
IMM-EKF  & 0.0544 \\ 
IMM-EKS & 0.0094 \\
IMM-UKF  & 0.0441 \\ 
IMM-UKS & 0.0089 \\
\hline 
\end{tabular}
\caption{Average MSEs of estimating the position
in Bearings Only Tracking of a Manouvering Target example over 100 Monte Carlo
runs.}
\label{table:eimm_rmse2}
\end{center}
\end{table} 
%

It should be noted that the performance of each tested method could be
tuned by optimizing their parameters (e.g. variance of process
noise of dynamic models, values of model transition matrix in IMM etc.)
more carefully, so the performance differences could change
radically. Still, it is clear that IMM filter does actually work also
with (atleast some) non-linear dynamic and measurement models, and should be
considered as a standard estimation method for multiple model
systems. Also, one should prefer IMM-UKF over IMM-EKF as the
performance is clearly (atleast in these cases) better, and the implementation
is easier, as we have seen in the previous examples.

\clearpage

%\newpage
%\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Functions in the toolbox}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In table \ref{table:functions} we have listed all the functions in the
toolbox and the page numbers (sorted by page numbers in increasing order), where
the function description begins. Some of the functions have optional
input parameters. The default values for these parameters are listed
in end of parameter descriptions.

%
\begin{table}
\begin{center}
\begin{tabular}{|r|r|}
\hline
Function & Page\\
\hline
\hline
\texttt{lti\_disc} & \pageref{page:lti_disc}\\
\hline
\texttt{kf\_predict} & \pageref{page:kf_predict}\\
\texttt{kf\_update} & \pageref{page:kf_update}\\
\texttt{rts\_smooth} & \pageref{page:rts_smooth}\\
\texttt{tf\_smooth} & \pageref{page:tf_smooth}\\
\hline
\texttt{ekf\_predict1} & \pageref{page:ekf_predict1}\\
\texttt{ekf\_update1} & \pageref{page:ekf_update1}\\
\texttt{ekf\_predict2} & \pageref{page:ekf_predict2}\\
\texttt{ekf\_update2} & \pageref{page:ekf_update2}\\
\texttt{erts\_smooth1} & \pageref{page:erts_smooth1}\\
\texttt{etf\_smooth1} & \pageref{page:etf_smooth1}\\
\hline
\texttt{ut\_weights} & \pageref{page:ut_weights}\\
\texttt{ut\_mweights} & \pageref{page:ut_mweights}\\
\texttt{ut\_sigmas} & \pageref{page:ut_sigmas}\\
\texttt{ut\_transform} & \pageref{page:ut_transform}\\
\texttt{ukf\_predict1} & \pageref{page:ukf_predict1}\\
\texttt{ukf\_update1} & \pageref{page:ukf_update1}\\
\texttt{ukf\_predict2} & \pageref{page:ukf_predict2}\\
\texttt{ukf\_update2} & \pageref{page:ukf_update1}\\
\texttt{ukf\_predict3} & \pageref{page:ukf_predict3}\\
\texttt{ukf\_update3} & \pageref{page:ukf_update1}\\
\texttt{urts\_smooth1} & \pageref{page:urts_smooth1}\\
\texttt{urts\_smooth2} & \pageref{page:urts_smooth2}\\
\texttt{utf\_smooth1} & \pageref{page:utf_smooth1}\\
\hline
\texttt{imm\_predict} & \pageref{page:imm_predict}\\
\texttt{imm\_update} & \pageref{page:imm_update}\\
\texttt{imm\_smooth} & \pageref{page:imm_smooth}\\
\texttt{eimm\_predict} & \pageref{page:eimm_predict}\\
\texttt{eimm\_update} & \pageref{page:eimm_update}\\
\texttt{eimm\_smooth} & \pageref{page:eimm_smooth}\\
\texttt{uimm\_predict} & \pageref{page:uimm_predict}\\
\texttt{uimm\_update} & \pageref{page:uimm_update}\\
\texttt{uimm\_smooth} & \pageref{page:uimm_smooth}\\
\hline
\texttt{der\_check} & \pageref{page:der_check}\\
\texttt{gauss\_pdf} & \pageref{page:gauss_pdf}\\
\texttt{gauss\_rnd} & \pageref{page:gauss_rnd}\\
\texttt{rk4} & \pageref{page:rk4}\\
\texttt{schol} & \pageref{page:schol}\\
\hline
\end{tabular}
\caption{Functions provided with the toolbox. Functions are sorted
  according to page numbers, from which function descriptions begin.}
\label{table:functions}
\end{center}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Discrete-time state space estimation}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\label{page:lti_disc}

%{\bf The discretization of a continous-time dynamic system} using the matrix fractional
%decomposition (equation (\ref{eq:noisy_ltisolution_cov1})) can be calculated with the function
%\texttt{lti\_disc}: \\

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{lti\_disc}} \\

%{\bf Description:}
%\begin{tabular}{l}
Discretizes a linear, continous-time dynamic system using the matrix \\
fraction decomposition (eq. (\ref{eq:noisy_ltisolution_cov1})). \\
%\end{tabular} \\

\hline

{\bf Syntax:}
%
\texttt{[A,Q] = lti\_disc(F,L,Qc,dt)} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{F} & $N\times N$ feedback matrix \\
\texttt{L} & $N\times L$ noise effect matrix & default: identity \\
\texttt{Qc} & $L \times L$ diagonal diffusion matrix & default: zeros \\
\texttt{dt} & time step                              & default: 1 \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{A} & The transition matrix\\
\texttt{Q} & The covariance of the discrete process\\
\end{tabular}\\

\hline
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Linear models}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Kalman filter}

%{\bf The prediction step} of the Kalman filter (eq. (\ref{eq:dkf_predict}))
%can be calculated using the function \texttt{kf\_predict}:\\



\label{page:kf_predict}


\begin{tabular}{|l|}
\hline

{\bf \texttt{kf\_predict}} \\

Calculates the {\bf prediction step} of the Kalman filter (eq.
(\ref{eq:dkf_predict})). \\

\hline

{\bf Syntax:}
%
\texttt{[X,P] = kf\_predict(X,P,A,Q,B,U)} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{X} & $N\times 1$ vector containing the state mean estimate of the previous step\\
\texttt{P} & $N\times N$ matrix containing the state covariance of the previous step\\
\texttt{A} & Transition matrix of the discrete model & default: identity\\
\texttt{Q} & Process noise of the discrete model & default: zero\\
\texttt{B} & Input effect matrix & default: identity\\
\texttt{U} & Constant input & default: empty\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X} & Predicted mean for the state on the next time step\\
\texttt{P} & Predicted covariance for the state on the next time step\\
\end{tabular}\\

\hline
\end{tabular}

%\noindent {\bf The update step} of the Kalman filter (eq. \ref{eq:dkf_update}) can
%be calculated using the function \texttt{kf\_update}: \\

\label{page:kf_update}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{kf\_update}} \\

Calculates the {\bf update step} of the Kalman filter (eq.
\ref{eq:dkf_update}). \\

\hline

{\bf Syntax:}
\texttt{[X,P,K,IM,IS,LH] = kf\_update(X,P,Y,H,R)} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l}
\texttt{X} & $N\times 1$ vector containing the predicted state mean estimate from the prediction step\\
\texttt{P} & $N\times N$ matrix containing the predicted state covariance of the prediction step\\
\texttt{Y} & $D\times 1$ vector containing the measurement on this time step\\
\texttt{H} & $N\times D$ measurement matrix on this time step\\
\texttt{R} & $S\times D$ measurement noise covariance matrix on this time step\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X}  & $N\times 1$ vector containing the updated state mean\\
\texttt{P}  & $N\times N$ matrix containing the updated state covariance\\
\texttt{K}  & Computed gain for the Kalman filter\\
\texttt{IM} & Mean of predictive distribution of \texttt{Y}\\
\texttt{IS} & Covariance or predictive mean of \texttt{Y}\\
\texttt{LH} & Predictive probability of the measurement\\
\end{tabular}\\

\hline


\end{tabular}
\subsubsection*{Kalman smoother}


%{\bf The smoothed estimate} for the state mean and covariance using {\bf the RTS smoother}
%(eq. (\ref{eq:kfrts})) can be calculated with function \texttt{rts\_smooth}:\\

\label{page:rts_smooth}

\begin{tabular}{|l|}
\hline

{\bf \texttt{rts\_smooth}} \\

Computes {\bf The smoothed estimate} for the state mean and covariance using {\bf the RTS smoother}
(eq. (\ref{eq:kfrts})).\\

\hline
{\bf Syntax:} 
\texttt{[M,P,S] = rts\_smooth(M,P,A,Q)} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l}
\texttt{M} & $N\times K$ matrix containing $K$ mean estimates calculated with Kalman filter\\
\texttt{P} & $N\times N\times K$ matrix of $K$ state covariances calculated with Kalman filter \\
\texttt{A} & $N\times N$ state transition matrix or $N\times N\times K$ matrix of $K$ state covariances
\\ & for each time step\\
\texttt{Q} & $N\times N$ process noise covariance matrix or $N\times N\times K$ matrix of $K$ process\\ & noise covariance matrices for each time step
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Smoothed state mean on each time step\\
\texttt{P} & Smoothed state covariance sequence on each time step\\
\texttt{S} & Smoother gain on each time step
\end{tabular}\\

\hline
\end{tabular}

%{\bf The smoothed estimate} for the state mean and covariance using {\bf the TF smoother}
%can be calculated with function \texttt{tf\_smooth}:\\

\label{page:tf_smooth}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{tf\_smooth}} \\

Calculates the {\bf The smoothed estimate} for the state mean and covariance using {\bf the TF smoother}.\\

\hline

{\bf Syntax:} 
\texttt{[M,P] = tf\_smooth(M,P,Y,A,Q,H,R,use\_inf)} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l}
\texttt{M} & $N\times K$ matrix containing $K$ mean estimates calculated with Kalman filter\\
\texttt{P} & $N\times N\times K$ matrix of $K$ state covariances calculated with Kalman filter \\
\texttt{Y} & $D\times K$ matrix containing the sequence of $K$ measurements \\
\texttt{A} & $N\times N$ state transition matrix or $N\times N\times K$ matrix of $K$ state covariances
\\ & for each time step\\
\texttt{Q} & $N\times N$ process noise covariance matrix or $N\times
N\times K$ matrix of $K$ process\\ & noise covariance matrices for
each time step \\
\texttt{H} & $D\times N$ measurement matrix \\
\texttt{R} & $D\times D$ measurement noise covariance \\
\texttt{use\_inf} & if information filter should be used
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Smoothed state mean on each time step\\
\texttt{P} & Smoothed state covariance sequence on each time step\\
\end{tabular}\\

\hline
\end{tabular}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Nonlinear models}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Extended Kalman filter}

%{\bf The prediction step} of the {\bf first order extended Kalman filter}
%(eq. (\ref{eq:dekf_predict1})) can be calculated using the function
%\texttt{ekf\_predict1}:\\

\label{page:ekf_predict1}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ekf\_predict1}} \\

Calculates the {\bf The prediction step} of the {\bf first order extended Kalman filter}
(eq. (\ref{eq:dekf_predict1})).\\

\hline

{\bf Syntax:}
%
\texttt{[M,P] = ekf\_predict1(M,P,[A,Q,a,W,param])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times 1$ vector containing the state mean estimate of the previous step\\
\texttt{P} & $N\times N$ matrix containing the state covariance of the previous step\\
\texttt{A} & Derivative of \texttt{a} with respect to state as matrix, inline function, & \\ & function handle or name of the function in form \texttt{A(x,param)} & default: identity\\
\texttt{Q} & Process noise of the discrete model & default: zero\\
\texttt{a} & Dynamic model function as vector, inline function, & \\ & function handle or name of the function in form \texttt{a(x,param)} & default: \texttt{A(x)*M}\\
\texttt{W} & Derivative of \texttt{a} with respect to noise as matrix, inline function, & \\ & function handle or name of the function in form \texttt{W(x,param)} & default: identity\\
\texttt{param} & Parameters of function a & default: empty\\
%\texttt{B} & Input effect matrix & default: identity,\\
%\texttt{U} & Constant input & default: empty.\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Predicted mean for the state on the next time step\\
\texttt{P} & Predicted covariance for the state on the next time step\\
\end{tabular}\\

\hline
\end{tabular}



%\noindent {\bf The update step} of the {\bf first order extended Kalman filter}
%(eq. (\ref{eq:dekf_update1})) can be calculated using the function
%\texttt{ekf\_update1}:\\

\label{page:ekf_update1}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ekf\_update1}} \\

Calculates the {\bf The update step} of the {\bf first order extended Kalman filter}
(eq. (\ref{eq:dekf_update1})).\\      

\hline

{\bf Syntax:}
%
\texttt{[M,P,K,IM,S,LH] = ekf\_update1(M,P,Y,H,R,h,V,[param])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times 1$ vector containing the state mean estimate &
\\ & after the prediction step\\
\texttt{P} & $N\times N$ matrix containing the state covariance after
& \\ & the prediction step\\
\texttt{Y} & $D\times 1$ measurement vector \\
\texttt{H} & Derivative of \texttt{h} with respect to state as matrix, inline function, & \\ & function handle or name of the function in form \texttt{H(x,param)}\\
\texttt{R} & Measurement noise covariance\\
\texttt{h} & Measurement model function as vector, inline function, & \\ & function handle or name of the function in form \texttt{h(x,param)} & default: \texttt{H(x)*M}\\
\texttt{V} & Derivative of \texttt{h} with respect to noise as matrix, inline function, & \\ & function handle or name of the function in form \texttt{V(x,param)} & default: identity\\
\texttt{param} & Parameters of function h & default: empty\\
%\texttt{B} & Input effect matrix & default: identity,\\
%\texttt{U} & Constant input & default: empty.\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Updated mean for the state on the next time step\\
\texttt{P} & Updated covariance for the state on the next time step\\
\texttt{K} & Computed Kalman gain\\
\texttt{MU} & Predictive mean of Y\\
\texttt{S} & Predictive covariance of Y\\
\texttt{LH} & Predictive probability (likelihood) of measurement\\
\end{tabular}\\

\hline
\end{tabular}

\label{page:ekf_predict2}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ekf\_predict2}} \\

Calculates the {\bf The prediction step} of the {\bf second order extended Kalman filter}
(eq. (\ref{eq:dekf_predict2})).\\

\hline

{\bf Syntax:}
%
\texttt{[M,P] = ekf\_predict2(M,P,[A,F,Q,a,W,param])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times 1$ vector containing the state mean estimate of the previous step\\
\texttt{P} & $N\times N$ matrix containing the state covariance of the previous step\\
\texttt{A} & Derivative of \texttt{a} with respect to state as matrix, inline function, & \\ & function handle or name of the function in form \texttt{A(x,param)} & default: identity\\
\texttt{F} & $N\times N\times N$ Hessian matrix of the state
transition w.r.t. state variables & \\ & as matrix, 
function handle of name of function in form \texttt{F(x,param)} & default:
identity \\
\texttt{Q} & Process noise of the discrete model & default: zero\\
\texttt{a} & Dynamic model function as vector, inline function, & \\ & function handle or name of the function in form \texttt{a(x,param)} & default: $A(x)*M$\\
\texttt{W} & Derivative of \texttt{a} with respect to noise as matrix, inline function, & \\ & function handle or name of the function in form \texttt{W(x,param)} & default: identity\\
\texttt{param} & Parameters of function a & default: empty\\
%\texttt{B} & Input effect matrix & default: identity,\\
%\texttt{U} & Constant input & default: empty.\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Predicted mean for the state on the next time step\\
\texttt{P} & Predicted covariance for the state on the next time step\\
\end{tabular}\\

\hline
\end{tabular}

\label{page:ekf_update2}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ekf\_update2}} \\

Calculates the {\bf The update step} of the {\bf second order extended Kalman filter}
(eq. (\ref{eq:dekf_update2})).\\      

\hline

{\bf Syntax:}
%
\texttt{[M,P,K,IM,S,LH] = ekf\_update2(M,P,Y,H,H\_xx,R,h,V,[param])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times 1$ vector containing the state mean estimate &
\\ & after the prediction step\\
\texttt{P} & $N\times N$ matrix containing the state covariance after
& \\ & the prediction step\\
\texttt{Y} & $D\times 1$ measurement vector \\
\texttt{H} & Derivative of \texttt{h} with respect to state as matrix, inline function, & \\ & function handle or name of the function in form \texttt{H(x,param)}\\
\texttt{H\_xx} & $D\times N \times N$ Hessian of \texttt{h} w.r.t. state
variables as matrix, inline function, & \\ & function handle or name
of function in form \texttt{H\_xx(x,param)} \\
\texttt{R} & Measurement noise covariance\\
\texttt{h} & Measurement model function as vector, inline function, & \\ & function handle or name of the function in form \texttt{h(x,param)} & default: \texttt{H(x)*M}\\
\texttt{V} & Derivative of \texttt{h} with respect to noise as matrix, inline function, & \\ & function handle or name of the function in form \texttt{V(x,param)} & default: identity\\
\texttt{param} & Parameters of function h & default: empty\\
%\texttt{B} & Input effect matrix & default: identity,\\
%\texttt{U} & Constant input & default: empty.\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Updated mean for the state on the next time step\\
\texttt{P} & Updated covariance for the state on the next time step\\
\texttt{K} & Computed Kalman gain\\
\texttt{MU} & Predictive mean of Y\\
\texttt{S} & Predictive covariance of Y\\
\texttt{LH} & Predictive probability (likelihood) of measurement\\
\end{tabular}\\

\hline
\end{tabular}



\subsubsection*{Extended Kalman smoothers}

%{\bf The smoothed estimate} for the state mean and covariance using
%{\bf the extended RTS smoother} (eq. (\ref{eq:derts})) can be calculated using the function
%\texttt{erts\_smooth1}:\\

In extended and unscented smoothers the parameters of dynamic and
measurement function are passed as a single cell array, vector or a matrix
containing the same parameters for each step, or if different parameters
are used on each step they must be a cell array of the format \\
\texttt{\{param\_1,param\_2,...\}}, where \texttt{param\_x} contains the parameters for
step x as a cell array, vector or matrix. \\

\label{page:erts_smooth1}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{erts\_smooth1}} \\

Calculates {\bf The smoothed estimate} for the state mean and covariance using
{\bf the extended RTS smoother} (eq. (\ref{eq:derts})).\\

\hline

{\bf Syntax:}
%
\texttt{[M,P,D] = erts\_smooth1(M,P,[A,Q,s,W,param,same\_p])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times K$ matrix containing the state mean estimates
produced & \\ &  by the EKF\\
\texttt{P} & $N\times N \times K$ matrix containing the state
covariances produced & \\ & by the EKF\\
\texttt{A} & Derivative of \texttt{a} with respect to state as matrix, inline
function, & \\ & function handle or name of the function in form
\texttt{A(x,param)} & default: zero\\
\texttt{Q} & Process noise of discrete model & default: zero\\ 
\texttt{a} & Dynamic model function as vector, inline function, & \\ & function handle or name of the function in form \texttt{a(x,param)} & default: \texttt{H(x)*M}\\
\texttt{W} & Derivative of \texttt{a} with respect to noise as matrix, inline function, & \\ & function handle or name of the function in form \texttt{V(x,param)} & default: identity\\
\texttt{param} & Parameters of function \texttt{h} & default: empty\\
\texttt{same\_p} & If 1 uses the same parameters on every time step & default: 1 \\
%\texttt{B} & Input effect matrix & default: identity,\\
%\texttt{U} & Constant input & default: empty.\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Smoothed state mean sequence for all time steps\\
\texttt{P} & Smoothed state covariance for all time steps\\
\texttt{D} & Smoother gain for all time steps\\
\end{tabular}\\

\hline
\end{tabular}

\label{page:etf_smooth1}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{etf\_smooth1}} \\

Calculates {\bf The smoothed estimate} for the state mean and covariance using
{\bf the extended TF smoother}.\\

\hline

{\bf Syntax:}
%
\texttt{[M,P] = etf\_smooth1(M,P,Y,A,Q,ia,W,aparam,H,R,h,V,hparam,s\_p\_a,s\_p\_h)}\\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times K$ matrix containing the state mean estimates
produced & \\ &  by the EKF\\
\texttt{P} & $N\times N \times K$ matrix containing the state
covariances produced & \\ & by the EKF\\
\texttt{Y} & Measurement vector \\
\texttt{A} & Derivative of \texttt{ia} with respect to state as matrix, inline
function, & \\ & function handle or name of the function in form
\texttt{A(x,param)} & default: identity\\
\texttt{Q} & Process noise of discrete model & default: zero\\
\texttt{ia} & Inverse prediction function as vector, inline function,
function handle & \\ & or name of function in form
\texttt{ia(x,param)} & default: \texttt{inv(A(x))*M} \\
\texttt{W} & Derivative of \texttt{ia} with respect to noise as matrix, inline function, & \\ & function handle or name of the function in form \texttt{V(x,param)} & default: identity\\
\texttt{aparam} & Parameters of function \texttt{ia} & default: empty\\
\texttt{H} & Derivative of \texttt{h} with respect to state as matrix,
        inline function, & \\ & function handle or name
        of function in form \texttt{H(x,param)} \\
\texttt{R} & Measurement noise covariance \\
\texttt{h} & Measurement model function as vector, inline
function, & \\ & function handle or name of the function in form
\texttt{h(x,param)} & default: \texttt{H(x)*M}\\
\texttt{V} & Derivative of \texttt{h} w.r.t. noise as matrix, inline
function & \\ & function handle or name of function in form
\texttt{V(x,param)} & default: identity \\
\texttt{hparam} & Parameters of h & default: \texttt{aparam}\\
\texttt{s\_p\_a} & If 1 uses the same parameters on every time step
for \texttt{a} & default: 1 \\
\texttt{s\_p\_h} & If 1 uses the same parameters on every time step
for \texttt{h} & default: 1\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Smoothed state mean sequence for all time steps\\
\texttt{P} & Smoothed state covariance for all time steps\\
\end{tabular}\\

\hline
\end{tabular}

\subsubsection*{Unscented Kalman Filter}



%The {\bf weights} for the {\bf unscented transform} using the
%{\bf summation} form (eq. (\ref{eq:ut_weights})) can be calculated using the function
%\texttt{ut\_weights}:\\

\label{page:ut_weights}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ut\_weights}} \\

Calculates the {\bf weights} for the {\bf unscented transform} using the
{\bf summation} form (eq. (\ref{eq:ut_weights})). \\

\hline
{\bf Syntax:}
%
\texttt{[WM,WC,c] = ut\_weights(n,alpha,beta,kappa)} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{n} & Dimensionality of random variable,  \\
\texttt{alpha} & Transformation parameter $\alpha$ & default: 0.5 \\
\texttt{beta}  & Transformation parameter $\beta$ & default: 2 \\
\texttt{kappa} & Transformation parameter $\kappa$ & default: 3-N\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{WM} & $(2n+1) \times 1$ vector containing the weights for mean estimation \\
\texttt{WC} & $(2n+1) \times 1$ vector containing the weights for covariance estimation \\
\texttt{c} & Scaling constant\\
\end{tabular}\\

\hline
\end{tabular} \\

%\noindent The {\bf weights} for the {\bf unscented transform} using the
%{\bf matrix} form (eq. (\ref{eq:vec_wm}) and (\ref{eq:mat_w})) can be calculated using the function
%\texttt{ut\_mweights}:\\

\label{page:ut_mweights}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ut\_mweights}} \\

Calculates the {\bf weights} for the {\bf unscented transform} using
the
{\bf matrix} form (eq. (\ref{eq:vec_wm}) and (\ref{eq:mat_w})). \\

\hline
{\bf Syntax:}
%
\texttt{[WM,W,c] = ut\_mweights(n,alpha,beta,kappa)} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{n} & Dimensionality of the state variable  \\
\texttt{alpha} & Transformation parameter $\alpha$ & default: 0.5 \\
\texttt{beta}  & Transformation parameter $\beta$ & default: 2 \\
\texttt{kappa} & Transformation parameter $\kappa$ & default: 3-N\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{WM} & $(2n+1) \times 1$ vector containing the weights for mean estimation \\
\texttt{W} & $(2n+1) \times (2n+1)$ matrix containing the weights for covariance estimation\\
\texttt{c} & Scaling constant\\
\end{tabular}\\

\hline
\end{tabular} \\

%\noindent The {\bf sigma points} for the {\bf unscented transform} 
%(eq. (\ref{eq:ut_sigmas})) can be generated using the function
%\texttt{ut\_sigmas}:\\

\label{page:ut_sigmas}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ut\_sigmas}} \\

Generates The {\bf sigma points} for the {\bf unscented transform} 
with eq. (\ref{eq:ut_sigmas}). \\

\hline
{\bf Syntax:}
%
\texttt{[X] = ut\_sigmas(M,P,c)} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l}
\texttt{M} & $N \times 1$ vector containing the initial state mean \\
\texttt{P} & $N \times N$ matrix containing the initial state covariance \\
\texttt{c} & Parameter returned by \texttt{ut\_weights} \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X} & $(2n+1) \times (2n+1)$ matrix containing the sigma points in its columns \\
\end{tabular}\\

\hline
\end{tabular}\\


%\noindent The {\bf unscented transform} using the summation or matrix form
% can be calculated using the function
%\texttt{ut\_transform}:\\

\label{page:ut_transform}


\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ut\_transform}} \\

Calculates the {\bf unscented transform} of function $\vec{g}$ using
the summation or matrix form.\\

\hline

{\bf Syntax:}
%
\texttt{[mu,S,C,X,Y,w] = ut\_transform(g,M,P,[param,alpha,beta,kappa,mat])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{g} & Transformation function of the form \texttt{g(x,param)} as & \\ &
matrix, inline function, function name or function reference \\
\texttt{M} & $N \times 1$ containing mean of $\vec{x}$ \\
\texttt{P} & $N \times N$ matrix containing the covariance of $\vec{x}$ \\
\texttt{param} & Parameters of function \texttt{g} & default: empty\\
\texttt{alpha} & Transformation parameter $\alpha$ & default: 0.5 \\
\texttt{beta} & Transformation parameter $\beta$ & default: 2 \\
\texttt{kappa} & Transformation parameter $\kappa$ & default: 3-N\\
\texttt{mat} & 1 if the matrix form is used, otherwise 0 & default: 0\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{mu} & Estimated mean of y\\
\texttt{S} & Estimated covariance of y\\
\texttt{C} & Estimated cross-covariance of x and y\\
\texttt{X} & Sigma points of x\\
\texttt{Y} & Sigma points of y\\
\texttt{w} & Weights of sigma points as cell-array of form \texttt{\{mean-weights,cov-weights,c\}}\\
\end{tabular}\\

\hline
\end{tabular} \\

\label{page:ukf_predict1}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ukf\_predict1}} \\

Calculates the {\bf The prediction step} of the {\bf non-augmented
form unscented Kalman filter} (eq. (\ref{eq:ukf1_predict})).\\

\hline

{\bf Syntax:}
%
\texttt{[M,P] = ukf\_predict1(M,P,a,Q,[param,alpha,beta,kappa])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times 1$ vector containing the state mean estimate of the previous step\\
\texttt{P} & $N\times N$ matrix containing the state covariance of the previous step\\
\texttt{a} & Dynamic model function as vector, inline function, & \\ &
function handle or name of the function in form \texttt{a(x,param)} \\
\texttt{Q} & Process noise of the discrete model & default: zero\\
\texttt{param} & Parameters of function a & default: empty\\
\texttt{alpha} & Transformation parameter $\alpha$ & default: $0.5$\\
\texttt{beta} & Transformation parameter $\beta$ & default: $2$\\
\texttt{kappa} & Transformation parameter $\kappa$ & default: $3-N$\\
\texttt{mat} & If 1 uses matrix form & default: 0\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Predicted mean for the state on the next time step\\
\texttt{P} & Predicted covariance for the state on the next time step\\
\end{tabular}\\

\hline
\end{tabular} \\

\label{page:ukf_update1}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ukf\_update1}} \\

Calculates the {\bf The update step} of the {\bf nonaugmented form unscented Kalman filter}
(eq. (\ref{eq:ukf1_update1}) and (\ref{eq:ukf1_update2})).\\      

\hline

{\bf Syntax:}
%
\texttt{[M,P,K,IM,S,LH] = ukf\_update1(M,P,Y,h,R,[param,alpha,beta,kappa,mat])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times 1$ vector containing the state mean estimate &
\\ & after the prediction step\\
\texttt{P} & $N\times N$ matrix containing the state covariance after
& \\ & the prediction step\\
\texttt{Y} & $D\times 1$ measurement vector \\
\texttt{h} & Measurement model function as vector, inline function, &
\\ & function handle or name of the function in form
\texttt{h(x,param)} \\
\texttt{R} & Measurement noise covariance\\
\texttt{param} & Parameters of function h & default: empty\\
\texttt{alpha} & Transformation parameter $\alpha$ & default: $0.5$\\
\texttt{beta} & Transformation parameter $\beta$ & default: $2$\\
\texttt{kappa} & Transformation parameter $\kappa$ & default: $3-N$\\
\texttt{mat} & If 1 uses matrix form & default: 0\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Updated mean for the state on the next time step\\
\texttt{P} & Updated covariance for the state on the next time step\\
\texttt{K} & Computed Kalman gain\\
\texttt{MU} & Predictive mean of Y\\
\texttt{S} & Predictive covariance of Y\\
\texttt{LH} & Predictive probability (likelihood) of measurement\\
\end{tabular}\\

\hline
\end{tabular}

\label{page:ukf_predict2}
\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ukf\_predict2}} \\

Calculates the {\bf The prediction step} of the {\bf augmented form
  unscented Kalman filter}
which concatenates \\ state variables and
process noise terms together. \\

\hline

{\bf Syntax:}
%
\texttt{[M,P] = ukf\_predict2(M,P,a,Q,[param,alpha,beta,kappa])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times 1$ vector containing the state mean estimate of the previous step\\
\texttt{P} & $N\times N$ matrix containing the state covariance of the previous step\\
\texttt{a} & Dynamic model function as vector, inline function, & \\ &
function handle or name of the function in form \texttt{a([x;q],param)} \\
\texttt{Q} & Process noise of the discrete model & default: zero\\
\texttt{param} & Parameters of function a & default: empty\\
\texttt{alpha} & Transformation parameter $\alpha$ & default: $0.5$\\
\texttt{beta} & Transformation parameter $\beta$ & default: $2$\\
\texttt{kappa} & Transformation parameter $\kappa$ & default: $3-N$\\
\texttt{mat} & If 1 uses matrix form & default: 0\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Predicted mean for the state on the next time step\\
\texttt{P} & Predicted covariance for the state on the next time step\\
\end{tabular}\\

\hline
\end{tabular} \\


\label{page:ukf_update2}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ukf\_update2}} \\

Calculates the {\bf The update step} of the {\bf augmented form
  unscented Kalman filter}, which concatenates state \\variables and
measurement noise terms together. Note that this function creates a new set of sigma points, so \\some odd-order moment
information carried by process noise terms might get lost. \\ 

\hline

{\bf Syntax:}
%
\texttt{[M,P,K,IM,S,LH] = ukf\_update2(M,P,Y,h,R,[param,alpha,beta,kappa,mat])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times 1$ vector containing the state mean estimate &
\\ & after the prediction step\\
\texttt{P} & $N\times N$ matrix containing the state covariance after
& \\ & the prediction step\\
\texttt{Y} & $D\times 1$ measurement vector \\
\texttt{h} & Measurement model function as vector, inline function, &
\\ & function handle or name of the function in form \texttt{h([x;r],param)} \\
\texttt{R} & Measurement noise covariance\\
\texttt{param} & Parameters of function h & default: empty\\
\texttt{alpha} & Transformation parameter $\alpha$ & default: $0.5$\\
\texttt{beta} & Transformation parameter $\beta$ & default: $2$\\
\texttt{kappa} & Transformation parameter $\kappa$ & default: $3-N$\\
\texttt{mat} & If 1 uses matrix form & default: 0\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Updated mean for the state on the next time step\\
\texttt{P} & Updated covariance for the state on the next time step\\
\texttt{K} & Computed Kalman gain\\
\texttt{MU} & Predictive mean of Y\\
\texttt{S} & Predictive covariance of Y\\
\texttt{LH} & Predictive probability (likelihood) of measurement\\
\end{tabular}\\

\hline
\end{tabular}

\label{page:ukf_predict3}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ukf\_predict3}} \\

Calculates the {\bf The prediction step} of the {\bf augmented form
  unscented Kalman filter} (eq.
(\ref{eq:ukf2_predict1}),(\ref{eq:ukf2_predict2}) and (\ref{eq:ukf2_predict3})),\\
which concatenates state variables, process and
measurement noise terms together. \\

\hline

{\bf Syntax:}
%
\texttt{[M,P] = ukf\_predict3(M,P,a,Q,[param,alpha,beta,kappa])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times 1$ vector containing the state mean estimate of the previous step\\
\texttt{P} & $N\times N$ matrix containing the state covariance of the previous step\\
\texttt{a} & Dynamic model function as vector, inline function, & \\ &
function handle or name of the function in form \texttt{a([x;q],param)} \\
\texttt{Q} & Process noise of the discrete model & default: zero\\
\texttt{param} & Parameters of function a & default: empty\\
\texttt{alpha} & Transformation parameter $\alpha$ & default: $0.5$\\
\texttt{beta} & Transformation parameter $\beta$ & default: $2$\\
\texttt{kappa} & Transformation parameter $\kappa$ & default: $3-N$\\
\texttt{mat} & If 1 uses matrix form & default: 0\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Predicted mean for the state on the next time step\\
\texttt{P} & Predicted covariance for the state on the next time step\\
\texttt{X} & Sigma points of x\\
\texttt{w} & Weights as cell array of form \texttt{\{mean-weights,cov-weights,c\}}\\
\end{tabular}\\

\hline
\end{tabular} \\


\label{page:ukf_update3}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{ukf\_update3}} \\

Calculates the {\bf The update step} of the {\bf augmented form
  unscented Kalman filter} (eq. (\ref{eq:ukf2_update1}) and
(\ref{eq:ukf2_update2})), \\ which concatenates state variables and
measurement noise terms together. Note that this
function  uses the \\ sigma points created in the prediction step, so that
odd-order moment information could get captured better.\\  

\hline

{\bf Syntax:}
%
\texttt{[M,P,K,IM,S,LH] = ukf\_update3(M,P,Y,h,R,X,w,[param,alpha,beta,kappa,mat])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times 1$ vector containing the state mean estimate &
\\ & after the prediction step\\
\texttt{P} & $N\times N$ matrix containing the state covariance after
& \\ & the prediction step\\
\texttt{Y} & $D\times 1$ measurement vector \\
\texttt{h} & Measurement model function as vector, inline function, &
\\ & function handle or name of the function in form \texttt{h([x;r],param)} \\
\texttt{R} & Measurement noise covariance\\
\texttt{X} & Sigma points of x\\
\texttt{w} & Weights as cell array of form \texttt{\{mean-weights,cov-weights,c\}}\\
\texttt{param} & Parameters of function h & default: empty\\
\texttt{alpha} & Transformation parameter $\alpha$ & default: $0.5$\\
\texttt{beta} & Transformation parameter $\beta$ & default: $2$\\
\texttt{kappa} & Transformation parameter $\kappa$ & default: $3-N$\\
\texttt{mat} & If 1 uses matrix form & default: 0\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Updated mean for the state on the next time step\\
\texttt{P} & Updated covariance for the state on the next time step\\
\texttt{K} & Computed Kalman gain\\
\texttt{MU} & Predictive mean of Y\\
\texttt{S} & Predictive covariance of Y\\
\texttt{LH} & Predictive probability (likelihood) of measurement\\
\end{tabular}\\

\hline
\end{tabular}


\label{page:urts_smooth1}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{urts\_smooth1}} \\

Calculates {\bf The smoothed estimate} for the state mean and covariance using\\
{\bf the nonaugmented form unscented RTS smoother}. Note that this
assumes \\ that the noises are additive.\\

\hline

{\bf Syntax:}
%
\texttt{[M,P,D] = urts\_smooth1(M,P,a,Q,[param,alpha,beta,kappa,mat,same\_p])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times K$ matrix containing the state mean estimates
produced & \\ &  by the EKF\\
\texttt{P} & $N\times N \times K$ matrix containing the state
covariances produced & \\ & by the EKF\\
\texttt{a} & Dynamic model function as vector, inline function, & \\ & function handle or name of the function in form \texttt{a(x,param)} \\
\texttt{Q} & Process noise of discrete model & default: zero\\ 
\texttt{param} & Parameters of function \texttt{a} & default: empty\\
\texttt{alpha} & Transformation parameter $\alpha$ & default: $0.5$\\
\texttt{beta} & Transformation parameter $\beta$ & default: $2$\\
\texttt{kappa} & Transformation parameter $\kappa$ & default: $3-N$\\
\texttt{mat} & If 1 uses matrix form & default: 0\\
\texttt{same\_p} & If 1 uses the same parameters on every time step & default: 1 \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Smoothed state mean sequence for all time steps\\
\texttt{P} & Smoothed state covariance for all time steps\\
\texttt{D} & Smoother gain for all time steps\\
\end{tabular}\\

\hline
\end{tabular}


\label{page:urts_smooth2}
\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{urts\_smooth2}} \\

Calculates {\bf The smoothed estimate} for the state mean and
covariance using \\
{\bf the augmented form unscented RTS smoother} (eq. (\ref{eq:urts1})
- (\ref{eq:urts5})).\\ 
Note that in this case noises can be non-additive.\\

\hline

{\bf Syntax:}
%
\texttt{[M,P,D] = urts\_smooth2(M,P,a,Q,[param,alpha,beta,kappa,mat,same\_p])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times K$ matrix containing the state mean estimates
produced & \\ &  by the EKF\\
\texttt{P} & $N\times N \times K$ matrix containing the state
covariances produced & \\ & by the EKF\\
\texttt{a} & Dynamic model function as vector, inline function, & \\ & function handle or name of the function in form \texttt{a([x;w],param)} \\
\texttt{Q} & Process noise of discrete model & default: zero\\ 
\texttt{param} & Parameters of function \texttt{a} & default: empty\\
\texttt{alpha} & Transformation parameter $\alpha$ & default: $0.5$\\
\texttt{beta} & Transformation parameter $\beta$ & default: $2$\\
\texttt{kappa} & Transformation parameter $\kappa$ & default: $3-N$\\
\texttt{mat} & If 1 uses matrix form & default: 0\\
\texttt{same\_p} & If 1 uses the same parameters on every time step & default: 1 \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Smoothed state mean sequence for all time steps\\
\texttt{P} & Smoothed state covariance for all time steps\\
\texttt{D} & Smoother gain for all time steps\\
\end{tabular}\\

\hline
\end{tabular}

\label{page:utf_smooth1}


\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{utf\_smooth1}} \\

Calculates {\bf The smoothed estimate} for the state mean and covariance using
{\bf the unscented TF smoother}.\\

\hline

{\bf Syntax:}
%
\texttt{[M,P] = utf\_smooth1(M,P,Y,[ia,Q,ap,h,R,hp,aa,bb,kk,mat,s\_p\_a,s\_p\_h])}\\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $N\times K$ matrix containing the state mean estimates
produced & \\ &  by the EKF\\
\texttt{P} & $N\times N \times K$ matrix containing the state
covariances produced & \\ & by the EKF\\
\texttt{Y} & Measurement vector \\
\texttt{ia} & Inverse prediction function as vector, inline function,
function handle & \\ & or name of function in form
\texttt{ia(x,param)} & default: identity \\
\texttt{Q} & Process noise of discrete model & default: zero\\
\texttt{ap} & Parameters of function \texttt{ia} & default: empty\\
\texttt{h} & Measurement model function as vector, inline
function, & \\ & function handle or name of the function in form
\texttt{h(x,param)} \\
\texttt{R} & Measurement noise covariance \\
\texttt{hp} & Parameters of h & default: \texttt{aparam}\\
\texttt{aa} & Transformation parameter $\alpha$ & default: $0.5$\\
\texttt{bb} & Transformation parameter $\beta$ & default: $2$\\
\texttt{kk} & Transformation parameter $\kappa$ & default: $3-N$\\
\texttt{mat} & If 1 uses matrix form & default: 0\\
\texttt{s\_p\_a} & If 1 uses the same parameters on every time step
for \texttt{a} & default: 1 \\
\texttt{s\_p\_h} & If 1 uses the same parameters on every time step
for \texttt{h} & default: 1\\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{M} & Smoothed state mean sequence for all time steps\\
\texttt{P} & Smoothed state covariance for all time steps\\
\end{tabular}\\

\hline
\end{tabular}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\subsection{Multiple Model Systems}


\label{page:imm_predict}


\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{imm\_predict}} \\

Calculates {\bf the prediction step} of {\bf IMM filter}.\\

\hline

{\bf Syntax:}
%
\texttt{[X\_p,P\_p,c\_j,X,P] = imm\_predict(X\_ip,P\_ip,MU\_ip,p\_ij,ind,dims,A,Q)}\\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{X\_ip} & Cell array containing $N^j \times 1$ mean state estimate
vectors for & \\ & each model $j$ after update step of previous time step
\\
\texttt{P\_ip} & Cell array containing $N^j \times N^j$ state covariance matrices for 
           & \\ &each model $j$ after update step of previous time step \\
\texttt{MU\_ip} & Vector containing the model probabilities at previous time step \\
\texttt{p\_ij} &  Model transition probability matrix \\
\texttt{ind} &  Indexes of state components for each model as a cell
array \\
\texttt{dims} &  Total number of different state components in the
combined system \\
\texttt{A} & State transition matrices for each model as a cell array \\
\texttt{Q} & Process noise matrices for each model as a cell array \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X\_p} & Predicted state mean for each model as a cell array \\
\texttt{P\_p} & Predicted state covariance for each model as a cell
array \\
\texttt{c\_j} & Normalizing factors for mixing probabilities \\
\texttt{X} & Combined predicted state mean estimate \\
\texttt{P} & Combined predicted state covariance estimate \\
\end{tabular}\\

\hline
\end{tabular}


\label{page:imm_update}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{imm\_update}} \\

Calculates {\bf the update step} of {\bf IMM filter}. \\


\hline

{\bf Syntax:}
%
\texttt{[X\_i,P\_i,MU,X,P] = imm\_update(X\_p,P\_p,c\_j,ind,dims,Y,H,R)}\\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{X\_p} & Cell array containing $N^j \times 1$ mean state estimate
vectors for & \\ & each model $j$ after update step of previous time step
\\
\texttt{P\_p} & Cell array containing $N^j \times N^j$ state covariance matrices for 
           & \\ &each model $j$ after update step of previous time step \\
\texttt{c\_j} & Normalizing factors for mixing probabilities \\
\texttt{ind} &  Indexes of state components for each model as a cell
array \\
\texttt{dims} &  Total number of different state components in the
combined system \\
\texttt{Y} & $D \times 1$ measurement vector \\
\texttt{H} & Measurement matrices for each model as a cell array \\
\texttt{R} & Measurement noise covariances for each model as a cell array \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X\_i} & Updated state mean for each model as a cell array \\
\texttt{P\_i} & Updated state covariance for each model as a cell
array \\
\texttt{MU} & Estimated probabilities of each model \\
\texttt{X} & Combined updated state mean estimate \\
\texttt{P} & Combined updated state covariance estimate \\
\end{tabular}\\

\hline
\end{tabular}

\label{page:imm_smooth}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{imm\_smooth}} \\

Smooths the filtered state estimates with the {\bf fixed-interval two-filter IMM smoother}.\\

\hline

{\bf Syntax:}

 \texttt{[X\_S,P\_S,X\_IS,P\_IS,MU\_S] = imm\_smooth(MM,PP,MM\_i,PP\_i,MU,p\_ij,...}\\
 \hspace{9.2 cm} \texttt{mu\_0j,ind,dims,A,Q,R,H,Y)}\\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{MM} & $N\times K$ matrix containing the means of forward-time
IMM-filter on each time step \\
\texttt{PP} & $N\times N \times K$ matrix containing the covariances of forward-time IMM-filter on each time step
\\
\texttt{MM\_i} &  Model-conditional means of forward-time IMM-filter on
each time step as a cell array \\
\texttt{PP\_i} & Model-conditional covariances of forward-time IMM-filter
on each time step as a cell array \\
\texttt{MU} & Model probabilities of forward-time IMM-filter on each time step \\
\texttt{p\_ij} & Model transition probability matrix \\
\texttt{mu\_0j} & Prior model probabilities \\
\texttt{ind} &  Indexes of state components for each model as a cell
array \\
\texttt{dims} &  Total number of different state components in the
combined system \\
\texttt{A} & State transition matrices for each model as a cell array \\
\texttt{Q} & Process noise matrices for each model as a cell array \\
\texttt{R} & Measurement noise covariances for each model as a cell array \\
\texttt{H} & Measurement matrices for each model as a cell array \\
\texttt{Y} & $D \times K$ sized measurement matrix \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X\_S} & Smoothed state means for each time step \\
\texttt{P\_S} & Smoothed state covariances for each time step \\
\texttt{X\_IS} & Model-conditioned smoothed state means for each time
step \\
\texttt{P\_IS} & Model-conditioned smoothed state covariances for each
time step \\
\texttt{MU\_S} & Smoothed model probabilities for each time step \\
\end{tabular}\\

\hline
\end{tabular}


\label{page:eimm_predict}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{eimm\_predict}} \\

Calculates {\bf the prediction step} of {\bf IMM-EKF filter}. Uses standard \\
Kalman filter update step for models having linear dynamics. \\

\hline

{\bf Syntax:}
%
\texttt{[X\_p,P\_p,c\_j,X,P] =
  eimm\_predict(X\_ip,P\_ip,MU\_ip,p\_ij,ind,...} \\
\hspace{8.1 cm} \texttt{dims,A,a,param,Q)}\\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{X\_ip} & Cell array containing $N^j \times 1$ mean state estimate
vectors for & \\ & each model $j$ after update step of previous time step
\\
\texttt{P\_ip} & Cell array containing $N^j \times N^j$ state covariance matrices for 
           & \\ &each model $j$ after update step of previous time step \\
\texttt{MU\_ip} & Vector containing the model probabilities at previous time step \\
\texttt{p\_ij} &  Model transition probability matrix \\
\texttt{ind} &  Indexes of state components for each model as a cell
array \\
\texttt{dims} &  Total number of different state components in the
combined system \\
\texttt{A} & Dynamic model matrices for each linear model and
Jacobians of each & \\ &
           non-linear model's dynamic model function as a cell array
\\
\texttt{a} & Function handles for dynamic model functions for each model
as a cell array
\\
\texttt{param} & Parameters of a for each model as a cell array
\\
\texttt{Q} & Process noise matrices for each model as a cell array \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X\_p} & Predicted state mean for each model as a cell array \\
\texttt{P\_p} & Predicted state covariance for each model as a cell
array \\
\texttt{c\_j} & Normalizing factors for mixing probabilities \\
\texttt{X} & Combined predicted state mean estimate \\
\texttt{P} & Combined predicted state covariance estimate \\
\end{tabular}\\

\hline
\end{tabular}


\label{page:eimm_update}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{eimm\_update}} \\

Calculates {\bf the update step} of {\bf IMM-EKF filter}. Uses standard \\
Kalman filter update step for models having linear measurements. \\

\hline

{\bf Syntax:}
%
\texttt{[X\_i,P\_i,MU,X,P] = eimm\_update(X\_p,P\_p,c\_j,ind,dims,...}
\\
\hspace{7.7 cm} \texttt{Y,H,h,h\_param,R)}\\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{X\_p} & Cell array containing $N^j \times 1$ mean state estimate
vectors for & \\ & each model $j$ after update step of previous time step
\\
\texttt{P\_p} & Cell array containing $N^j \times N^j$ state covariance matrices for 
           & \\ &each model $j$ after update step of previous time step \\
\texttt{c\_j} & Normalizing factors for mixing probabilities \\
\texttt{ind} &  Indexes of state components for each model as a cell
array \\
\texttt{dims} &  Total number of different state components in the
combined system \\
\texttt{Y} & $D \times 1$ measurement vector \\
\texttt{H} & Measurement matrices for each linear model and Jacobians
of each & \\ & non-linear model's measurement model function as a cell array \\
\texttt{h} & Cell array containing function handles for measurement
functions & \\ & for each model having non-linear measurements \\
\texttt{h\_param} & Parameters of h as a cell array \\
\texttt{R} & Measurement noise covariances for each model as a cell array \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X\_i} & Updated state mean for each model as a cell array \\
\texttt{P\_i} & Updated state covariance for each model as a cell
array \\
\texttt{MU} & Estimated probabilities of each model \\
\texttt{X} & Combined updated state mean estimate \\
\texttt{P} & Combined updated state covariance estimate \\
\end{tabular}\\

\hline
\end{tabular}


\label{page:eimm_smooth}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{eimm\_smooth}} \\

Smooths the filtered state estimates with the {\bf EKF based fixed-interval
  two-filter IMM smoother}.\\

\hline

{\bf Syntax:}

 \texttt{[X\_S,P\_S,X\_IS,P\_IS,MU\_S] = eimm\_smooth(MM,PP,MM\_i,PP\_i,MU,p\_ij,...}\\
 \hspace{9.4 cm} \texttt{mu\_0j,ind,dims,A,a,a\_param,}\\
 \hspace{9.4 cm} \texttt{Q,R,H,h,h\_param,Y)}\\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{MM} & $N\times K$ matrix containing the means & \\ & of forward-time
IMM-filter on each time step \\
\texttt{PP} & $N\times N \times K$ matrix containing the covariances &
\\ & of forward-time IMM-filter on each time step
\\
\texttt{MM\_i} &  Model-conditional means of forward-time IMM-filter
on & \\ &
each time step as a cell array \\
\texttt{PP\_i} & Model-conditional covariances of forward-time IMM-filter
on & \\ & each time step as a cell array\\
\texttt{MU} & Model probabilities of forward-time IMM-filter on each time step \\
\texttt{p\_ij} & Model transition probability matrix \\
\texttt{mu\_0j} & Prior model probabilities \\
\texttt{ind} &  Indexes of state components for each model as a cell
array \\
\texttt{dims} &  Total number of different state components in the
combined system \\
\texttt{A} & Dynamic model matrices for each linear model and Jacobians of each
& \\ & non-linear model's measurement model function as a cell array
\\
\texttt{a} & Cell array containing function handles for measurement
functions & \\ &
for each model having non-linear measurements \\
\texttt{a\_param} & Parameters of a as a cell array \\
\texttt{Q} & Process noise matrices for each model as a cell array \\
\texttt{R} & Measurement noise covariances for each model as a cell array \\
\texttt{H} & Measurement matrices for each linear model and Jacobians
of each & \\ & non-linear model's measurement model function as a cell array \\
\texttt{h} & Cell array containing function handles for measurement
functions & \\ & for each model having non-linear measurements \\
\texttt{h\_param} & Parameters of h as a cell array \\
\texttt{Y} & $D \times K$ sized measurement matrix \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X\_S} & Smoothed state means for each time step \\
\texttt{P\_S} & Smoothed state covariances for each time step \\
\texttt{X\_IS} & Model-conditioned smoothed state means for each time
step \\
\texttt{P\_IS} & Model-conditioned smoothed state covariances for each
time step \\
\texttt{MU\_S} & Smoothed model probabilities for each time step \\
\end{tabular}\\

\hline
\end{tabular}


\label{page:uimm_predict}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{uimm\_predict}} \\

Calculates {\bf the prediction step} of {\bf IMM-UKF filter}. Uses
standard \\  Kalman filter prediction step for models having linear dynamics. \\

\hline

{\bf Syntax:}
%
\texttt{[X\_p,P\_p,c\_j,X,P] =
  uimm\_predict(X\_ip,P\_ip,MU\_ip,p\_ij,ind,...} \\
\hspace{8.1 cm} \texttt{dims,a,param,Q)}\\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{X\_ip} & Cell array containing $N^j \times 1$ mean state estimate
vectors for & \\ & each model $j$ after update step of previous time step
\\
\texttt{P\_ip} & Cell array containing $N^j \times N^j$ state covariance matrices for 
           & \\ &each model $j$ after update step of previous time step \\
\texttt{MU\_ip} & Vector containing the model probabilities at previous time step \\
\texttt{p\_ij} &  Model transition probability matrix \\
\texttt{ind} &  Indexes of state components for each model as a cell
array \\
\texttt{dims} &  Total number of different state components in the
combined system \\
\texttt{A} & Dynamic model matrices for each linear model and
Jacobians of each & \\ &
           non-linear model's dynamic model function as a cell array
\\
\texttt{a} & Function handles for dynamic model functions for each model
as a cell array
\\
\texttt{param} & Parameters of a for each model as a cell array
\\
\texttt{Q} & Process noise matrices for each model as a cell array \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X\_p} & Predicted state mean for each model as a cell array \\
\texttt{P\_p} & Predicted state covariance for each model as a cell
array \\
\texttt{c\_j} & Normalizing factors for mixing probabilities \\
\texttt{X} & Combined predicted state mean estimate \\
\texttt{P} & Combined predicted state covariance estimate \\
\end{tabular}\\

\hline
\end{tabular}


\label{page:uimm_update}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{uimm\_update}} \\

Calculates {\bf the update step} of {\bf IMM-UKF filter}. Uses standard \\
Kalman filter update step for models having linear measurements. \\

\hline

{\bf Syntax:}
%
\texttt{[X\_i,P\_i,MU,X,P] = uimm\_update(X\_p,P\_p,c\_j,ind,dims,...}
\\
\hspace{7.7 cm} \texttt{Y,H,h,h\_param,R)}\\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{X\_p} & Cell array containing $N^j \times 1$ mean state estimate
vectors for & \\ & each model $j$ after update step of previous time step
\\
\texttt{P\_p} & Cell array containing $N^j \times N^j$ state covariance matrices for 
           & \\ &each model $j$ after update step of previous time step \\
\texttt{c\_j} & Normalizing factors for mixing probabilities \\
\texttt{ind} &  Indexes of state components for each model as a cell
array \\
\texttt{dims} &  Total number of different state components in the
combined system \\
\texttt{Y} & $D \times 1$ measurement vector \\
\texttt{H} & Measurement matrices for each linear model and Jacobians
of each & \\ & non-linear model's measurement model function as a cell
array \\
\texttt{h} & Cell array containing function handles for measurement
functions & \\ & for each model having non-linear measurements \\
\texttt{h\_param} & Parameters of h as a cell array \\
\texttt{R} & Measurement noise covariances for each model as a cell array \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X\_i} & Updated state mean for each model as a cell array \\
\texttt{P\_i} & Updated state covariance for each model as a cell
array \\
\texttt{MU} & Estimated probabilities of each model \\
\texttt{X} & Combined updated state mean estimate \\
\texttt{P} & Combined updated state covariance estimate \\
\end{tabular}\\

\hline
\end{tabular}


\label{page:uimm_smooth}

\noindent \begin{tabular}{|l|}
\hline

{\bf \texttt{uimm\_smooth}} \\

Smooths the filtered state estimates with the {\bf UKF based fixed-interval
  two-filter IMM smoother}.\\

\hline

{\bf Syntax:}

 \texttt{[X\_S,P\_S,X\_IS,P\_IS,MU\_S] = uimm\_smooth(MM,PP,MM\_i,PP\_i,MU,p\_ij,...}\\
 \hspace{9.4 cm} \texttt{mu\_0j,ind,dims,A,a,a\_param,}\\
 \hspace{9.4 cm} \texttt{Q,R,H,h,h\_param,Y)}\\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{MM} & $N\times K$ matrix containing the means & \\ & of forward-time
IMM-filter on each time step \\
\texttt{PP} & $N\times N \times K$ matrix containing the covariances &
\\ & of forward-time IMM-filter on each time step
\\
\texttt{MM\_i} &  Model-conditional means of forward-time IMM-filter
on & \\ &
each time step as a cell array \\
\texttt{PP\_i} & Model-conditional covariances of forward-time IMM-filter
on & \\ & each time step as a cell array\\
\texttt{MU} & Model probabilities of forward-time IMM-filter on each time step \\
\texttt{p\_ij} & Model transition probability matrix \\
\texttt{mu\_0j} & Prior model probabilities \\
\texttt{ind} &  Indexes of state components for each model as a cell
array \\
\texttt{dims} &  Total number of different state components in the
combined system \\
\texttt{A} & Dynamic model matrices for each linear model and Jacobians of each
& \\ & non-linear model's measurement model function as a cell array
\\
\texttt{a} & Cell array containing function handles for measurement
functions & \\ &
for each model having non-linear measurements \\
\texttt{a\_param} & Parameters of a as a cell array \\
\texttt{Q} & Process noise matrices for each model as a cell array \\
\texttt{R} & Measurement noise covariances for each model as a cell array \\
\texttt{H} & Measurement matrices for each linear model and Jacobians
of each & \\ & non-linear model's measurement model function as a cell array \\
\texttt{h} & Cell array containing function handles for measurement
functions & \\ & for each model having non-linear measurements. \\
\texttt{h\_param} & Parameters of h as a cell array \\
\texttt{Y} & $D \times K$ sized measurement matrix  \\
\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X\_S} & Smoothed state means for each time step \\
\texttt{P\_S} & Smoothed state covariances for each time step \\
\texttt{X\_IS} & Model-conditioned smoothed state means for each time
step \\
\texttt{P\_IS} & Model-conditioned smoothed state covariances for each
time step \\
\texttt{MU\_S} & Smoothed model probabilities for each time step \\
\end{tabular}\\

\hline
\end{tabular}


\subsection{Other functions}



For convience and to make this toolbox independent of any other
toolboxes (e.g. Statistics toolbox) we provide
some functions, which are useful for methods described above.

\label{page:gauss_pdf}

\begin{tabular}{|l|}
\hline

{\bf \texttt{gauss\_pdf}} \\

Calculates the density of a multivariate Gaussian distribution. \\

\hline
{\bf Syntax:}
%
\texttt{P = gauss\_pdf(X,M,S)} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{X} & $D\times N$ matrix containing $N$ vectors with size $D\times 1$ \\
\texttt{M} & $D\times 1$ mean of distribution or $N$ values as $D\times N$ matrix \\
\texttt{S} & $D\times D$ covariance matrix \\

\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{P} & Vector containing the probabilities of \texttt{X} \\
\end{tabular}\\

\hline

%\hline
\end{tabular}\\


\label{page:gauss_rnd}


\begin{tabular}{|l|}
\hline

{\bf \texttt{gauss\_rnd}} \\

Generates random samples from multivariate Gaussian distribution. \\

\hline
{\bf Syntax:}
%
\texttt{P = gauss\_rnd(M,S,N)} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{M} & $D\times 1$ mean of distribution or $N$ values as $D\times N$ matrix \\
\texttt{S} & $D\times D$ covariance matrix \\
\texttt{N} & Number of samples & default: 1\\

\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{X} & $D\times (K*N)$ matrix of samples\\
\end{tabular}\\

\hline

%\hline
\end{tabular}\\

\label{page:der_check}

\begin{tabular}{|l|}
\hline

{\bf \texttt{der\_check}} \\

Evaluates the derivative of given functions derivative analytically
and \\ using finite differences. If no output parameters are not
given, issues a \\warning if the derivatives differ too much.  
Otherwise the derivatives are \\stored in output parameters. The purpose
of this is to check that the \\ derivatives  of the
dynamic and measurement functions are bug free.\\

\hline
{\bf Syntax:}
%
\texttt{[D0,D1] = der\_check(F,DF,INDEX,[P1,P2,P3,...])} \\

\hline
{\bf Input:}
%
\begin{tabular}{l l l}
\texttt{F}     & Name of the function or inline function in form \texttt{F(P1,P2,...)} \\
\texttt{DF}    & Derivative value as matrix, name of derivative
function or inline & \\ & function in form \texttt{DF(P1,P2,...)} \\
\texttt{INDEX} & Index of parameter of interest. \texttt{DF} should
calculate the derivative & \\ & w.r.t. parameter \texttt{Pn}, where
\texttt{n} is the index. \\
\texttt{Pn}    & Parameters of the function \\

\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{D0} & Actual derivative \\
\texttt{D1} & Approximate derivative \\
\end{tabular}\\

\hline

%\hline
\end{tabular}\\


\label{page:rk4}

\begin{tabular}{|l|}
\hline

{\bf \texttt{rk4}} \\

Performs one fourth order Runge-Kutta iteration step for differential \\
equation of the form\\
$d\vec{x}/dt = f(\vec{x}(t),\theta(t))$,\\
or for the chained case \\
$d\vec{x}/dt = f(\vec{x}(t),\vec{y}(t),\theta(t))$ \\
$d\vec{y}/dt = g(\vec{x}(t),\theta(t))$,\\
where $\theta(t)$ is some parameter vector for functions $f$ and $g$ on
time $t$.\\\\

See the source file for more details and examples.\\

\hline
{\bf Syntax:}
%
\texttt{[x,Y] = rk4(f,dt,x,[P1,P2,P3,Y])} \\

\hline
{\bf Input:} 
%
\begin{tabular}{l l l}
\texttt{f}  & Name of the function in form \texttt{f(x,P(:))}, or
inline function & \\ & taking the same parameters. In chained case the
function should & \\ & be \texttt{f(x,y,P(:))} \\
\texttt{dt} & Step size as scalar \\
\texttt{x}  & Value of x from the previous time step \\
\texttt{P1} & Values of parameters of \texttt{f} at initial time $t$
& \\ & as a cell array or scalar & default: empty\\
\texttt{P2} & Values of parameters of \texttt{f} at time $t+\dt/2$
& default: \texttt{P1} \\
\texttt{P3} &  Values of parameters of \texttt{f} at time $t+\dt$ &
default: \texttt{P2} \\
\texttt{Y}  &  Cell array of partial results $y_1,y_2,y_3,y_4$ in the
RK algorithm & \\ & of the second parameter in the integrated
function. This can be & \\ & used for chaining the integrators & default: empty\\

\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{x} & Next value of x\\
\texttt{Y} & Cell array of partial results in Runge-Kutta algorithm\\
\end{tabular}\\

\hline

%\hline
\end{tabular}

\label{page:schol}

\begin{tabular}{|l|}
\hline

{\bf \texttt{schol}} \\

Computes the lower triangular Cholesky factor $\mat{L}$ of symmetric
positive \\ semidefinite matrix $\mat{A}$ such that $\mat{A} = \mat{L}\mat{L}^T$.\\

\hline
{\bf Syntax:}
%
\texttt{[L,def] = schol(A)} \\

\hline
{\bf Input:} 
%
\begin{tabular}{l l l}
\texttt{A} Matrix to be factorized \\

\end{tabular}\\
\hline

{\bf Output:}
%
\begin{tabular}{l l}
\texttt{L} & Lower triangular Cholesky factor if \texttt{A} is not
negative definite \\
\texttt{def} & Value 1,0,-1 denoting that \texttt{A} was positive
definite, positive \\ & semidefinite of negative definite, respectively \\
\end{tabular}\\

\hline

%\hline
\end{tabular}


%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

%\section{Full source code for the examples}

%Here is the full source code for all the examples introduced in the toolbox.
%You can also find them as m-files in the zipped package, where the m-files
%for the toolboxes functions are located. Feel free to experiment with the code, and
%see how the methods behave with different parameter values.

%\subsection{2D CWPA-model}

%\listinginput[1]{1}{examples/example1.m}

%\subsection{Tracking a random sine signal}

%\listinginput[1]{1}{examples/ekf_sine_demo.m}

%\listinginput[1]{1}{examples/ekf_sine_f.m}

%\listinginput[1]{1}{examples/ekf_sine_h.m}

%\listinginput[1]{1}{examples/ekf_sine_dh_dx.m}

\begin{thebibliography}{77}

\bibitem{bar-shalom2001} Bar-Shalom,Y., Li, X.-R., and Kirubarajan, T. (2001).
{\it Estimation with Applications to Tracking and Navigation.} Wiley Interscience.

\bibitem{bernardo1994} Bernardo, J.M. and Smith, A.F.M. (1994). 
{\it Bayesian Theory.} John Wiley \& Sons.

\bibitem{cox1994} Cox, H. (1964). 
{\it On the estimation of state variables and parameters for noisy dynamic systems.} IEEE Transactions on Automatic Control, 9(1):5-12.

\bibitem{doucet2001} Doucet, A., de Freitas, N., and Gordon, N., editors (2001).
{\it Sequential Monte Carlo Methods in Practice.} Springer.

\bibitem{fraser1969} Fraser, D. C. and Potter, J. E. (1969)
{\it The Optimum Linear Smoother as a Combination of Two Optimum Linear Filters.} IEEE Transactions on Automatic Control AC-14, 387-390.

\bibitem{gelb1974} Gelb, A., editor (1974).
{\it Applied Optimal Estimation.} The MIT Press.

\bibitem{gelman1995} Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. R. (1995).
{\it Bayesian Data Analysis.} Chapman \& Hall.

\bibitem{gordon1993} Gordon, N. J., Salmond, D. J., and Smith, A. F.
  M. (1993).
{\it Novel approach to nonlinear/non-Gaussian Bayesian state
  estimation} in IEEE Proceedings on Radar an Signal Processing,
volume 140, pages 107-113.

\bibitem{grewal2001} Grewal, M. S. and Andrews, A. P. (2001).
{\it Kalman Filtering, Theory and Practice Using MATLAB.} Wiley Interscience.

\bibitem{helmick1995} Helmick, R. E. (1995).
{\it Fixed-Interval Smoothing for Markovian Switching Systems.} IEEE
Transactions on Information Theory, 41(6):1845-1855.

\bibitem{jazwinski1970} Jazwinski, A. H. (1970).
{\it Stochastic Processes and Filtering Theory.} Academic Press.

\bibitem{julier2004a} Julier, S. J. and uhlmann, J. K. (2004).
{\it Corrections to "Unscented Filtering and Nonlinear Estimation"} In
{Proceedings of the IEEE}, 92(12):1958.

\bibitem{julier2004b} Julier, S. J. and Uhlmann, J. K. (2004).
{\it Unscented filtering and nonlinear estimation.} In {Proceedings of the IEEE}, 92(3):401-422.

\bibitem{julier1995} Julier, S. J., Uhlmann, J. K., and Durrant-Whyte, H. F. (1995).
{\it A new approach for filtering nonlinear systems.} In {\it Proceedings of the 1995 American Control, Conference, Seattle, Washington}, pages 1628-1632.

\bibitem{kalman1960} Kalman, R. E. (1960).
{\it A new approach to linear filtering and prediction problems.} Transactions of the ASME, Journal of Basic Engineering, 82:34-45.

\bibitem{kitagawa} Kitagawa, G. (1996).
{\it Monte Carlo filter and smoother for non-Gaussian nonlinear state
  space models.} Journal of Computational and Graphical Statistics, 5:1-25.

\bibitem{Kotecha2003} Kotecha, J. H., and Djuric, P. M. (2003).
{\it Gaussian Particle Filtering.} IEEE Transactions on Signal
Processing, 51(10):2592-2600.

\bibitem{maybeck1982a} Maybeck, P. (1982).
{\it Stochastic Models, Estimation and Control, Volume 2.} Academic press.

\bibitem{ristic2004} Ristic, B., Arulampalam, S., and Gordon, N. (2004).
{\it Beyond the Kalman Filter.} Artech House.

\bibitem{sage1971} Sage, A. P. and Melsa, J. L.  (1971).
{\it Estimation Theory with Applications to Communications and Control.} McGraw-Hill Book Company.

\bibitem{sarkka2006a} Särkkä, S. (2006).
{\it Recursive Bayesian Inference on Stochastic Differential Equations.} Doctoral dissertation, Helsinki University of Technology.

\bibitem{sarkka2006b} Särkkä, S., Vehtari, A. and Lampinen, J. (2007)
{\it Rao-Blackwellized Particle Filter for Multiple
  Target Tracking.} Information Fusion, 8(1):2-15. 

\bibitem{sarkka2006c} Särkkä, S. (2006)
{\it Unscented Rauch-Tung-Striebel Smoother.} Submitted.

\bibitem{vanhatalo} Vanhatalo, J., and Vehtari, A., (2006).
{\it MCMC Methods for MLP-networks and Gaussian Process and Stuff - A
  documentation for Matlab Toolbox MCMCstuff.} Toolbox can be found at
\url{http://www.lce.hut.fi/research/mm/mcmcstuff/}.

\bibitem{wan2001} Wan, E. A. and van der Merwe, R. (2001).
{\it The unscented Kalman filter.} In Haykin, S., editor {\it Kalman filtering and Neural Networks}, chapter 7. Wiley.

\bibitem{wu2005} Wu, Y., Hu, D., Wu, M., and Hu, X. (2005).
{\it Unscented Kalman Filtering for Additive Noise Case: Augmented
  versus Nonaugmented.} IEEE Signal Processing Letters 12(5):357-360.
%\bibitem{} . ().
%{\it .} .

%\bibitem{} . ().
%{\it .} .

\end{thebibliography}



\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
